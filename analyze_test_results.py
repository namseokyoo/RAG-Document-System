"""
í…ŒìŠ¤íŠ¸ ê²°ê³¼ì˜ ì˜ë¯¸ì  í’ˆì§ˆ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
- ì˜ˆìƒ ë‹µë³€ê³¼ ì‹¤ì œ ë‹µë³€ì˜ ì˜ë¯¸ì  ì¼ì¹˜ë„ í‰ê°€
- ë¬¸ì„œ íƒ€ì… ê²€ìƒ‰ ì •í™•ë„ í™•ì¸
- ì£¼ìš” ë¬¸ì œì  ì‹ë³„
"""

import json
from typing import Dict, List, Any
from pathlib import Path
import re


def analyze_semantic_quality(results_file: str) -> Dict[str, Any]:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ì˜ ì˜ë¯¸ì  í’ˆì§ˆ ë¶„ì„"""
    
    print(f"ğŸ“Š ë¶„ì„ ì‹œì‘: {results_file}\n")
    
    with open(results_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    pdf_results = data.get("pdf_results", [])
    pptx_results = data.get("pptx_results", [])
    topic_results = data.get("topic_switching_results", [])
    
    analysis = {
        "pdf_quality": {
            "total": len(pdf_results),
            "correct_document_type": 0,  # PDF ì§ˆë¬¸ì´ PDF íŒŒì¼ì„ ê²€ìƒ‰
            "wrong_document_type": 0,    # PDF ì§ˆë¬¸ì´ PPTX íŒŒì¼ì„ ê²€ìƒ‰
            "mixed_document_type": 0,    # PDFì™€ PPTXê°€ ì„ì„
            "semantically_correct": 0,    # ì˜ë¯¸ì ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€
            "semantically_wrong": 0,     # ì˜ë¯¸ì ìœ¼ë¡œ ì˜ëª»ëœ ë‹µë³€
            "issues": []
        },
        "pptx_quality": {
            "total": len(pptx_results),
            "correct_document_type": 0,
            "wrong_document_type": 0,
            "mixed_document_type": 0,
            "semantically_correct": 0,
            "semantically_wrong": 0,
            "issues": []
        }
    }
    
    # PDF ê²°ê³¼ ë¶„ì„
    print("ğŸ“„ PDF ê²°ê³¼ ë¶„ì„ ì¤‘...")
    for idx, result in enumerate(pdf_results, 1):
        question = result.get("question", "")
        expected_sources = result.get("expected_sources", [])
        actual_sources = result.get("actual_sources", [])
        actual_answer = result.get("actual_answer", "")
        expected_answer = result.get("expected_answer", "")
        
        # ë¬¸ì„œ íƒ€ì… í™•ì¸
        expected_pdf_files = []
        for source in expected_sources:
            doc_name = source.get("ë¬¸ì„œ", "")
            if "pdf" in doc_name.lower():
                expected_pdf_files.append(doc_name)
        
        actual_pdf_files = [s.get("file_name", "") for s in actual_sources if "pdf" in s.get("file_name", "").lower()]
        actual_pptx_files = [s.get("file_name", "") for s in actual_sources if "pptx" in s.get("file_name", "").lower()]
        
        has_pdf = len(actual_pdf_files) > 0
        has_pptx = len(actual_pptx_files) > 0
        
        # ë¬¸ì„œ íƒ€ì… ì •í™•ë„ í‰ê°€
        if expected_pdf_files:  # ì˜ˆìƒ ë‹µë³€ì´ PDFë¥¼ ì°¸ì¡°í•˜ëŠ” ê²½ìš°
            if has_pdf and not has_pptx:
                analysis["pdf_quality"]["correct_document_type"] += 1
            elif has_pptx and not has_pdf:
                analysis["pdf_quality"]["wrong_document_type"] += 1
                analysis["pdf_quality"]["issues"].append({
                    "index": idx,
                    "question": question[:80] + "..." if len(question) > 80 else question,
                    "expected_pdf": expected_pdf_files,
                    "actual_pptx": actual_pptx_files[:3],
                    "issue_type": "wrong_document_type"
                })
            elif has_pdf and has_pptx:
                analysis["pdf_quality"]["mixed_document_type"] += 1
                analysis["pdf_quality"]["issues"].append({
                    "index": idx,
                    "question": question[:80] + "..." if len(question) > 80 else question,
                    "expected_pdf": expected_pdf_files,
                    "actual_pdf": actual_pdf_files[:2],
                    "actual_pptx": actual_pptx_files[:2],
                    "issue_type": "mixed_document_type"
                })
        
        # ì˜ë¯¸ì  ì¼ì¹˜ë„ ê°„ë‹¨ í‰ê°€ (í‚¤ì›Œë“œ ê¸°ë°˜)
        if expected_answer and actual_answer:
            # ì˜ˆìƒ ë‹µë³€ì˜ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
            expected_keywords = set(re.findall(r'\b\w{4,}\b', expected_answer.lower()))
            actual_keywords = set(re.findall(r'\b\w{4,}\b', actual_answer.lower()))
            
            # í‚¤ì›Œë“œ ê²¹ì¹¨ ë¹„ìœ¨ ê³„ì‚°
            if expected_keywords:
                overlap_ratio = len(expected_keywords & actual_keywords) / len(expected_keywords)
                
                # ì˜ë¯¸ì ìœ¼ë¡œ ì™„ì „íˆ ë‹¤ë¥¸ ê²½ìš° (ë§¤ì¶œ, OLED ë“± ì˜ëª»ëœ í‚¤ì›Œë“œ)
                wrong_keywords = ["ë§¤ì¶œ", "ì±„ë„", "ì˜¨ë¼ì¸", "ì˜¤í”„ë¼ì¸", "ë¶„ê¸°", "ì–µì›"]
                has_wrong_keywords = any(kw in actual_answer.lower() for kw in wrong_keywords)
                
                if has_wrong_keywords and len(expected_pdf_files) > 0:
                    analysis["pdf_quality"]["semantically_wrong"] += 1
                    if idx not in [i["index"] for i in analysis["pdf_quality"]["issues"]]:
                        analysis["pdf_quality"]["issues"].append({
                            "index": idx,
                            "question": question[:80] + "..." if len(question) > 80 else question,
                            "issue_type": "semantically_wrong",
                            "expected_keywords": list(expected_keywords)[:5],
                            "actual_keywords": list(actual_keywords)[:5]
                        })
                elif overlap_ratio > 0.3:
                    analysis["pdf_quality"]["semantically_correct"] += 1
    
    # PPTX ê²°ê³¼ ë¶„ì„
    print("ğŸ“Š PPTX ê²°ê³¼ ë¶„ì„ ì¤‘...")
    for idx, result in enumerate(pptx_results, 1):
        question = result.get("question", "")
        actual_sources = result.get("actual_sources", [])
        expected_answer = result.get("expected_answer", "")
        actual_answer = result.get("actual_answer", "")
        
        actual_has_pptx = any("pptx" in s.get("file_name", "").lower() for s in actual_sources)
        actual_has_pdf = any("pdf" in s.get("file_name", "").lower() for s in actual_sources)
        
        if actual_has_pptx and not actual_has_pdf:
            analysis["pptx_quality"]["correct_document_type"] += 1
        elif actual_has_pdf and not actual_has_pptx:
            analysis["pptx_quality"]["wrong_document_type"] += 1
            analysis["pptx_quality"]["issues"].append({
                "index": idx,
                "question": question[:80] + "..." if len(question) > 80 else question,
                "actual_pdf": [s.get("file_name", "") for s in actual_sources if "pdf" in s.get("file_name", "").lower()][:3],
                "issue_type": "wrong_document_type"
            })
        elif actual_has_pptx and actual_has_pdf:
            analysis["pptx_quality"]["mixed_document_type"] += 1
        
        # ì˜ë¯¸ì  ì¼ì¹˜ë„ í‰ê°€
        if expected_answer and actual_answer:
            expected_keywords = set(re.findall(r'\b\w{4,}\b', expected_answer.lower()))
            actual_keywords = set(re.findall(r'\b\w{4,}\b', actual_answer.lower()))
            
            if expected_keywords:
                overlap_ratio = len(expected_keywords & actual_keywords) / len(expected_keywords)
                
                if overlap_ratio > 0.3:
                    analysis["pptx_quality"]["semantically_correct"] += 1
                else:
                    analysis["pptx_quality"]["semantically_wrong"] += 1
    
    return analysis


def print_analysis_report(analysis: Dict[str, Any]):
    """ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ì¶œë ¥"""
    
    print("\n" + "="*80)
    print("ğŸ“Š ì˜ë¯¸ì  í’ˆì§ˆ ë¶„ì„ ê²°ê³¼")
    print("="*80)
    
    # PDF í’ˆì§ˆ
    pdf_q = analysis["pdf_quality"]
    print(f"\nğŸ“„ PDF ë¬¸ì„œ ì§ˆë¬¸ ë¶„ì„:")
    print(f"  ì´ ì§ˆë¬¸ ìˆ˜: {pdf_q['total']}")
    print(f"  âœ… ì˜¬ë°”ë¥¸ ë¬¸ì„œ íƒ€ì… ê²€ìƒ‰: {pdf_q['correct_document_type']} ({pdf_q['correct_document_type']/pdf_q['total']*100:.1f}%)")
    print(f"  âŒ ì˜ëª»ëœ ë¬¸ì„œ íƒ€ì… ê²€ìƒ‰: {pdf_q['wrong_document_type']} ({pdf_q['wrong_document_type']/pdf_q['total']*100:.1f}%)")
    print(f"  âš ï¸  í˜¼í•© ë¬¸ì„œ íƒ€ì… ê²€ìƒ‰: {pdf_q['mixed_document_type']} ({pdf_q['mixed_document_type']/pdf_q['total']*100:.1f}%)")
    print(f"  âœ… ì˜ë¯¸ì ìœ¼ë¡œ ì •í™•: {pdf_q['semantically_correct']}")
    print(f"  âŒ ì˜ë¯¸ì ìœ¼ë¡œ ì˜ëª»: {pdf_q['semantically_wrong']}")
    
    if pdf_q['issues']:
        print(f"\n  âš ï¸  ë¬¸ì œ ì‚¬ë¡€ ({len(pdf_q['issues'])}ê°œ):")
        for issue in pdf_q['issues'][:10]:  # ìƒìœ„ 10ê°œë§Œ
            print(f"\n    [{issue['index']}] {issue.get('issue_type', 'unknown')}")
            print(f"        ì§ˆë¬¸: {issue.get('question', '')}")
            if issue.get('expected_pdf'):
                print(f"        ì˜ˆìƒ: {issue['expected_pdf']}")
            if issue.get('actual_pptx'):
                print(f"        ì‹¤ì œ(PPTX): {issue['actual_pptx']}")
            if issue.get('actual_pdf'):
                print(f"        ì‹¤ì œ(PDF): {issue['actual_pdf']}")
    
    # PPTX í’ˆì§ˆ
    pptx_q = analysis["pptx_quality"]
    print(f"\nğŸ“Š PPTX ë¬¸ì„œ ì§ˆë¬¸ ë¶„ì„:")
    print(f"  ì´ ì§ˆë¬¸ ìˆ˜: {pptx_q['total']}")
    print(f"  âœ… ì˜¬ë°”ë¥¸ ë¬¸ì„œ íƒ€ì… ê²€ìƒ‰: {pptx_q['correct_document_type']} ({pptx_q['correct_document_type']/pptx_q['total']*100:.1f}%)")
    print(f"  âŒ ì˜ëª»ëœ ë¬¸ì„œ íƒ€ì… ê²€ìƒ‰: {pptx_q['wrong_document_type']} ({pptx_q['wrong_document_type']/pptx_q['total']*100:.1f}%)")
    print(f"  âš ï¸  í˜¼í•© ë¬¸ì„œ íƒ€ì… ê²€ìƒ‰: {pptx_q['mixed_document_type']} ({pptx_q['mixed_document_type']/pptx_q['total']*100:.1f}%)")
    print(f"  âœ… ì˜ë¯¸ì ìœ¼ë¡œ ì •í™•: {pptx_q['semantically_correct']}")
    print(f"  âŒ ì˜ë¯¸ì ìœ¼ë¡œ ì˜ëª»: {pptx_q['semantically_wrong']}")
    
    if pptx_q['issues']:
        print(f"\n  âš ï¸  ë¬¸ì œ ì‚¬ë¡€ ({len(pptx_q['issues'])}ê°œ):")
        for issue in pptx_q['issues'][:5]:
            print(f"\n    [{issue['index']}] {issue.get('issue_type', 'unknown')}")
            print(f"        ì§ˆë¬¸: {issue.get('question', '')}")
            if issue.get('actual_pdf'):
                print(f"        ì‹¤ì œ(PDF): {issue['actual_pdf']}")
    
    # ì „ì²´ ìš”ì•½
    print(f"\n" + "="*80)
    print("ğŸ“ˆ ì „ì²´ ìš”ì•½")
    print("="*80)
    
    total_questions = pdf_q['total'] + pptx_q['total']
    total_correct_type = pdf_q['correct_document_type'] + pptx_q['correct_document_type']
    total_wrong_type = pdf_q['wrong_document_type'] + pptx_q['wrong_document_type']
    
    print(f"  ì´ ì§ˆë¬¸ ìˆ˜: {total_questions}")
    print(f"  ì˜¬ë°”ë¥¸ ë¬¸ì„œ íƒ€ì… ê²€ìƒ‰ë¥ : {total_correct_type/total_questions*100:.1f}%")
    print(f"  ì˜ëª»ëœ ë¬¸ì„œ íƒ€ì… ê²€ìƒ‰ë¥ : {total_wrong_type/total_questions*100:.1f}%")
    print(f"  ì´ ë¬¸ì œ ì‚¬ë¡€: {len(pdf_q['issues']) + len(pptx_q['issues'])}ê°œ")


if __name__ == "__main__":
    results_file = "test_results/test_results_20251105_013634.json"
    
    if not Path(results_file).exists():
        print(f"âŒ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {results_file}")
    else:
        analysis = analyze_semantic_quality(results_file)
        print_analysis_report(analysis)
        
        # JSONìœ¼ë¡œë„ ì €ì¥
        output_file = "test_results/semantic_analysis.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        print(f"\nâœ… ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_file}")









