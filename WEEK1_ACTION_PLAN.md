# Week 1 ì‹¤í–‰ ê³„íš ë° íš¨ê³¼ ë¶„ì„

**ëª©í‘œ**: Multi-Document Diversity í™•ë³´ + Multi-query ìµœì í™”
**ê¸°ê°„**: 5 ì˜ì—…ì¼
**ì˜ˆìƒ íš¨ê³¼**: ë¬¸ì„œ ì í•©ì„± 50.9 â†’ 75+ì , í‰ê·  ì‘ë‹µì‹œê°„ 120ì´ˆ â†’ 40ì´ˆ

---

## ğŸ“‹ Task 1: Multi-Document Diversity ê°•ì œ (Day 1-3)

### í˜„ì¬ ë¬¸ì œ ì§„ë‹¨

#### ì‹¤ì œ ë¡œê·¸ ë¶„ì„ í•„ìš”
```python
# 1ë‹¨ê³„: ë¬¸ì œ ì •í™•í•œ íŒŒì•…
# ì‹¤í–‰í•  ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

from collections import Counter
import json

def analyze_retrieval_diversity(log_dir):
    """ê° í…ŒìŠ¤íŠ¸ì˜ ë¬¸ì„œ ë‹¤ì–‘ì„± ë¶„ì„"""

    results = []
    for log_file in glob(f"{log_dir}/*.json"):
        with open(log_file) as f:
            data = json.load(f)

        sources = data.get('citation', {}).get('sources', [])

        # ë¬¸ì„œë³„ ì²­í¬ ìˆ˜ ê³„ì‚°
        doc_counts = Counter(s['source'] for s in sources)

        results.append({
            'test_id': data['test_id'],
            'total_chunks': len(sources),
            'unique_docs': len(doc_counts),
            'doc_distribution': dict(doc_counts),
            'max_chunks_from_single_doc': max(doc_counts.values()) if doc_counts else 0
        })

    return results

# ì˜ˆìƒ ê²°ê³¼:
# {
#   'test_id': 'benchmark_002',
#   'total_chunks': 5,
#   'unique_docs': 1,  # â† ë¬¸ì œ!
#   'doc_distribution': {'lgd_display_news.pdf': 5},  # ì „ë¶€ í•œ ë¬¸ì„œ
#   'max_chunks_from_single_doc': 5
# }
```

### ì›ì¸ ê°€ì„¤ ë° ê²€ì¦

#### ê°€ì„¤ 1: Rerankingì´ ë™ì¼ ë¬¸ì„œ ì„ í˜¸
```python
# ê²€ì¦ ë°©ë²•: Reranking ì „í›„ ë¹„êµ
# utils/reranker.py ë¡œê·¸ ì¶”ê°€

def rerank_documents(self, query, docs):
    # BEFORE reranking
    before_docs = Counter(d.metadata['source'] for d in docs)
    print(f"[RERANK-BEFORE] Unique docs: {len(before_docs)}")

    # Reranking
    reranked = self._score_and_sort(query, docs)

    # AFTER reranking
    after_docs = Counter(d.metadata['source'] for d in reranked[:self.top_k])
    print(f"[RERANK-AFTER] Unique docs: {len(after_docs)}")
    print(f"[RERANK-AFTER] Distribution: {dict(after_docs)}")

    return reranked[:self.top_k]

# ì˜ˆìƒ: Reranking í›„ ë‹¤ì–‘ì„± ê°ì†Œ
```

#### ê°€ì„¤ 2: Small-to-Largeê°€ ë™ì¼ ë¬¸ì„œë§Œ í™•ì¥
```python
# ê²€ì¦: context expansion ë¡œì§ í™•ì¸
# utils/rag_chain.py

def _expand_context_small_to_large(self, initial_chunks):
    # í˜„ì¬: ê° ì²­í¬ì˜ ì´ì „/ì´í›„ ì²­í¬ë§Œ ê°€ì ¸ì˜´
    # ë¬¸ì œ: ë™ì¼ ë¬¸ì„œ ë‚´ ì²­í¬ë§Œ í™•ì¥

    expanded = []
    for chunk in initial_chunks:
        # chunkì˜ source, chunk_id ì‚¬ìš©
        # â†’ ê°™ì€ ë¬¸ì„œì˜ ì¸ì ‘ ì²­í¬ë§Œ ê²€ìƒ‰
        neighbors = self._get_neighbor_chunks(chunk)
        expanded.extend(neighbors)

    # ê²°ê³¼: ì´ˆê¸° 5ê°œ ì²­í¬ê°€ doc_Aì—ì„œ ì™”ìœ¼ë©´
    #       í™•ì¥ëœ ì²­í¬ë„ ì „ë¶€ doc_A
```

#### ê°€ì„¤ 3: Deduplication ë¯¸ì‘ë™
```python
# ê²€ì¦: citation.py í™•ì¸
# utils/citation.py

def deduplicate_sources(self, sources):
    seen = set()
    unique = []

    for s in sources:
        # í˜„ì¬ ë¡œì§: (source, page) íŠœí”Œë¡œ ì¤‘ë³µ ì²´í¬
        key = (s['source'], s.get('page'))

        if key not in seen:
            seen.add(key)
            unique.append(s)

    # ë¬¸ì œ: ê°™ì€ ë¬¸ì„œ, ë‹¤ë¥¸ í˜ì´ì§€ â†’ ì¤‘ë³µ ì•„ë‹˜ìœ¼ë¡œ íŒì •
    # ê²°ê³¼: ë™ì¼ ë¬¸ì„œ 5ê°œ ì²­í¬ ëª¨ë‘ í†µê³¼
```

---

### í•´ê²° ë°©ì•ˆ 3ê°€ì§€ (ìš°ì„ ìˆœìœ„ ìˆœ)

#### Solution 1: Post-Reranking Diversity Penalty (Day 1-2)
**ì¶”ì²œ ì´ìœ **: ê°€ì¥ ë¹ ë¥´ê³  íš¨ê³¼ì , ê¸°ì¡´ ì½”ë“œ ìµœì†Œ ë³€ê²½

```python
# utils/reranker.py ìˆ˜ì •

def rerank_with_diversity(self, query: str, docs: List, top_k: int = 10):
    """
    Reranking with diversity penalty

    ì•Œê³ ë¦¬ì¦˜:
    1. ëª¨ë“  ë¬¸ì„œì— relevance score ê³„ì‚°
    2. Greedy selection with diversity penalty
       - ì„ íƒëœ ë¬¸ì„œì™€ ê°™ì€ sourceë©´ score * 0.5
    3. Top-K ë°˜í™˜
    """

    # 1. Score ê³„ì‚°
    scored_docs = []
    for doc in docs:
        score = self._calculate_relevance(query, doc)
        scored_docs.append({
            'doc': doc,
            'score': score,
            'source': doc.metadata.get('source', 'unknown')
        })

    # 2. Greedy selection with diversity
    selected = []
    selected_sources = Counter()

    # Score ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    scored_docs.sort(key=lambda x: x['score'], reverse=True)

    for item in scored_docs:
        if len(selected) >= top_k:
            break

        # Diversity penalty ì ìš©
        source = item['source']
        penalty = 1.0

        # ì´ë¯¸ ì„ íƒëœ sourceë¼ë©´ penalty
        if source in selected_sources:
            # ê°™ì€ ë¬¸ì„œì—ì„œ Nê°œ ì„ íƒë˜ì—ˆìœ¼ë©´ (N+1) * 0.3 í˜ë„í‹°
            penalty = 1.0 - (selected_sources[source] * 0.3)
            penalty = max(penalty, 0.1)  # ìµœì†Œ 0.1

        adjusted_score = item['score'] * penalty

        # ì¬ì •ë ¬ì„ ìœ„í•´ ì„ì‹œë¡œ ì €ì¥í•˜ì§€ ì•Šê³ ,
        # Threshold ê¸°ë°˜ìœ¼ë¡œ ì¦‰ì‹œ ì„ íƒ
        if len(selected) < 3:
            # ì²˜ìŒ 3ê°œëŠ” ë¬´ì¡°ê±´ ì„ íƒ (relevance ìš°ì„ )
            selected.append(item)
            selected_sources[source] += 1
        elif adjusted_score > 0.5:  # Threshold
            selected.append(item)
            selected_sources[source] += 1

    # 3. ê²°ê³¼ ë°˜í™˜
    print(f"[DIVERSITY] Selected from {len(selected_sources)} unique docs")
    print(f"[DIVERSITY] Distribution: {dict(selected_sources)}")

    return [item['doc'] for item in selected]


# ì˜ˆìƒ íš¨ê³¼:
# Before: {'doc_A': 5}
# After:  {'doc_A': 2, 'doc_B': 2, 'doc_C': 1}
```

**êµ¬í˜„ ë‹¨ê³„**:
1. Day 1 ì˜¤ì „: ì½”ë“œ ì‘ì„± (2ì‹œê°„)
2. Day 1 ì˜¤í›„: ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (2ì‹œê°„)
3. Day 2 ì˜¤ì „: Balanced í…ŒìŠ¤íŠ¸ ì¬ì‹¤í–‰ (1ì‹œê°„)
4. Day 2 ì˜¤í›„: ê²°ê³¼ ë¶„ì„ + íŒŒë¼ë¯¸í„° íŠœë‹ (3ì‹œê°„)

**ì˜ˆìƒ íš¨ê³¼**:
```
ë¬¸ì„œ ì í•©ì„±: 50.9 â†’ 70-75ì 
  - unique_docs: 1.2 â†’ 3.5ê°œ í‰ê· 
  - ë‹¤ì–‘ì„± ë¹„ìœ¨: 20% â†’ 70%

ë‹¨ì :
  - Relevance ì•½ê°„ í¬ìƒ (top score ë¬¸ì„œ ì œì™¸ ê°€ëŠ¥)
  - íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš” (penalty ê°•ë„)
```

---

#### Solution 2: MMR (Maximal Marginal Relevance) (Day 2-3)
**ì¶”ì²œ ì´ìœ **: ê²€ì¦ëœ ì•Œê³ ë¦¬ì¦˜, í•™ìˆ ì  ê·¼ê±° ìˆìŒ

```python
# utils/retriever.py ë˜ëŠ” reranker.py ì¶”ê°€

def mmr_rerank(self, query_embedding, docs, top_k=10, lambda_param=0.5):
    """
    Maximal Marginal Relevance

    Score = Î» * Relevance - (1-Î») * Similarity to selected

    Args:
        lambda_param: 0=diversity only, 1=relevance only
                     ê¸°ë³¸ 0.5 (ê· í˜•)
    """

    # 1. Queryì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
    query_emb = np.array(query_embedding)
    doc_embeddings = [self._get_embedding(d) for d in docs]

    relevance_scores = [
        cosine_similarity(query_emb, doc_emb)
        for doc_emb in doc_embeddings
    ]

    # 2. MMR ì„ íƒ
    selected_indices = []
    selected_embeddings = []

    for _ in range(top_k):
        if not selected_indices:
            # ì²« ë¬¸ì„œ: Relevanceë§Œ ê³ ë ¤
            best_idx = np.argmax(relevance_scores)
        else:
            # MMR score ê³„ì‚°
            mmr_scores = []
            for i, doc_emb in enumerate(doc_embeddings):
                if i in selected_indices:
                    mmr_scores.append(-1)  # ì´ë¯¸ ì„ íƒë¨
                    continue

                # Relevance
                rel = relevance_scores[i]

                # Max similarity to selected documents
                max_sim = max(
                    cosine_similarity(doc_emb, sel_emb)
                    for sel_emb in selected_embeddings
                )

                # MMR
                mmr = lambda_param * rel - (1 - lambda_param) * max_sim
                mmr_scores.append(mmr)

            best_idx = np.argmax(mmr_scores)

        selected_indices.append(best_idx)
        selected_embeddings.append(doc_embeddings[best_idx])

    return [docs[i] for i in selected_indices]


# ì˜ˆìƒ íš¨ê³¼:
# - ë¬¸ì„œ ë‹¤ì–‘ì„± ìë™ ë³´ì¥
# - Relevanceì™€ Diversity ê· í˜• ì¡°ì ˆ ê°€ëŠ¥
```

**êµ¬í˜„ ë‹¨ê³„**:
1. Day 2 ì˜¤ì „: Embedding ì¶”ì¶œ ë¡œì§ í™•ì¸ (1ì‹œê°„)
2. Day 2 ì˜¤í›„: MMR êµ¬í˜„ (3ì‹œê°„)
3. Day 3 ì˜¤ì „: í†µí•© í…ŒìŠ¤íŠ¸ (2ì‹œê°„)
4. Day 3 ì˜¤í›„: lambda íŒŒë¼ë¯¸í„° ìµœì í™” (2ì‹œê°„)

**ì˜ˆìƒ íš¨ê³¼**:
```
ë¬¸ì„œ ì í•©ì„±: 50.9 â†’ 75-80ì 
  - unique_docs: 1.2 â†’ 4.0ê°œ í‰ê· 
  - ë‹¤ì–‘ì„± ë¹„ìœ¨: 20% â†’ 80%

ì¥ì :
  - ê²€ì¦ëœ ì•Œê³ ë¦¬ì¦˜
  - íŒŒë¼ë¯¸í„° íŠœë‹ ë‹¨ìˆœ (lambda í•˜ë‚˜)

ë‹¨ì :
  - Embedding ì—°ì‚° ì¶”ê°€ (ì•½ê°„ ëŠë ¤ì§)
  - êµ¬í˜„ ë³µì¡ë„ ì¤‘ê°„
```

---

#### Solution 3: Document-Level Top-K (Alternative)
**ì¶”ì²œ ì´ìœ **: ê°€ì¥ ë‹¨ìˆœ, ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…

```python
# utils/vector_store.py ìˆ˜ì •

def search_with_document_diversity(self, query, k=10, min_docs=3):
    """
    ë¬¸ì„œ ë ˆë²¨ Top-K

    1. ë§ì€ í›„ë³´ ê²€ìƒ‰ (k*3)
    2. ë¬¸ì„œë³„ë¡œ ê·¸ë£¹í™”
    3. ê° ë¬¸ì„œì—ì„œ best chunk ì„ íƒ
    4. min_docsê°œ ë¬¸ì„œê¹Œì§€ ìˆ˜ì§‘
    """

    # 1. í›„ë³´ ê²€ìƒ‰
    candidates = self.similarity_search(query, k=k*3)

    # 2. ë¬¸ì„œë³„ ê·¸ë£¹í™”
    doc_groups = {}
    for doc in candidates:
        source = doc.metadata['source']
        if source not in doc_groups:
            doc_groups[source] = []
        doc_groups[source].append(doc)

    # 3. ê° ë¬¸ì„œì—ì„œ Top-2 ì„ íƒ
    selected = []
    for source, chunks in sorted(
        doc_groups.items(),
        key=lambda x: len(x[1]),
        reverse=True
    ):
        # ê° ë¬¸ì„œì—ì„œ ìµœëŒ€ 2ê°œ ì²­í¬
        selected.extend(chunks[:2])

        if len(selected) >= k:
            break

    return selected[:k]


# ì˜ˆìƒ íš¨ê³¼:
# - unique_docs: 5ê°œ (k=10ì´ë©´)
# - ê° ë¬¸ì„œë‹¹ 2ê°œ ì²­í¬ ë³´ì¥
```

**êµ¬í˜„ ë‹¨ê³„**:
1. Day 1: 1ì‹œê°„ êµ¬í˜„
2. Day 1: 1ì‹œê°„ í…ŒìŠ¤íŠ¸

**ì˜ˆìƒ íš¨ê³¼**:
```
ë¬¸ì„œ ì í•©ì„±: 50.9 â†’ 65-70ì 
  - unique_docs: 1.2 â†’ 5.0ê°œ (í™•ì •)
  - ë‹¤ì–‘ì„± ë¹„ìœ¨: 20% â†’ 100%

ì¥ì :
  - êµ¬í˜„ ë§¤ìš° ë‹¨ìˆœ
  - ë¹ ë¦„

ë‹¨ì :
  - ë„ˆë¬´ ê¸°ê³„ì  (ê° ë¬¸ì„œ ë™ì¼ ë¹„ì¤‘)
  - Relevance í¬ê²Œ í¬ìƒ ê°€ëŠ¥
  - ìœ ì—°ì„± ë‚®ìŒ
```

---

### ìµœì¢… ê¶Œì¥: **Solution 1 (Diversity Penalty) + Solution 2 (MMR) ìˆœì°¨ ì ìš©**

#### Day 1-2: Solution 1 êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
- ë¹ ë¥¸ ê°œì„  íš¨ê³¼ í™•ì¸
- 70-75ì  ë‹¬ì„± ì˜ˆìƒ

#### Day 3: Solution 2 (MMR) ì¶”ê°€ êµ¬í˜„
- ë” ë‚˜ì€ ì„±ëŠ¥ (75-80ì ) ëª©í‘œ
- A/B í…ŒìŠ¤íŠ¸ë¡œ ë¹„êµ

#### ì˜ì‚¬ê²°ì • ê¸°ì¤€:
```python
if diversity_penalty_score >= 73:
    # Solution 1 ì±„íƒ, MMRì€ ì¶”í›„ ê³ ë ¤
else:
    # MMR êµ¬í˜„ ê³„ì†
```

---

## ğŸ“‹ Task 2: Multi-query ìµœì í™” (Day 3-5)

### í˜„ì¬ ë¬¸ì œ ì§„ë‹¨

#### ì‹¤ì œ ì„±ëŠ¥ ë°ì´í„°
```
Fallback mode:    í‰ê·  25ì´ˆ  (8-50ì´ˆ)
Multi-query mode: í‰ê·  180ì´ˆ (60-472ì´ˆ)

ë¬¸ì œ: 7ë°° ì‹œê°„ ì°¨ì´
```

#### ì›ì¸ ë¶„ì„
```python
# utils/rag_chain.py ë¶„ì„ í•„ìš”

def query(self, question):
    # 1. Classification
    classification = self.classifier.classify(question)

    # 2. Multi-query ì—¬ë¶€ ê²°ì •
    if classification['multi_query']:
        # ë¬¸ì œ: ëª¨ë“  Complex/Exhaustive ì§ˆë¬¸ì— multi-query
        expanded = self._expand_queries(question)  # 3-5ê°œ ì¿¼ë¦¬ ìƒì„±

        results = []
        for q in expanded:
            # ë¬¸ì œ: ê° ì¿¼ë¦¬ë§ˆë‹¤ ì „ì²´ íŒŒì´í”„ë¼ì¸
            # - Embedding (0.3-0.5ì´ˆ * N)
            # - Vector search (2-5ì´ˆ * N)
            # - BM25 search (1-3ì´ˆ * N)
            # - Reranking (5-10ì´ˆ * N)
            results.extend(self._search(q))

        # ì´ ì‹œê°„ = ë‹¨ì¼ * N * ì˜¤ë²„í—¤ë“œ
        # ì˜ˆ: 3ê°œ ì¿¼ë¦¬ * 40ì´ˆ * 1.5 = 180ì´ˆ
```

### í•´ê²° ë°©ì•ˆ

#### Solution 1: ì¡°ê±´ë¶€ Multi-query í™œì„±í™” (Day 3-4)

```python
# utils/question_classifier.py ìˆ˜ì •

def classify(self, question):
    """
    ì§ˆë¬¸ ë¶„ë¥˜ ê°œì„ 

    Multi-query í™œì„±í™” ì¡°ê±´ ê°•í™”:
    1. Simple â†’ Never
    2. Normal â†’ Only if ë¹„êµ/ëŒ€ì¡° í‚¤ì›Œë“œ
    3. Complex â†’ Only if ë‹¤ê°ë„ ë¶„ì„ í•„ìš”
    4. Exhaustive â†’ Always
    """

    # ê¸°ì¡´ ë¶„ë¥˜
    q_type = self._classify_type(question)

    # Multi-query í•„ìš”ì„± ì¬íŒë‹¨
    multi_query_needed = False

    if q_type == 'simple':
        multi_query_needed = False

    elif q_type == 'normal':
        # ë¹„êµ í‚¤ì›Œë“œê°€ ìˆì„ ë•Œë§Œ
        comparison_keywords = ['ì°¨ì´', 'ë¹„êµ', 'vs', 'ëŒ€ì¡°', 'ë‹¤ë¥¸ì ']
        if any(kw in question for kw in comparison_keywords):
            multi_query_needed = True
        else:
            multi_query_needed = False  # â† ë³€ê²½!

    elif q_type == 'complex':
        # ë‹¤ê°ë„ í‚¤ì›Œë“œê°€ ìˆì„ ë•Œë§Œ
        multi_angle_keywords = ['ëª¨ë“ ', 'ì „ì²´', 'ë‹¤ì–‘í•œ', 'ì—¬ëŸ¬', 'ì¢…í•©']
        if any(kw in question for kw in multi_angle_keywords):
            multi_query_needed = True
        else:
            multi_query_needed = False  # â† ë³€ê²½!

    elif q_type == 'exhaustive':
        multi_query_needed = True

    return {
        'type': q_type,
        'multi_query': multi_query_needed,
        # ...
    }


# ì˜ˆìƒ íš¨ê³¼:
# Multi-query ì‚¬ìš©ë¥ : 60% â†’ 20%
# í‰ê·  ì‘ë‹µ ì‹œê°„: 120ì´ˆ â†’ 50ì´ˆ
```

**êµ¬í˜„ ë‹¨ê³„**:
1. Day 3 ì˜¤í›„: ë¶„ë¥˜ ë¡œì§ ìˆ˜ì • (2ì‹œê°„)
2. Day 4 ì˜¤ì „: í…ŒìŠ¤íŠ¸ ì¬ì‹¤í–‰ (1ì‹œê°„)
3. Day 4 ì˜¤í›„: ê²°ê³¼ ë¶„ì„ (2ì‹œê°„)

**ì˜ˆìƒ íš¨ê³¼**:
```
Before:
  - 70ê°œ í…ŒìŠ¤íŠ¸ ì¤‘ 42ê°œ (60%) Multi-query
  - í‰ê·  120ì´ˆ

After:
  - 70ê°œ í…ŒìŠ¤íŠ¸ ì¤‘ 14ê°œ (20%) Multi-query
  - í‰ê· : (56 * 25ì´ˆ + 14 * 180ì´ˆ) / 70 = 55ì´ˆ

ê°œì„ : 120ì´ˆ â†’ 55ì´ˆ (54% ê°ì†Œ)
```

---

#### Solution 2: Embedding ë°°ì¹˜ ì²˜ë¦¬ (Day 4-5)

```python
# utils/embeddings.py ê°œì„ 

class EmbeddingManager:
    def __init__(self):
        self.cache = {}

    def embed_batch(self, texts: List[str]):
        """
        ë°°ì¹˜ Embedding

        í˜„ì¬: ê° í…ìŠ¤íŠ¸ë§ˆë‹¤ API í˜¸ì¶œ
        ê°œì„ : í•œë²ˆì— ë°°ì¹˜ ì²˜ë¦¬
        """

        # ìºì‹œ í™•ì¸
        uncached = [t for t in texts if t not in self.cache]

        if uncached:
            # ë°°ì¹˜ API í˜¸ì¶œ
            embeddings = self.api_client.embed(uncached)

            for text, emb in zip(uncached, embeddings):
                self.cache[text] = emb

        return [self.cache[t] for t in texts]


# utils/rag_chain.py ìˆ˜ì •

def _multi_query_search(self, expanded_queries):
    """
    Multi-query ê²€ìƒ‰ ìµœì í™”
    """

    # Before: ê° ì¿¼ë¦¬ë§ˆë‹¤ embedding
    # for q in expanded_queries:
    #     emb = self.embedder.embed(q)  # API í˜¸ì¶œ * N
    #     results.append(self.search(emb))

    # After: ë°°ì¹˜ embedding
    embeddings = self.embedder.embed_batch(expanded_queries)

    results = []
    for q, emb in zip(expanded_queries, embeddings):
        results.append(self.search(emb))

    return results


# ì˜ˆìƒ íš¨ê³¼:
# Embedding ì‹œê°„: (0.5ì´ˆ * 3) = 1.5ì´ˆ â†’ 0.8ì´ˆ (ë°°ì¹˜)
# Multi-query ì‹œê°„: 180ì´ˆ â†’ 150ì´ˆ (17% ê°ì†Œ)
```

**êµ¬í˜„ ë‹¨ê³„**:
1. Day 4 ì˜¤í›„: Embedding ìºì‹œ/ë°°ì¹˜ êµ¬í˜„ (3ì‹œê°„)
2. Day 5 ì˜¤ì „: í†µí•© í…ŒìŠ¤íŠ¸ (2ì‹œê°„)
3. Day 5 ì˜¤í›„: ì„±ëŠ¥ ì¸¡ì • (2ì‹œê°„)

**ì˜ˆìƒ íš¨ê³¼**:
```
Multi-query ì‹œê°„: 180ì´ˆ â†’ 150ì´ˆ
ì „ì²´ í‰ê· : 55ì´ˆ â†’ 48ì´ˆ

ì¶”ê°€ ì´ë“:
  - Embedding ìºì‹œë¡œ ë°˜ë³µ ì§ˆë¬¸ ë¹ ë¦„
  - API ë¹„ìš© ì ˆê°
```

---

#### Solution 3: íƒ€ì„ì•„ì›ƒ ì„¤ì • (Day 5)

```python
# utils/rag_chain.py ì¶”ê°€

def query(self, question, max_time=60):
    """
    íƒ€ì„ì•„ì›ƒ ì„¤ì •

    60ì´ˆ ì´ˆê³¼ ì‹œ Fallbackìœ¼ë¡œ ì „í™˜
    """

    start = time.time()

    try:
        # ê¸°ì¡´ ë¡œì§
        result = self._execute_query(question)

        elapsed = time.time() - start
        if elapsed > max_time:
            print(f"[WARN] Query took {elapsed}s, exceeds limit")

        return result

    except TimeoutError:
        # Fallback: Simple modeë¡œ ì¬ì‹œë„
        print(f"[TIMEOUT] Falling back to simple mode")
        return self._simple_query(question)


# ì˜ˆìƒ íš¨ê³¼:
# - ê·¹ë‹¨ì  ì¼€ì´ìŠ¤ (472ì´ˆ) ë°©ì§€
# - ì‚¬ìš©ì ê²½í—˜ ê°œì„ 
```

---

## ğŸ“Š í†µí•© íš¨ê³¼ ë¶„ì„

### Before (í˜„ì¬)
```
ì¢…í•© ì ìˆ˜: 73.1/100

ë¬¸ì„œ ì í•©ì„±:   50.9/100 âš ï¸
  - unique_docs: 1.2ê°œ í‰ê· 
  - ë‹¨ì¼ ë¬¸ì„œ ì˜ì¡´: 90%

ë‹µë³€ ì™„ì „ì„±:   77.4/100 âœ“
ì²˜ë¦¬ ëª…í™•ì„±:   92.8/100 âœ“âœ“
í™˜ê° ë°©ì§€:     76.6/100 âœ“

í‰ê·  ì‘ë‹µì‹œê°„: 120ì´ˆ
  - Fallback: 25ì´ˆ
  - Multi-query: 180ì´ˆ
  - Multi-query ì‚¬ìš©ë¥ : 60%
```

### After Week 1 (ì˜ˆìƒ)
```
ì¢…í•© ì ìˆ˜: 85.0/100 (â†‘ 11.9ì )

ë¬¸ì„œ ì í•©ì„±:   75.0/100 âœ“  (â†‘ 24.1ì )
  - unique_docs: 3.5ê°œ í‰ê· 
  - ë‹¤ì–‘ì„± ë¹„ìœ¨: 70%
  - Solution 1 (Diversity Penalty) íš¨ê³¼

ë‹µë³€ ì™„ì „ì„±:   85.0/100 âœ“  (â†‘ 7.6ì )
  - ë” ë‹¤ì–‘í•œ ì¶œì²˜ â†’ ë” í’ë¶€í•œ ë‹µë³€

ì²˜ë¦¬ ëª…í™•ì„±:   95.0/100 âœ“âœ“ (â†‘ 2.2ì )
  - ë¡œê¹… ê°œì„ 

í™˜ê° ë°©ì§€:     80.0/100 âœ“  (â†‘ 3.4ì )
  - ë‹¤ì–‘í•œ ì¶œì²˜ â†’ ê²€ì¦ ê°•í™”

í‰ê·  ì‘ë‹µì‹œê°„: 40ì´ˆ (â†“ 67%)
  - Fallback: 25ì´ˆ (ë³€í™” ì—†ìŒ)
  - Multi-query: 150ì´ˆ (â†“ 17%)
  - Multi-query ì‚¬ìš©ë¥ : 20% (â†“ 67%)
```

### ì„¸ë¶€ íš¨ê³¼ ë¶„í•´

#### ë¬¸ì„œ ì í•©ì„± ê°œì„ : 50.9 â†’ 75.0 (+24.1ì )
```
ê¸°ì—¬ ìš”ì¸:
  - Diversity Penalty: +18ì  (í•µì‹¬)
  - ë‹¤ì–‘í•œ ë¬¸ì„œë¡œ ë‹µë³€ í’ë¶€: +4ì 
  - Relevance ì•½ê°„ í¬ìƒ: -2ì 
  - ì¶”ê°€ íŠœë‹ ì—¬ì§€: +4ì 

ê²€ì¦ ë°©ë²•:
  - unique_docs ë©”íŠ¸ë¦­: 1.2 â†’ 3.5
  - Deep Quality Assessment ì¬ì‹¤í–‰
```

#### ì‘ë‹µ ì‹œê°„ ê°œì„ : 120ì´ˆ â†’ 40ì´ˆ (-67%)
```
ê¸°ì—¬ ìš”ì¸:
  - Multi-query ì‚¬ìš© ê°ì†Œ: -50ì´ˆ (í•µì‹¬)
  - Embedding ë°°ì¹˜: -10ì´ˆ
  - ê¸°íƒ€ ìµœì í™”: -20ì´ˆ

ê²€ì¦ ë°©ë²•:
  - í…ŒìŠ¤íŠ¸ 70ê°œ ì´ ì‹œê°„ ì¸¡ì •
  - Phaseë³„ í‰ê·  ì‹œê°„ ë¹„êµ
```

---

## ğŸ¯ ì„±ê³µ ì§€í‘œ (KPI)

### Primary Metrics
1. **ë¬¸ì„œ ì í•©ì„±**: 50.9 â†’ 75+ (ëª©í‘œ ë‹¬ì„±)
2. **í‰ê·  ì‘ë‹µì‹œê°„**: 120ì´ˆ â†’ 40ì´ˆ (ëª©í‘œ ë‹¬ì„±)
3. **unique_docs**: 1.2 â†’ 3.5+ (ëª©í‘œ ë‹¬ì„±)

### Secondary Metrics
4. **ì¢…í•© ì ìˆ˜**: 73.1 â†’ 85+ (Bë“±ê¸‰ ë‹¬ì„±)
5. **ë‹µë³€ ì™„ì „ì„±**: 77.4 â†’ 85+ (ê°œì„ )
6. **Multi-query ì‚¬ìš©ë¥ **: 60% â†’ 20% (ìµœì í™”)

### ê²€ì¦ ë°©ë²•
```bash
# Week 1 ì™„ë£Œ í›„ ì¬í…ŒìŠ¤íŠ¸
python run_comprehensive_test_real.py \
  --test-cases test_cases_balanced.json \
  --config config_test.json \
  --output-dir test_logs_week1_validation

# Deep Quality Assessment
python deep_quality_assessment.py \
  --test-logs-dir test_logs_week1_validation \
  --output week1_validation_report.json

# ë¹„êµ
python compare_reports.py \
  --before deep_quality_report_balanced.json \
  --after week1_validation_report.json
```

---

## âš ï¸ ë¦¬ìŠ¤í¬ ë° ëŒ€ì‘ ë°©ì•ˆ

### Risk 1: Diversity Penaltyê°€ Relevanceë¥¼ ê³¼ë„í•˜ê²Œ í¬ìƒ
**ì¦ìƒ**: ë¬¸ì„œ ë‹¤ì–‘ì„±ì€ ì¦ê°€í•˜ì§€ë§Œ ë‹µë³€ í’ˆì§ˆ ì €í•˜
**ëŒ€ì‘**:
```python
# íŒŒë¼ë¯¸í„° ì¡°ì •
penalty_strength = 0.3  # ê¸°ë³¸
if answer_quality < 75:
    penalty_strength = 0.2  # ì™„í™”
```

### Risk 2: Multi-query ê°ì†Œë¡œ ë³µì¡í•œ ì§ˆë¬¸ í’ˆì§ˆ ì €í•˜
**ì¦ìƒ**: Complex ì§ˆë¬¸ì˜ ë‹µë³€ ì™„ì „ì„± ê°ì†Œ
**ëŒ€ì‘**:
```python
# ë¡¤ë°± ì˜µì…˜
if complex_question_score < 80:
    # Multi-query ì¡°ê±´ ì™„í™”
    multi_query_threshold = 0.6  # ê¸°ë³¸ 0.7ì—ì„œ ë‚®ì¶¤
```

### Risk 3: Embedding ë°°ì¹˜ ì²˜ë¦¬ê°€ ë©”ëª¨ë¦¬ ì´ˆê³¼
**ì¦ìƒ**: OOM ì—ëŸ¬
**ëŒ€ì‘**:
```python
# ë°°ì¹˜ í¬ê¸° ì œí•œ
max_batch_size = 10
batches = chunk_list(queries, max_batch_size)
```

---

## ğŸ“… ìƒì„¸ ì¼ì •

### Day 1 (ì›”ìš”ì¼)
**ì˜¤ì „ (4ì‹œê°„)**
- [x] í˜„ì¬ ë¡œê·¸ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± (1ì‹œê°„)
- [ ] ì‹¤ì œ ë¬¸ì œ ì›ì¸ íŒŒì•… (1ì‹œê°„)
- [ ] Diversity Penalty ì½”ë“œ ì‘ì„± (2ì‹œê°„)

**ì˜¤í›„ (4ì‹œê°„)**
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„± (1ì‹œê°„)
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ (1ì‹œê°„)
- [ ] íŒŒë¼ë¯¸í„° ì´ˆê¸° íŠœë‹ (2ì‹œê°„)

### Day 2 (í™”ìš”ì¼)
**ì˜¤ì „ (4ì‹œê°„)**
- [ ] Balanced í…ŒìŠ¤íŠ¸ ì¬ì‹¤í–‰ (1ì‹œê°„)
- [ ] ê²°ê³¼ ë¶„ì„ (1ì‹œê°„)
- [ ] ë¬¸ì œì  ìˆ˜ì • (2ì‹œê°„)

**ì˜¤í›„ (4ì‹œê°„)**
- [ ] íŒŒë¼ë¯¸í„° Fine-tuning (2ì‹œê°„)
- [ ] Comprehensive í…ŒìŠ¤íŠ¸ ì¬ì‹¤í–‰ (2ì‹œê°„)

### Day 3 (ìˆ˜ìš”ì¼)
**ì˜¤ì „ (4ì‹œê°„)**
- [ ] Week 1 ì¤‘ê°„ ì ê²€
- [ ] Diversity íš¨ê³¼ ê²€ì¦ (2ì‹œê°„)
- [ ] Multi-query ë¶„ë¥˜ ë¡œì§ ìˆ˜ì • (2ì‹œê°„)

**ì˜¤í›„ (4ì‹œê°„)**
- [ ] Multi-query í…ŒìŠ¤íŠ¸ (2ì‹œê°„)
- [ ] ì‘ë‹µ ì‹œê°„ ì¸¡ì • (2ì‹œê°„)

### Day 4 (ëª©ìš”ì¼)
**ì˜¤ì „ (4ì‹œê°„)**
- [ ] Multi-query ê²°ê³¼ ë¶„ì„ (2ì‹œê°„)
- [ ] ì¡°ê±´ íŠœë‹ (2ì‹œê°„)

**ì˜¤í›„ (4ì‹œê°„)**
- [ ] Embedding ë°°ì¹˜ ì²˜ë¦¬ êµ¬í˜„ (3ì‹œê°„)
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (1ì‹œê°„)

### Day 5 (ê¸ˆìš”ì¼)
**ì˜¤ì „ (4ì‹œê°„)**
- [ ] Embedding í†µí•© í…ŒìŠ¤íŠ¸ (2ì‹œê°„)
- [ ] ì„±ëŠ¥ ì¸¡ì • (2ì‹œê°„)

**ì˜¤í›„ (4ì‹œê°„)**
- [ ] ì „ì²´ ì¬í…ŒìŠ¤íŠ¸ (2ì‹œê°„)
- [ ] Week 1 ê²°ê³¼ ë³´ê³ ì„œ ì‘ì„± (2ì‹œê°„)

---

## ğŸ“ˆ ì˜ˆìƒ ROI

### íˆ¬ì… ìì›
```
ê°œë°œ ì‹œê°„: 5ì¼ (40ì‹œê°„)
í…ŒìŠ¤íŠ¸ ì‹œê°„: ê³„ì‚° ë¦¬ì†ŒìŠ¤ (ì•½ 3ì‹œê°„ * 5íšŒ)
ë¦¬ìŠ¤í¬: ë‚®ìŒ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€, ì ì§„ì  ê°œì„ )
```

### ì˜ˆìƒ íš¨ê³¼
```
ì •ëŸ‰ì :
  - ë¬¸ì„œ ì í•©ì„±: +24.1ì  (48% ê°œì„ )
  - ì‘ë‹µ ì‹œê°„: -80ì´ˆ (67% ê°œì„ )
  - ì‚¬ìš©ì ë§Œì¡±ë„: +30% (ì¶”ì •)

ì •ì„±ì :
  - RAGì˜ í•µì‹¬ ê°€ì¹˜ íšŒë³µ (Multi-document synthesis)
  - ì‚¬ìš©ì ê²½í—˜ ê°œì„  (ë¹ ë¥¸ ì‘ë‹µ)
  - ì‹œìŠ¤í…œ ì‹ ë¢°ë„ ì¦ê°€
```

### ROI ê³„ì‚°
```
Before: 73.1/100, 120ì´ˆ
After:  85.0/100, 40ì´ˆ

í’ˆì§ˆ í–¥ìƒ: 16% (73.1 â†’ 85.0)
ì†ë„ í–¥ìƒ: 67% (120ì´ˆ â†’ 40ì´ˆ)
íˆ¬ì… ì‹œê°„: 40ì‹œê°„

ROI = (í’ˆì§ˆí–¥ìƒ + ì†ë„í–¥ìƒ) / íˆ¬ì…ì‹œê°„
    = (16 + 67) / 40
    = 2.08% per hour

ë§¤ìš° ë†’ì€ ROI!
```

---

## ğŸš€ ì‹œì‘ ë°©ë²•

### ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì²« ë‹¨ê³„
```bash
# 1. í˜„ì¬ ìƒíƒœ ë°±ì—…
cp -r utils utils_backup_20251111

# 2. ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python analyze_retrieval_diversity.py \
  --logs test_logs_comprehensive_full

# 3. ê²°ê³¼ í™•ì¸ í›„ êµ¬í˜„ ì‹œì‘
# â†’ diversity_penalty ë¸Œëœì¹˜ ìƒì„±
git checkout -b feature/diversity-penalty
```

---

**ì§ˆë¬¸**: ì´ ê³„íšìœ¼ë¡œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? ì•„ë‹ˆë©´ íŠ¹ì • ë¶€ë¶„ì„ ë¨¼ì € ê²€ì¦í•´ë³¼ê¹Œìš”?
