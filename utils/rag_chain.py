from typing import List, Dict, Any, Optional, Iterator
from langchain_ollama import OllamaLLM
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from utils.reranker import get_reranker
from utils.request_llm import RequestLLM
import json
import re


class RAGChain:
    def __init__(self, vectorstore, 
                 llm_api_type: str = "ollama",
                 llm_base_url: str = "http://localhost:11434", 
                 llm_model: str = "llama3",
                 llm_api_key: str = "",
                 temperature: float = 0.3,
                 top_k: int = 3,
                 use_reranker: bool = True,
                 reranker_model: str = "multilingual-mini",
                 reranker_initial_k: int = 20,
                 enable_synonym_expansion: bool = True,
                 enable_multi_query: bool = True):
        self.llm_api_type = llm_api_type
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.llm_api_key = llm_api_key
        self.temperature = temperature
        self.top_k = top_k
        self.vectorstore = vectorstore
        
        # Re-ranker ÏÑ§Ï†ï (Í∏∞Î≥∏ ÌôúÏÑ±Ìôî)
        self.use_reranker = use_reranker
        self.reranker_model = reranker_model
        self.reranker_initial_k = max(reranker_initial_k, top_k * 5)
        
        # Re-ranker Ï¥àÍ∏∞Ìôî (ÏÇ¨Ïö© Ïãú)
        if self.use_reranker:
            self.reranker = get_reranker(model_name=reranker_model)
        
        # ÎßàÏßÄÎßâ Í≤ÄÏÉâ Í≤∞Í≥º Ï∫êÏãú (Ï∂úÏ≤ò ÌëúÏãúÏö©)
        self._last_retrieved_docs = []
        
        # LLM Ï¥àÍ∏∞Ìôî - API ÌÉÄÏûÖÏóê Îî∞Îùº Îã§Î•∏ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÇ¨Ïö©
        self.llm = self._create_llm()
        
        # ÎèôÏùòÏñ¥ ÌôïÏû• ÏÑ§Ï†ï
        self.enable_synonym_expansion = enable_synonym_expansion
        self.enable_multi_query = enable_multi_query
        
        # Retriever ÏÑ§Ï†ï - vectorstoreÎäî Ïù¥ÎØ∏ Chroma Ïù∏Ïä§ÌÑ¥Ïä§
        self.retriever = vectorstore.as_retriever(
            search_kwargs={"k": max(self.top_k * 8, 24)}
        )
        
        # ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø - ÎåÄÌôî Ïù¥Î†• Ìè¨Ìï® Î≤ÑÏ†Ñ
        self.prompt_template = """ÎãπÏã†ÏùÄ ÏπúÏ†àÌïòÍ≥† Ï†ÑÎ¨∏Ï†ÅÏù∏ AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§. Ï†úÍ≥µÎêú Î¨∏ÏÑú ÎÇ¥Ïö©Í≥º Ïù¥Ï†Ñ ÎåÄÌôî ÎÇ¥Ïö©ÏùÑ Í∏∞Î∞òÏúºÎ°ú ÏÇ¨Ïö©ÏûêÏùò ÏßàÎ¨∏Ïóê Ï†ïÌôïÌïòÍ≥† ÏÉÅÏÑ∏ÌïòÍ≤å ÎãµÎ≥ÄÌï¥Ï£ºÏÑ∏Ïöî.

Ïù¥Ï†Ñ ÎåÄÌôî ÎÇ¥Ïö©:
{chat_history}

Ï∞∏Í≥† Î¨∏ÏÑú:
{context}

ÌòÑÏû¨ ÏßàÎ¨∏: {question}

ÎãµÎ≥Ä ÏßÄÏπ®:
1. Ïù¥Ï†Ñ ÎåÄÌôî ÎÇ¥Ïö©ÏùÑ Ï∞∏Í≥†ÌïòÏó¨ Î¨∏Îß•Ïóê ÎßûÎäî ÎãµÎ≥ÄÏùÑ Ï†úÍ≥µÌïòÏÑ∏Ïöî.
2. Î¨∏ÏÑúÏóêÏÑú Ï∞æÏùÄ Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°ú Î™ÖÌôïÌïòÍ≥† ÏûêÏÑ∏ÌïòÍ≤å ÏÑ§Î™ÖÌïòÏÑ∏Ïöî.
3. Í∞ÄÎä•Ìïú Í≤ΩÏö∞ Î¨∏ÏÑúÏùò Ïñ¥Îäê Î∂ÄÎ∂ÑÏóêÏÑú Ï†ïÎ≥¥Î•º ÏñªÏóàÎäîÏßÄ ÏûêÏó∞Ïä§ÎüΩÍ≤å Ïñ∏Í∏âÌïòÏÑ∏Ïöî.
4. Î¨∏ÏÑúÏóê Í¥ÄÎ†® Ï†ïÎ≥¥Í∞Ä ÏûàÎã§Î©¥ Ï∂îÎ°†ÌïòÏó¨ ÎèÑÏõÄÏù¥ ÎêòÎäî ÎãµÎ≥ÄÏùÑ Ï†úÍ≥µÌïòÏÑ∏Ïöî.
5. ÏûêÏ≤¥Ï†ÅÏúºÎ°ú Ï∂îÎ°†ÌïòÏó¨ ÎåÄÎãµÌïòÎäî ÎÇ¥Ïö©ÏùÄ Ï∂îÎ°†Ìïú ÎÇ¥Ïö©Ïù¥ÎùºÎäî Í≤ÉÏùÑ Ïñ∏Í∏âÌïòÏÑ∏Ïöî.
6. Î¨∏ÏÑúÏóê Ï†ÑÌòÄ Í¥ÄÎ†® ÏóÜÎäî ÎÇ¥Ïö©Îßå ÏûàÏùÑ Í≤ΩÏö∞ÏóêÎßå "Ï£ÑÏÜ°Ìï©ÎãàÎã§. Ï†úÍ≥µÎêú Î¨∏ÏÑúÏóêÏÑú Í¥ÄÎ†® Ï†ïÎ≥¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§"ÎùºÍ≥† ÎãµÎ≥ÄÌïòÏÑ∏Ïöî.
7. ÏπúÍ∑ºÌïòÎ©¥ÏÑúÎèÑ Ï†ÑÎ¨∏Ï†ÅÏù∏ ÌÜ§ÏúºÎ°ú ÎãµÎ≥ÄÌïòÏÑ∏Ïöî.

ÎãµÎ≥Ä:"""

        self.prompt = PromptTemplate(
            template=self.prompt_template,
            input_variables=["chat_history", "context", "question"]
        )
        
        # LCEL Î∞©ÏãùÏúºÎ°ú Ï≤¥Ïù∏ Íµ¨ÏÑ± (ÎåÄÌôî Ïù¥Î†• Ìè¨Ìï®)
        self.chain = (
            {
                "context": lambda x: self._get_context(x["question"]),
                "chat_history": lambda x: x.get("chat_history", "Ïù¥Ï†Ñ ÎåÄÌôî ÏóÜÏùå"),
                "question": lambda x: x["question"]
            }
            | self.prompt
            | self.llm
            | StrOutputParser()
        )

    def _create_llm(self):
        """API ÌÉÄÏûÖÏóê Îî∞Îùº Ï†ÅÏ†àÌïú LLM ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÉùÏÑ±"""
        if self.llm_api_type == "request":
            return RequestLLM(
                base_url=self.llm_base_url,
                model=self.llm_model,
                temperature=self.temperature,
                timeout=60
            )
        elif self.llm_api_type == "ollama":
            return OllamaLLM(
                base_url=self.llm_base_url,
                model=self.llm_model,
                temperature=self.temperature
            )
        elif self.llm_api_type == "openai":
            kwargs = {
                "model": self.llm_model,
                "temperature": self.temperature,
                "api_key": self.llm_api_key if self.llm_api_key else "not-needed"
            }
            return ChatOpenAI(**kwargs)
        elif self.llm_api_type == "openai-compatible":
            kwargs = {
                "model": self.llm_model,
                "temperature": self.temperature,
                "base_url": self.llm_base_url,
                "api_key": self.llm_api_key if self.llm_api_key else "not-needed"
            }
            return ChatOpenAI(**kwargs)
        else:
            raise ValueError(f"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî API ÌÉÄÏûÖ: {self.llm_api_type}")

    def _format_docs(self, docs: List[Document]) -> str:
        return "\n\n".join([
            f"Î¨∏ÏÑú {i+1} (Ï∂úÏ≤ò: {doc.metadata.get('file_name', 'Unknown')}, "
            f"ÌéòÏù¥ÏßÄ: {doc.metadata.get('page_number', 'Unknown')}):\n{doc.page_content}"
            for i, doc in enumerate(docs)
        ])

    def _unique_by_file(self, pairs: List[tuple], k: int) -> List[tuple]:
        """(Document, score) Î¶¨Ïä§Ìä∏ÏóêÏÑú ÌååÏùºÎ™Ö Í∏∞Ï§ÄÏúºÎ°ú Ï§ëÎ≥µÏùÑ Ï†úÍ±∞ÌïòÎ©∞ ÏµúÎåÄ kÍ∞ú Î∞òÌôò"""
        seen = set()
        results: List[tuple] = []
        for doc, score in pairs:
            file_name = doc.metadata.get("file_name", "")
            if file_name in seen:
                continue
            seen.add(file_name)
            results.append((doc, score))
            if len(results) >= k:
                break
        return results

    def _search_candidates(self, question: str) -> List[tuple]:
        """ÌïòÏù¥Î∏åÎ¶¨Îìú(ÌÇ§ÏõåÎìú+Î≤°ÌÑ∞) ‚Üí Re-ranker ÏûÖÎ†• ÌõÑÎ≥¥ ÌôïÎ≥¥"""
        try:
            # ÌïòÏù¥Î∏åÎ¶¨ÎìúÎ°ú ÎÑâÎÑâÌûà ÌõÑÎ≥¥ ÌôïÎ≥¥
            hybrid = self.vectorstore.similarity_search_hybrid(
                question, initial_k=max(self.top_k * 8, 40), top_k=max(self.top_k * 8, 40)
            )
            return hybrid
        except Exception:
            # Ìè¥Î∞±: Î≤°ÌÑ∞ Í≤ÄÏÉâ
            return self.vectorstore.similarity_search_with_score(question, k=max(self.top_k * 8, 40))

    def _get_context(self, question: str) -> str:
        # Multi-Query Rewriting Ï†ÅÏö©
        if self.enable_multi_query:
            queries = self.generate_rewritten_queries(question, num_queries=3)
            all_retrieved_chunks = []
            chunk_id_set = set()
            
            # Î™®Îì† ÏøºÎ¶¨Ïóê ÎåÄÌï¥ Í≤ÄÏÉâ ÏàòÌñâ
            for query in queries:
                try:
                    if self.use_reranker:
                        base = self._search_candidates(query)
                        if base:
                            docs_for_rerank = [{
                                "page_content": d.page_content,
                                "metadata": d.metadata,
                                "vector_score": s,
                                "document": d
                            } for d, s in base]
                            reranked = self.reranker.rerank(query, docs_for_rerank, top_k=max(self.top_k * 3, 15))
                            results = [(d["document"], d.get("rerank_score", 0)) for d in reranked]
                        else:
                            results = []
                    else:
                        results = self.vectorstore.similarity_search_with_score(query, k=max(self.top_k * 3, 15))
                    
                    # Ï§ëÎ≥µ Ï†úÍ±∞ (Î¨∏ÏÑú ÎÇ¥Ïö© Í∏∞Ï§Ä)
                    for doc, score in results:
                        doc_id = f"{doc.metadata.get('source', '')}_{doc.page_content[:50]}"
                        if doc_id not in chunk_id_set:
                            all_retrieved_chunks.append((doc, score))
                            chunk_id_set.add(doc_id)
                            
                except Exception as e:
                    print(f"ÏøºÎ¶¨ '{query}' Í≤ÄÏÉâ Ïã§Ìå®: {e}")
                    continue
            
            if all_retrieved_chunks:
                # ÏõêÎ≥∏ ÏøºÎ¶¨Î°ú Ïû¨ÏàúÏúÑ Îß§ÍπÄ
                if self.use_reranker:
                    docs_for_final_rerank = [{
                        "page_content": d.page_content,
                        "metadata": d.metadata,
                        "vector_score": s,
                        "document": d
                    } for d, s in all_retrieved_chunks]
                    final_reranked = self.reranker.rerank(question, docs_for_final_rerank, top_k=max(self.top_k * 2, 20))
                    pairs = [(d["document"], d.get("rerank_score", 0)) for d in final_reranked]
                else:
                    pairs = all_retrieved_chunks
                
                dedup = self._unique_by_file(pairs, self.top_k)
                self._last_retrieved_docs = dedup
                docs = [d for d, _ in dedup]
                return self._format_docs(docs)
        
        # Ìè¥Î∞±: Îã®Ïùº ÏøºÎ¶¨ Í≤ÄÏÉâ (ÎèôÏùòÏñ¥ ÌôïÏû• Ìè¨Ìï®)
        expanded_question = self.expand_query_with_synonyms(question)
        
        if self.use_reranker:
            base = self._search_candidates(expanded_question)
            if not base:
                self._last_retrieved_docs = []
                return ""
            # base Îäî (doc, score) ÌòïÌÉú
            docs_for_rerank = [{
                "page_content": d.page_content,
                "metadata": d.metadata,
                "vector_score": s,
                "document": d
            } for d, s in base]
            reranked = self.reranker.rerank(expanded_question, docs_for_rerank, top_k=max(self.top_k * 8, 40))
            pairs = [(d["document"], d.get("rerank_score", 0)) for d in reranked]
            dedup = self._unique_by_file(pairs, self.top_k)
            
            # Ï∫êÏãú Ï†ÄÏû•: Ïã§Ï†ú ÏÇ¨Ïö©Îêú Î¨∏ÏÑúÏôÄ Ï†êÏàò
            self._last_retrieved_docs = dedup  # [(doc, score), ...]
            
            docs = [d for d, _ in dedup]
        else:
            pairs = self.vectorstore.similarity_search_with_score(expanded_question, k=max(self.top_k * 8, 40))
            dedup = self._unique_by_file(pairs, self.top_k)
            
            # Ï∫êÏãú Ï†ÄÏû•
            self._last_retrieved_docs = dedup
            
            docs = [d for d, _ in dedup]
        return self._format_docs(docs)

    def expand_query_with_synonyms(self, original_query: str) -> str:
        """LLMÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÏõêÎ≥∏ ÏøºÎ¶¨Ïóê ÎåÄÌïú ÎèôÏùòÏñ¥/Ïó∞Í¥ÄÏñ¥Î•º ÏÉùÏÑ±ÌïòÍ≥† ÌôïÏû•Îêú ÏøºÎ¶¨Î•º Î∞òÌôò"""
        if not self.enable_synonym_expansion:
            return original_query
            
        try:
            prompt = f"""
ÏÇ¨Ïö©ÏûêÏùò Í≤ÄÏÉâ ÏøºÎ¶¨: "{original_query}"

Ïù¥ ÏøºÎ¶¨ÏôÄ Í¥ÄÎ†®Îêú ÎèôÏùòÏñ¥ ÎòêÎäî Î∞ÄÏ†ëÌïòÍ≤å Ïó∞Í¥ÄÎêú Í≤ÄÏÉâ Ïö©Ïñ¥ 3Í∞úÎ•º ÏÉùÏÑ±Ìï¥ Ï§ò.
Í≤∞Í≥ºÎäî JSON Î¶¨Ïä§Ìä∏ ÌòïÏãùÏúºÎ°úÎßå ÏùëÎãµÌï¥ Ï§ò.
ÏòàÏãú: ["Ïö©Ïñ¥1", "Ïö©Ïñ¥2", "Ïö©Ïñ¥3"]
"""
            
            response = self.llm.invoke(prompt)
            
            # ÏùëÎãµÏùÑ Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò
            if hasattr(response, 'content'):
                response_text = response.content
            elif hasattr(response, 'text'):
                response_text = response.text
            else:
                response_text = str(response)
            
            # JSON ÌååÏã± ÏãúÎèÑ
            try:
                # ÏùëÎãµÏóêÏÑú JSON Î∂ÄÎ∂ÑÎßå Ï∂îÏ∂ú
                json_match = re.search(r'\[.*?\]', response_text)
                if json_match:
                    related_terms = json.loads(json_match.group())
                else:
                    # JSON ÌòïÏãùÏù¥ ÏïÑÎãå Í≤ΩÏö∞ ÌÖçÏä§Ìä∏ÏóêÏÑú Ï∂îÏ∂ú
                    lines = response_text.strip().split('\n')
                    related_terms = []
                    for line in lines:
                        line = line.strip().strip('"[]')
                        if line and len(line) > 1:
                            related_terms.append(line)
                    related_terms = related_terms[:3]  # ÏµúÎåÄ 3Í∞ú
                
                # ÏõêÎ≥∏ ÏøºÎ¶¨ÏôÄ Ïó∞Í¥ÄÏñ¥Î•º Í≤∞Ìï©
                if related_terms:
                    expanded_query = f"{original_query} (Í¥ÄÎ†® Ïö©Ïñ¥: {', '.join(related_terms)})"
                    print(f"üîç ÎèôÏùòÏñ¥ ÌôïÏû•: {original_query} ‚Üí {expanded_query}")
                    return expanded_query
                    
            except (json.JSONDecodeError, ValueError) as e:
                print(f"ÎèôÏùòÏñ¥ ÌååÏã± Ïã§Ìå®: {e}")
                
        except Exception as e:
            print(f"ÎèôÏùòÏñ¥ ÌôïÏû• Ïã§Ìå®: {e}")
        
        return original_query

    def generate_rewritten_queries(self, original_query: str, num_queries: int = 3) -> List[str]:
        """LLMÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÏõêÎ≥∏ ÏøºÎ¶¨Î•º Ïó¨Îü¨ Í¥ÄÏ†êÏóêÏÑú Ïû¨ÏûëÏÑ±Ìïú ÎåÄÏïà ÏøºÎ¶¨ Î¶¨Ïä§Ìä∏Î•º ÏÉùÏÑ±"""
        if not self.enable_multi_query:
            return [original_query]
            
        try:
            prompt = f"""
ÎãπÏã†ÏùÄ ÏÇ¨Ïö©ÏûêÏùò ÏßàÎ¨∏ÏùÑ Îçî ÎÇòÏùÄ Í≤ÄÏÉâ Í≤∞Í≥ºÎ°ú Ïù¥ÎÅÑÎäî Ï†ÑÎ¨∏ Í≤ÄÏÉâ ÏóîÏßÄÎãàÏñ¥ÏûÖÎãàÎã§.
Îã§Ïùå ÏõêÎ≥∏ ÏøºÎ¶¨Î•º {num_queries}Í∞úÏùò ÏÑúÎ°ú Îã§Î•∏ Í¥ÄÏ†êÏóêÏÑú Ïû¨ÏûëÏÑ±Ìï¥ Ï£ºÏã≠ÏãúÏò§.

- ÏõêÎ≥∏ ÏøºÎ¶¨Îäî Í∑∏ÎåÄÎ°ú Ïú†ÏßÄÌïòÏã≠ÏãúÏò§.
- ÏøºÎ¶¨Îì§ÏùÄ ÏÑúÎ°ú Îã§Î•∏ Ï†ëÍ∑º Î∞©Ïãù(Ïòà: Í∏∞Ïà†Ï†Å ÏßàÎ¨∏, Í∞úÎÖêÏ†Å ÏßàÎ¨∏, Î¨∏Ï†ú Ìï¥Í≤∞)ÏùÑ Î∞òÏòÅÌï¥Ïïº Ìï©ÎãàÎã§.

ÏõêÎ≥∏ ÏøºÎ¶¨: "{original_query}"

Í≤∞Í≥ºÎäî JSON Î¶¨Ïä§Ìä∏ ÌòïÏãùÏúºÎ°úÎßå ÏùëÎãµÌï¥ Ï£ºÏã≠ÏãúÏò§. 
ÏòàÏãú: ["ÏøºÎ¶¨1", "ÏøºÎ¶¨2", "ÏøºÎ¶¨3"]
"""
            
            response = self.llm.invoke(prompt)
            
            # ÏùëÎãµÏùÑ Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò
            if hasattr(response, 'content'):
                response_text = response.content
            elif hasattr(response, 'text'):
                response_text = response.text
            else:
                response_text = str(response)
            
            # JSON ÌååÏã± ÏãúÎèÑ
            try:
                # ÏùëÎãµÏóêÏÑú JSON Î∂ÄÎ∂ÑÎßå Ï∂îÏ∂ú
                json_match = re.search(r'\[.*?\]', response_text)
                if json_match:
                    rewritten_queries = json.loads(json_match.group())
                else:
                    # JSON ÌòïÏãùÏù¥ ÏïÑÎãå Í≤ΩÏö∞ ÌÖçÏä§Ìä∏ÏóêÏÑú Ï∂îÏ∂ú
                    lines = response_text.strip().split('\n')
                    rewritten_queries = []
                    for line in lines:
                        line = line.strip().strip('"[]')
                        if line and len(line) > 1:
                            rewritten_queries.append(line)
                    rewritten_queries = rewritten_queries[:num_queries]  # ÏµúÎåÄ num_queriesÍ∞ú
                
                # ÏõêÎ≥∏ ÏøºÎ¶¨Í∞Ä Ìè¨Ìï®ÎêòÏßÄ ÏïäÏïòÎã§Î©¥, Î¶¨Ïä§Ìä∏Ïùò Îß® ÏïûÏóê Ï∂îÍ∞Ä
                if original_query not in rewritten_queries:
                    rewritten_queries.insert(0, original_query)
                    
                print(f"üîÑ Îã§Ï§ë ÏøºÎ¶¨ ÏÉùÏÑ±: {original_query} ‚Üí {len(rewritten_queries)}Í∞ú ÏøºÎ¶¨")
                return rewritten_queries
                    
            except (json.JSONDecodeError, ValueError) as e:
                print(f"Îã§Ï§ë ÏøºÎ¶¨ ÌååÏã± Ïã§Ìå®: {e}")
                
        except Exception as e:
            print(f"Îã§Ï§ë ÏøºÎ¶¨ ÏÉùÏÑ± Ïã§Ìå®: {e}")
        
        return [original_query]

    def _format_chat_history(self, messages: List[Dict[str, str]], max_messages: int = 5) -> str:
        if not messages:
            return "Ïù¥Ï†Ñ ÎåÄÌôî ÏóÜÏùå"
        recent_messages = messages[-max_messages * 2:] if len(messages) > max_messages * 2 else messages
        formatted = []
        for msg in recent_messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            if role == "user":
                formatted.append(f"ÏÇ¨Ïö©Ïûê: {content}")
            elif role == "assistant":
                formatted.append(f"Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏: {content}")
        return "\n".join(formatted) if formatted else "Ïù¥Ï†Ñ ÎåÄÌôî ÏóÜÏùå"

    def query(self, question: str, chat_history: List[Dict[str, str]] = None) -> Dict[str, Any]:
        try:
            formatted_history = self._format_chat_history(chat_history or [])
            answer = self.chain.invoke({
                "question": question,
                "chat_history": formatted_history
            })
            docs_with_scores = self.vectorstore.similarity_search_with_score(
                question, k=max(self.top_k * 5, 20)
            )
            dedup = self._unique_by_file(docs_with_scores, self.top_k)
            probs = self._normalize_scores(dedup, is_reranker=False)
            sources = []
            for (doc, score), p in zip(dedup, probs):
                if p < 15.0:
                    continue
                    source_info = {
                        "file_name": doc.metadata.get("file_name", "Unknown"),
                        "page_number": doc.metadata.get("page_number", "Unknown"),
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "similarity_score": float(round(p, 1))
                    }
                    sources.append(source_info)
            return {
                "answer": answer,
                "sources": sources[: self.top_k],
                "success": True
            }
        except Exception as e:
            return {
                "answer": f"Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}",
                "sources": [],
                "success": False
            }
    
    def query_stream(self, question: str, chat_history: List[Dict[str, str]] = None) -> Iterator[str]:
        try:
            formatted_history = self._format_chat_history(chat_history or [])
            for chunk in self.chain.stream({
                "question": question,
                "chat_history": formatted_history
            }):
                yield chunk
        except Exception as e:
            yield f"Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {str(e)}"
    
    def get_source_documents(self, question: str = None) -> List[Dict[str, Any]]:
        """Ï∫êÏãúÎêú Í≤ÄÏÉâ Í≤∞Í≥ºÎ•º Ï∂úÏ≤òÎ°ú Î∞òÌôò (ÎãµÎ≥Ä ÏÉùÏÑ±Ïóê Ïã§Ï†ú ÏÇ¨Ïö©Îêú Î¨∏ÏÑú)"""
        try:
            if not self._last_retrieved_docs:
                return []
            
            # Ï∫êÏãúÎêú Î¨∏ÏÑúÏóê Ï†êÏàò Ï†ïÍ∑úÌôî Ï†ÅÏö©
            is_reranker = self.use_reranker
            probs = self._normalize_scores(self._last_retrieved_docs, is_reranker=is_reranker)
            
            sources = []
            for (doc, raw_score), normalized_score in zip(self._last_retrieved_docs, probs):
                # 15% ÏûÑÍ≥ÑÍ∞í Ï†úÍ±∞ - Ïã§Ï†ú ÏÇ¨Ïö©Îêú Î¨∏ÏÑúÎäî Î™®Îëê ÌëúÏãú
                sources.append({
                    "file_name": doc.metadata.get("file_name", "Unknown"),
                    "page_number": doc.metadata.get("page_number", "Unknown"),
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "similarity_score": float(round(normalized_score, 1)),
                    "raw_score": float(round(raw_score, 4))  # ÎîîÎ≤ÑÍπÖÏö©
                })
            
            return sources
        except Exception as e:
            print(f"Ï∂úÏ≤ò Î¨∏ÏÑú Í≤ÄÏÉâ Ïã§Ìå®: {e}")
            return []
    
    def clear_memory(self):
        pass
    
    def update_llm(self, llm_api_type: str, llm_base_url: str, llm_model: str, 
                   llm_api_key: str = "", temperature: float = 0.7):
        self.llm_api_type = llm_api_type
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.llm_api_key = llm_api_key
        self.temperature = temperature
        self.llm = self._create_llm()
        self.chain = (
            {
                "context": lambda x: self._get_context(x["question"]),
                "chat_history": lambda x: x.get("chat_history", "Ïù¥Ï†Ñ ÎåÄÌôî ÏóÜÏùå"),
                "question": lambda x: x["question"]
            }
            | self.prompt
            | self.llm
            | StrOutputParser()
        )
    
    def update_retriever(self, vectorstore, top_k: int = 3):
        self.vectorstore = vectorstore
        self.top_k = top_k
        self.retriever = vectorstore.as_retriever(
            search_kwargs={"k": max(top_k * 5, 20)}
        )
        self.chain = (
            {
                "context": lambda x: self._get_context(x["question"]),
                "chat_history": lambda x: x.get("chat_history", "Ïù¥Ï†Ñ ÎåÄÌôî ÏóÜÏùå"),
                "question": lambda x: x["question"]
            }
            | self.prompt
            | self.llm
            | StrOutputParser()
        )

    def _to_percentage(self, scores: List[float], is_reranker: bool) -> List[float]:
        """Ï†êÏàò Î¶¨Ïä§Ìä∏Î•º 0~100%Î°ú Ï†ïÍ∑úÌôî"""
        if not scores:
            return []
        if is_reranker:
            mn = min(scores)
            mx = max(scores)
            if abs(mx - mn) < 1e-9:
                return [50.0 for _ in scores]
            return [max(0.0, min(100.0, (s - mn) / (mx - mn) * 100.0)) for s in scores]
        # Î≤°ÌÑ∞ Í≤ÄÏÉâ: Í±∞Î¶¨Í∞Ä 0~2 (ÏûëÏùÑÏàòÎ°ù Ïú†ÏÇ¨) Í∞ÄÏ†ï ‚Üí Ïú†ÏÇ¨ÎèÑÎ°ú Î≥ÄÌôò
        return [max(0.0, min(100.0, (2.0 - s) / 2.0 * 100.0)) for s in scores]

    def _normalize_scores(self, pairs: List[tuple], is_reranker: bool) -> List[float]:
        """(doc, raw_score) -> 0~100% ÌôïÎ•†Ìòï Ï†êÏàòÎ°ú Î≥¥Ï†ï
        - reranker: softmax(raw)
        - vector:   softmax(sim) with sim=exp(-alpha*distance), alpha=1.5
        """
        import math
        if not pairs:
            return []
        raw = [s for _, s in pairs]
        if is_reranker:
            mx = max(raw)
            exps = [math.exp(s - mx) for s in raw]
        else:
            # distance -> similarity (ÌÅ∞Í∞í Ï¢ãÍ≤å) ÌõÑ softmax
            alpha = 1.5
            sims = [math.exp(-alpha * max(0.0, s)) for s in raw]
            mx = max(sims)
            exps = [math.exp(v - mx) for v in sims]
        Z = sum(exps) or 1.0
        probs = [min(100.0, max(0.0, 100.0 * v / Z)) for v in exps]
        return probs

