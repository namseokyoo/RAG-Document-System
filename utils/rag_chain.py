from typing import List, Dict, Any, Optional, Iterator
from langchain_ollama import OllamaLLM
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from utils.reranker import get_reranker
from utils.request_llm import RequestLLM
from utils.small_to_large_search import SmallToLargeSearch
from utils.hybrid_retriever import HybridRetriever  # Phase 4: Hybrid Search
import json
import re
import time
import logging
import statistics
import numpy as np

logger = logging.getLogger(__name__)


class RAGChain:
    def __init__(self, vectorstore,
                 llm_api_type: str = "ollama",
                 llm_base_url: str = "http://localhost:11434",
                 llm_model: str = "llama3",
                 llm_api_key: str = "",
                 temperature: float = 0.3,
                 max_tokens: int = 4096,  # Phase D: ë³µì¡í•œ ì§ˆë¬¸/ë²ˆì—­ ëŒ€ì‘
                 top_k: int = 3,
                 use_reranker: bool = True,
                 reranker_model: str = "multilingual-mini",
                 reranker_initial_k: int = 20,
                 enable_synonym_expansion: bool = True,
                 enable_multi_query: bool = True,
                 multi_query_num: int = 3,
                 # Phase 4: Hybrid Search (BM25 + Vector)
                 enable_hybrid_search: bool = True,
                 hybrid_bm25_weight: float = 0.5,
                 # Small-to-Large context size
                 small_to_large_context_size: int = 800,  # ê¸°ë³¸ê°’ í†µì¼ (300 â†’ 800)
                 # Phase A-3: Self-Consistency Check
                 enable_self_consistency: bool = False,
                 self_consistency_n: int = 3):
        self.llm_api_type = llm_api_type
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.llm_api_key = llm_api_key
        self.temperature = temperature
        self.max_tokens = max_tokens  # Phase D
        self.top_k = top_k
        self.vectorstore = vectorstore
        self.vectorstore_manager = vectorstore  # ChatWidgetì—ì„œ ì ‘ê·¼ìš©

        # Re-ranker ì„¤ì • (ê¸°ë³¸ í™œì„±í™”)
        self.use_reranker = use_reranker
        self.reranker_model = reranker_model
        self.reranker_initial_k = max(reranker_initial_k, top_k * 5)
        
        # Re-ranker ì´ˆê¸°í™” (ì‚¬ìš© ì‹œ)
        self.reranker = None
        if self.use_reranker:
            try:
                self.reranker = get_reranker(model_name=reranker_model)
                logger.info(f"Re-ranker ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {reranker_model}")
            except Exception as e:
                # ì—ëŸ¬ ë©”ì‹œì§€ì—ì„œ ì¤‘ë³µ ì œê±° (reranker.pyì—ì„œ ì´ë¯¸ ìƒì„¸ ë©”ì‹œì§€ ì¶œë ¥)
                error_msg = str(e)
                if "ì˜¤í”„ë¼ì¸ ëª¨ë“œì—ì„œ" in error_msg or "ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in error_msg:
                    # ì´ë¯¸ í¬ë§·ëœ ì—ëŸ¬ ë©”ì‹œì§€ì´ë¯€ë¡œ ê°„ë‹¨íˆë§Œ ë¡œê¹…
                    logger.warning(f"Re-ranker ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ({reranker_model}): ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    logger.warning(f"Re-ranker ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ({reranker_model}): {error_msg}")
                logger.warning("Re-ranker ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                self.use_reranker = False
                self.reranker = None
        
        # ë§ˆì§€ë§‰ ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ (ì¶œì²˜ í‘œì‹œìš©)
        self._last_retrieved_docs = []
        
        # Chat history ìºì‹œ (ë„ë©”ì¸ ê°ì§€ìš©)
        self._chat_history_cache = []
        
        # LLM ì´ˆê¸°í™” - API íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
        self.llm = self._create_llm()
        
        # ë™ì˜ì–´ í™•ì¥ ì„¤ì •
        self.enable_synonym_expansion = enable_synonym_expansion
        self.multi_query_num = max(0, multi_query_num)
        self.enable_multi_query = enable_multi_query and self.multi_query_num > 0

        # Small-to-Large ì»¨í…ìŠ¤íŠ¸ í¬ê¸° ì„¤ì •
        self.small_to_large_context_size = small_to_large_context_size

        # Small-to-Large ê²€ìƒ‰ ì´ˆê¸°í™”
        self.small_to_large_search = SmallToLargeSearch(vectorstore)

        # Phase 4: Hybrid Search (BM25 + Vector) ì´ˆê¸°í™”
        self.enable_hybrid_search = enable_hybrid_search
        self.hybrid_retriever = None
        if self.enable_hybrid_search:
            try:
                self.hybrid_retriever = HybridRetriever(
                    vector_manager=vectorstore,
                    bm25_weight=hybrid_bm25_weight
                )
                # BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
                self.hybrid_retriever.build_bm25_index()
                logger.info(f"Hybrid Search ì´ˆê¸°í™” ì™„ë£Œ (BM25: {hybrid_bm25_weight}, Vector: {1-hybrid_bm25_weight})")
            except Exception as e:
                logger.warning(f"Hybrid Search ì´ˆê¸°í™” ì‹¤íŒ¨: {e}, ê¸°ë³¸ ê²€ìƒ‰ ëª¨ë“œë¡œ ì§„í–‰")
                self.enable_hybrid_search = False
                self.hybrid_retriever = None

        # Phase A-3: Self-Consistency Check ì„¤ì •
        self.enable_self_consistency = enable_self_consistency
        self.self_consistency_n = max(2, self_consistency_n)  # ìµœì†Œ 2íšŒ
        if self.enable_self_consistency:
            logger.info(f"Self-Consistency Check í™œì„±í™” (n={self.self_consistency_n})")
        else:
            logger.info("Self-Consistency Check ë¹„í™œì„±í™” (ë‹¨ì¼ ìƒì„±)")

        # Score-based Filtering ì„¤ì • (OpenAI ìŠ¤íƒ€ì¼)
        self.enable_score_filtering = True  # í•­ìƒ í™œì„±í™”
        self.score_threshold = 0.5  # ìµœì†Œ ì ìˆ˜ (configì—ì„œ ì„¤ì • ê°€ëŠ¥)
        self.max_num_results = 20  # ìµœëŒ€ ë¬¸ì„œ ìˆ˜
        self.min_num_results = 3   # ìµœì†Œ ë¬¸ì„œ ìˆ˜ (ì•ˆì „ë§)
        self.enable_adaptive_threshold = True  # ë™ì  threshold
        self.adaptive_threshold_percentile = 0.6  # top1 ëŒ€ë¹„ ë¹„ìœ¨
        logger.info(f"Score-based Filtering í™œì„±í™” (threshold={self.score_threshold}, max={self.max_num_results})")

        # Exhaustive Retrieval ì„¤ì • (ëŒ€ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬)
        self.enable_exhaustive_retrieval = True  # "ëª¨ë“ /ì „ì²´" í‚¤ì›Œë“œ ê°ì§€
        self.exhaustive_max_results = 100  # Exhaustive mode ìµœëŒ€ ë¬¸ì„œ ìˆ˜
        self.enable_single_file_optimization = True  # ë‹¨ì¼ íŒŒì¼ ìµœì í™”
        logger.info(f"Exhaustive Retrieval í™œì„±í™” (max={self.exhaustive_max_results})")

        # ë„ë©”ì¸ ìš©ì–´ ì‚¬ì „ (ì—”í‹°í‹° ê°ì§€ìš©)
        self._domain_lexicon = {
            "TADF", "ACRSA", "DABNA1", "HF", "OLED", "EQE",
            "FRET", "PLQY", "DMAC-TRZ", "AZB-TRZ", "Î½-DABNA"
        }
        
        # Retriever ì„¤ì • - vectorstoreëŠ” VectorStoreManager ì¸ìŠ¤í„´ìŠ¤
        self.retriever = vectorstore.vectorstore.as_retriever(
            search_kwargs={"k": max(self.top_k * 8, 24)}
        )
        
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (Phase D: Answer Naturalization)
        self.base_prompt_template = """ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ì œê³µëœ ë¬¸ì„œ:
{context}

ì´ì „ ëŒ€í™”:
{chat_history}

ì§ˆë¬¸:
{question}

---

ë‹µë³€ ê°€ì´ë“œ:

1. **ìì—°ìŠ¤ëŸ¬ìš´ í˜•ì‹**:
   - ì„¹ì…˜ ì œëª© ì—†ì´ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ìœ¼ë¡œ ì‘ì„±
   - ì§ˆë¬¸ì´ ê°„ë‹¨í•˜ë©´ ì§§ê²Œ (1-2ë¬¸ì¥), ë³µì¡í•˜ë©´ ì—¬ëŸ¬ ë¬¸ë‹¨ìœ¼ë¡œ
   - ì‚¬ìš©ì ì˜ë„ì— ë§ê²Œ ë‹µë³€ (ë²ˆì—­/ìš”ì•½/ì„¤ëª… ë“±)
   - **ìˆ˜ì‹, ìˆ˜ì¹˜, ê¸°í˜¸ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì •í™•íˆ ì¶”ì¶œí•˜ì—¬ í¬í•¨** (ì˜ˆ: R ~ t^(1/3), Pe_C = Ï‡_0 / M_0)

2. **Inline Citation** (í•„ìˆ˜):
   - ëª¨ë“  ì‚¬ì‹¤ì— [ë²ˆí˜¸] í‘œì‹œ
   - ì˜ˆì‹œ: "kFRET ê°’ì€ 87.8%ì…ë‹ˆë‹¤[1]."
   - ì—¬ëŸ¬ ì¶œì²˜: "TADFë¥¼ í™œìš©í•˜ë©°[1], ë†’ì€ íš¨ìœ¨ì„ ë³´ì…ë‹ˆë‹¤[2]."

3. **ì˜ˆì‹œ**:

ì§ˆë¬¸: "kFRET ê°’ì€?"
ë‹µë³€: ì œê³µëœ ë¬¸ì„œì— ë”°ë¥´ë©´, kFRET ê°’ì€ ì•½ 87.8%ì…ë‹ˆë‹¤[1]. ì´ëŠ” í˜•ê´‘ ë„í€íŠ¸ì™€ í˜¸ìŠ¤íŠ¸ ê°„ì˜ ì—ë„ˆì§€ ì „ë‹¬ íš¨ìœ¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤[1].

ì§ˆë¬¸: "TADFë€ ë¬´ì—‡ì¸ê°€?"
ë‹µë³€: TADF(Thermally Activated Delayed Fluorescence)ëŠ” ì‚¼ì¤‘í•­ ì—¬ê¸°ìë¥¼ ì—´ì ìœ¼ë¡œ í™œì„±í™”í•˜ì—¬ ì¼ì¤‘í•­ìœ¼ë¡œ ì¬ë³€í™˜í•˜ëŠ” ë°œê´‘ ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤[1]. ì´ë¥¼ í†µí•´ OLEDì—ì„œ ì´ë¡ ì ìœ¼ë¡œ 100%ì˜ ë‚´ë¶€ ì–‘ì íš¨ìœ¨ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤[1][2].

ì§ˆë¬¸: "ì„œë¡  ë²ˆì—­í•´ì¤˜"
ë‹µë³€: í•˜ì´ë¸Œë¦¬ë“œ í˜•ê´‘ OLEDëŠ” TADF ë³´ì¡° í˜¸ìŠ¤íŠ¸ì™€ í˜•ê´‘ ë„í€íŠ¸ë¥¼ ê²°í•©í•œ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤[1]. ì´ ì ‘ê·¼ë²•ì€ TADFì˜ ë†’ì€ íš¨ìœ¨ê³¼ í˜•ê´‘ ë„í€íŠ¸ì˜ ìš°ìˆ˜í•œ ìƒ‰ìˆœë„ë¥¼ ë™ì‹œì— ë‹¬ì„±í•©ë‹ˆë‹¤[1][2].

ì§ˆë¬¸: "Pe_CëŠ” ë¬´ì—‡ì„ ë‚˜íƒ€ë‚´ë‚˜?"
ë‹µë³€: í™”í•™ì£¼ì„± PÃ©clet ìˆ˜(Pe_C)ëŠ” ë°©í–¥ì„± ìˆëŠ” í™”í•™ì£¼ì„±ê³¼ ë°©í–¥ì„± ì—†ëŠ” í™œì„± í™•ì‚° ì‚¬ì´ì˜ ê²½ìŸì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤[1]. Pe_C â‰¡ Ï‡_0 / M_0ë¡œ ì •ì˜ë©ë‹ˆë‹¤[1].

ì§ˆë¬¸: "í•©ì„± ì˜¨ë„ëŠ”?"
ë‹µë³€: ë¬¸ì„œì—ì„œëŠ” ìœ ê¸° í•©ì„± ê³¼ì •ì„ ì„¤ëª…í•˜ê³  ìˆì§€ë§Œ[1], êµ¬ì²´ì ì¸ í•©ì„± ì˜¨ë„ëŠ” ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.

4. **ì¤‘ìš”**:
   - ë¬¸ì„œì— ê·¼ê±°í•˜ì§€ ì•Šì€ ì¶”ì¸¡ì€ í•˜ì§€ ë§ˆì„¸ìš”. ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
   - ìˆ˜í•™ ê³µì‹, ë¶€ë“±ì‹, ê´€ê³„ì‹ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”.

ë‹µë³€:"""
        
        # ì§ˆë¬¸ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.prompt_templates = {
            "specific_info": """ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ì œê³µëœ ë¬¸ì„œ:
{context}

ì´ì „ ëŒ€í™”:
{chat_history}

ì§ˆë¬¸:
{question}

---

ë‹µë³€ ê°€ì´ë“œ:

1. **ìì—°ìŠ¤ëŸ¬ìš´ í˜•ì‹**:
   - ì„¹ì…˜ ì œëª© ì—†ì´ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ìœ¼ë¡œ ì‘ì„±
   - êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ê°„ê²°í•˜ê²Œ ì œì‹œ
   - **ìˆ˜ì¹˜, ì´ë¦„, ìˆ˜ì‹, ê¸°í˜¸ëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ì •í™•íˆ ì¸ìš©** (ê³¼í•™ì  í‘œê¸°ë²•, ì§€ìˆ˜, íŠ¹ìˆ˜ë¬¸ì í¬í•¨)

2. **Inline Citation** (í•„ìˆ˜):
   - ëª¨ë“  ì‚¬ì‹¤ì— [ë²ˆí˜¸] í‘œì‹œ
   - ì˜ˆì‹œ: "ACRSAë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤[1]."

3. **ì˜ˆì‹œ**:

ì§ˆë¬¸: "kFRET ê°’ì€?"
ë‹µë³€: ì œê³µëœ ë¬¸ì„œì— ë”°ë¥´ë©´, kFRET ê°’ì€ 1.81Ã—10^7 s^-1ì…ë‹ˆë‹¤[1].

ì§ˆë¬¸: "ì‚¬ìš©í•œ TADF ì¬ë£ŒëŠ”?"
ë‹µë³€: ë…¼ë¬¸ì—ì„œ ACRSA (spiro-linked TADF molecule)ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤[1]. ë¹„êµ ì‹¤í—˜ì„ ìœ„í•´ DABNA1ë„ ì–¸ê¸‰ë˜ì–´ ìˆìŠµë‹ˆë‹¤[2].

ì§ˆë¬¸: "Pe_C ì •ì˜ëŠ”?"
ë‹µë³€: Pe_C â‰¡ Ï‡_0 / M_0ë¡œ ì •ì˜ë©ë‹ˆë‹¤[1].

4. **ì¤‘ìš”**:
   - ë¬¸ì„œì— ê·¼ê±°í•˜ì§€ ì•Šì€ ì¶”ì¸¡ì€ í•˜ì§€ ë§ˆì„¸ìš”. ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
   - ìˆ˜í•™ ê³µì‹ì´ë‚˜ ìˆ˜ì¹˜ëŠ” ì ˆëŒ€ ìƒëµí•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.

ë‹µë³€:""",
            
            "summary": """ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ì œê³µëœ ë¬¸ì„œ:
{context}

ì´ì „ ëŒ€í™”:
{chat_history}

ì§ˆë¬¸:
{question}

---

ë‹µë³€ ê°€ì´ë“œ:

1. **ìì—°ìŠ¤ëŸ¬ìš´ í˜•ì‹**:
   - ì„¹ì…˜ ì œëª© ì—†ì´ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ìœ¼ë¡œ ì‘ì„±
   - ìš”ì•½ì€ ê°„ê²°í•˜ê³  í•µì‹¬ë§Œ ì œì‹œ
   - ì£¼ìš” ë‚´ìš©ì„ ë…¼ë¦¬ì  ìˆœì„œë¡œ êµ¬ì„±

2. **Inline Citation** (í•„ìˆ˜):
   - ëª¨ë“  ì‚¬ì‹¤ì— [ë²ˆí˜¸] í‘œì‹œ
   - ì˜ˆì‹œ: "TADF ì¬ë£Œë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤[1]."

3. **ì˜ˆì‹œ**:

ì§ˆë¬¸: "í•µì‹¬ ë‚´ìš© ìš”ì•½í•´ì¤˜"
ë‹µë³€: ì´ ë…¼ë¬¸ì€ TADF ì¬ë£Œë¥¼ ì‚¬ìš©í•œ OLEDì˜ íš¨ìœ¨ ê°œì„ ì— ê´€í•œ ì—°êµ¬ì…ë‹ˆë‹¤[1]. ACRSA ê¸°ë°˜ ë””ë°”ì´ìŠ¤ë¥¼ í†µí•´ ë†’ì€ ë°œê´‘ íš¨ìœ¨ì„ ë‹¬ì„±í–ˆìœ¼ë©°[2], ê¸°ì¡´ ì¬ë£Œ ëŒ€ë¹„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤[3].

4. **ì¤‘ìš”**:
   ë¬¸ì„œì— ê·¼ê±°í•˜ì§€ ì•Šì€ ì¶”ì¸¡ì€ í•˜ì§€ ë§ˆì„¸ìš”. ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

ë‹µë³€:""",
            
            "comparison": """ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ì œê³µëœ ë¬¸ì„œ:
{context}

ì´ì „ ëŒ€í™”:
{chat_history}

ì§ˆë¬¸:
{question}

---

ë‹µë³€ ê°€ì´ë“œ:

1. **ìì—°ìŠ¤ëŸ¬ìš´ í˜•ì‹**:
   - ì„¹ì…˜ ì œëª© ì—†ì´ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ìœ¼ë¡œ ì‘ì„±
   - ë¹„êµ ëŒ€ìƒë“¤ì˜ ì°¨ì´ì ê³¼ ê³µí†µì ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…
   - **ìˆ˜ì‹, ìˆ˜ì¹˜, ê¸°í˜¸ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì •í™•íˆ ì¶”ì¶œí•˜ì—¬ í¬í•¨** (ì˜ˆ: Pe_C >= Pe_C,crit, R ~ t^(1/3), Î± <= Î±_crit)
   - êµ¬ì²´ì ì¸ ì¡°ê±´, ê¸°ì¤€, ì„ê³„ê°’ì„ ëª…ì‹œ

2. **Inline Citation** (í•„ìˆ˜):
   - ëª¨ë“  ì‚¬ì‹¤ì— [ë²ˆí˜¸] í‘œì‹œ
   - ì˜ˆì‹œ: "ACRSAëŠ” spiro-linked êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤[1]."

3. **ì˜ˆì‹œ**:

ì§ˆë¬¸: "ACRSAì™€ DABNA1ì˜ ì°¨ì´ì ì€?"
ë‹µë³€: ACRSAì™€ DABNA1ì€ ë‘˜ ë‹¤ TADF ì¬ë£Œì´ì§€ë§Œ êµ¬ì¡°ì  ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤[1]. ACRSAëŠ” spiro-linked êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆì–´ ë¶„ì ê°„ ìƒí˜¸ì‘ìš©ì„ ìµœì†Œí™”í•˜ë©°[1], ì´ë¥¼ í†µí•´ ë†’ì€ ë°œê´‘ íš¨ìœ¨ì„ ë‹¬ì„±í•©ë‹ˆë‹¤[2]. ë°˜ë©´ DABNA1ì€ ë‹¤ë¥¸ ë¶„ì êµ¬ì¡°ë¥¼ ê°€ì§€ë©°[1], ë¹„êµ ì‹¤í—˜ì—ì„œ ACRSAë³´ë‹¤ ë‚®ì€ íš¨ìœ¨ì„ ë³´ì˜€ìŠµë‹ˆë‹¤[3].

ì§ˆë¬¸: "MIPS ì–µì œ ê¸°ì¤€ì€?"
ë‹µë³€: í™”í•™ì£¼ì„±ì´ MIPSë¥¼ ì–µì œí•˜ê¸° ìœ„í•´ì„œëŠ” ë‘ ê°€ì§€ ê¸°ì¤€ì´ ë™ì‹œì— ë§Œì¡±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤[1]. ì²«ì§¸, í™˜ì›ëœ í™”í•™ì£¼ì„± PÃ©clet ìˆ˜ê°€ ì„ê³„ê°’ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤ (Pe_C' >= Pe_C,crit')[1]. ë‘˜ì§¸, ìœ íš¨ ì§‘ë‹¨ í™•ì‚°ë„ ë¹„ìœ¨ Î±ê°€ ì„ê³„ê°’ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤ (Î± <= Î±_crit)[1].

4. **ì¤‘ìš”**:
   - ë¬¸ì„œì— ê·¼ê±°í•˜ì§€ ì•Šì€ ì¶”ì¸¡ì€ í•˜ì§€ ë§ˆì„¸ìš”. ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
   - ìˆ˜í•™ ê³µì‹, ë¶€ë“±ì‹, ê´€ê³„ì‹ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”.

ë‹µë³€:""",
            
            "relationship": """ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ì œê³µëœ ë¬¸ì„œ:
{context}

ì´ì „ ëŒ€í™”:
{chat_history}

ì§ˆë¬¸:
{question}

---

ë‹µë³€ ê°€ì´ë“œ:

1. **ìì—°ìŠ¤ëŸ¬ìš´ í˜•ì‹**:
   - ì„¹ì…˜ ì œëª© ì—†ì´ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ìœ¼ë¡œ ì‘ì„±
   - ìš”ì†Œë“¤ ê°„ì˜ ê´€ê³„, ì¸ê³¼ê´€ê³„, ë©”ì»¤ë‹ˆì¦˜ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…
   - êµ¬ì²´ì ì¸ ì˜í–¥ì´ë‚˜ ê²°ê³¼ë¥¼ ëª…í™•íˆ ì œì‹œ
   - **ìˆ˜ì‹ì´ë‚˜ ìˆ˜ì¹˜ë¡œ ê´€ê³„ê°€ í‘œí˜„ë˜ë©´ ë°˜ë“œì‹œ ì •í™•íˆ í¬í•¨** (ì˜ˆ: J = -Mâˆ‡Ï† + Ï‡âˆ‡c)

2. **Inline Citation** (í•„ìˆ˜):
   - ëª¨ë“  ì‚¬ì‹¤ì— [ë²ˆí˜¸] í‘œì‹œ
   - ì˜ˆì‹œ: "spiro-linked êµ¬ì¡°ëŠ” ìƒí˜¸ì‘ìš©ì„ ê°ì†Œì‹œí‚µë‹ˆë‹¤[1]."

3. **ì˜ˆì‹œ**:

ì§ˆë¬¸: "TADF ì¬ë£Œì˜ êµ¬ì¡°ê°€ ë°œê´‘ íš¨ìœ¨ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€?"
ë‹µë³€: ë¬¸ì„œì— ë”°ë¥´ë©´, TADF ì¬ë£Œì˜ spiro-linked êµ¬ì¡°ëŠ” ë¶„ì ê°„ ìƒí˜¸ì‘ìš©ì„ ìµœì†Œí™”í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤[1]. ì´ëŸ¬í•œ êµ¬ì¡°ì  íŠ¹ì„±ì€ ë¶„ìë“¤ ì‚¬ì´ì˜ ì—ë„ˆì§€ ì†ì‹¤ì„ ì¤„ì´ë©°[1], ê²°ê³¼ì ìœ¼ë¡œ ë†’ì€ ë°œê´‘ íš¨ìœ¨ì„ ë‹¬ì„±í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤[2]. TADF ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•œ ì—ë„ˆì§€ ì „ë‹¬ì´ ìµœì í™”ë˜ë©´ì„œ[2], ì „ì²´ì ì¸ ë””ë°”ì´ìŠ¤ ì„±ëŠ¥ì´ í–¥ìƒë©ë‹ˆë‹¤[3].

ì§ˆë¬¸: "í™”í•™ì£¼ì„±ì´ ì…ì í”ŒëŸ­ìŠ¤ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€?"
ë‹µë³€: ì…ì í”ŒëŸ­ìŠ¤(J)ëŠ” í™œì„± ë¸Œë¼ìš´ ìš´ë™ í•­ê³¼ í™”í•™ì£¼ì„± í•­ì˜ ë‘ ê°€ì§€ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤[1]. í™”í•™ì£¼ì„± í•­ì€ J = -Ï‡âˆ‡f(c)ë¡œ í‘œí˜„ë˜ë©°, ì…ìê°€ í™”í•™ìœ ì¸ë¬¼ì§ˆ êµ¬ë°°ë¥¼ ë”°ë¼ ì´ë™í•˜ë„ë¡ ë§Œë“­ë‹ˆë‹¤[1].

4. **ì¤‘ìš”**:
   - ë¬¸ì„œì— ê·¼ê±°í•˜ì§€ ì•Šì€ ì¶”ì¸¡ì€ í•˜ì§€ ë§ˆì„¸ìš”. ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
   - ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìˆ˜ì‹ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”.

ë‹µë³€:""",
            
            "general": self.base_prompt_template
        }
        
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ (ë‚˜ì¤‘ì— ì§ˆë¬¸ íƒ€ì…ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì„ íƒ)
        self.prompt_template = self.base_prompt_template
        
        self.prompt = PromptTemplate(
            template=self.prompt_template,
            input_variables=["chat_history", "context", "question"]
        )
        
        # LCEL ë°©ì‹ìœ¼ë¡œ ì²´ì¸ êµ¬ì„± (ëŒ€í™” ì´ë ¥ í¬í•¨)
        self.chain = (
            {
                "context": lambda x: self._get_context(x["question"]),
                "chat_history": lambda x: x.get("chat_history", "ì´ì „ ëŒ€í™” ì—†ìŒ"),
                "question": lambda x: x["question"]
            }
            | self.prompt
            | self.llm
            | StrOutputParser()
        )

        # Question Classifier ì´ˆê¸°í™” (Quick Wins: ì§ˆë¬¸ ìœ í˜•ë³„ ìµœì í™”)
        from utils.question_classifier import create_classifier
        try:
            self.question_classifier = create_classifier(
                llm=self.llm,
                use_llm=True,  # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ
                verbose=False  # ë°°í¬ ì‹œ False
            )
            logger.info("Question Classifier ì´ˆê¸°í™” ì™„ë£Œ (í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ)")
        except Exception as e:
            logger.warning(f"Question Classifier ì´ˆê¸°í™” ì‹¤íŒ¨: {e}, ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©")
            self.question_classifier = None

    def _create_llm(self):
        """API íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
        if self.llm_api_type == "request":
            return RequestLLM(
                base_url=self.llm_base_url,
                model=self.llm_model,
                temperature=self.temperature,
                max_tokens=self.max_tokens,  # Phase D
                timeout=60
            )
        elif self.llm_api_type == "ollama":
            return OllamaLLM(
                base_url=self.llm_base_url,
                model=self.llm_model,
                temperature=self.temperature,
                num_predict=self.max_tokens  # Phase D: OllamaëŠ” num_predict ì‚¬ìš©
            )
        elif self.llm_api_type == "openai":
            kwargs = {
                "model": self.llm_model,
                "temperature": self.temperature,
                "max_tokens": self.max_tokens,  # Phase D
                "api_key": self.llm_api_key if self.llm_api_key else "not-needed"
            }
            return ChatOpenAI(**kwargs)
        elif self.llm_api_type == "openai-compatible":
            kwargs = {
                "model": self.llm_model,
                "temperature": self.temperature,
                "base_url": self.llm_base_url,
                "api_key": self.llm_api_key if self.llm_api_key else "not-needed"
            }
            return ChatOpenAI(**kwargs)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” API íƒ€ì…: {self.llm_api_type}")

    def _format_docs(self, docs: List[Document]) -> str:
        """ë¬¸ì„œë¥¼ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ… (ìƒìš© ì„œë¹„ìŠ¤ ìˆ˜ì¤€ ê°œì„ )"""
        formatted_sections = []
        
        for i, doc in enumerate(docs, 1):
            metadata = doc.metadata or {}
            file_name = metadata.get('file_name', 'Unknown')
            page_number = metadata.get('page_number', 'Unknown')
            chunk_type = metadata.get('chunk_type', 'unknown')
            section_title = metadata.get('section_title', '')
            
            # ë¬¸ì„œ ë²ˆí˜¸ì™€ ë©”íƒ€ë°ì´í„°
            header = f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            header += f"ğŸ“„ ë¬¸ì„œ #{i}\n"
            header += f"   íŒŒì¼ëª…: {file_name}\n"
            header += f"   í˜ì´ì§€: {page_number}\n"
            if chunk_type != 'unknown':
                header += f"   ì²­í¬ íƒ€ì…: {chunk_type}\n"
            if section_title:
                header += f"   ì„¹ì…˜: {section_title}\n"
            header += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
            
            # ë¬¸ì„œ ë‚´ìš©
            content = doc.page_content.strip()
            
            formatted_sections.append(header + content)
        
        return "\n\n".join(formatted_sections)

    def _unique_by_file(self, pairs: List[tuple], k: int) -> List[tuple]:
        """(Document, score) ë¦¬ìŠ¤íŠ¸ì—ì„œ íŒŒì¼ëª… ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µì„ ì œê±°í•˜ë©° ìµœëŒ€ kê°œ ë°˜í™˜
        ê°œì„ : PPTXëŠ” ìŠ¬ë¼ì´ë“œ ë‹¨ìœ„, PDFëŠ” í˜ì´ì§€ ë‹¨ìœ„ë¡œ ì¤‘ë³µ ì œê±°"""
        seen = set()
        results: List[tuple] = []
        file_chunk_counts = {}  # íŒŒì¼ë³„ ì²­í¬ ê°œìˆ˜ ì¶”ì 
        
        for doc, score in pairs:
            file_name = doc.metadata.get("file_name", "")
            chunk_type = doc.metadata.get("chunk_type", "")
            
            # PPTX: ìŠ¬ë¼ì´ë“œ ë‹¨ìœ„ ì¤‘ë³µ ì œê±°, PDF: í˜ì´ì§€ ë‹¨ìœ„ ì¤‘ë³µ ì œê±°
            # slide_number ë˜ëŠ” page_numberê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ file_nameë§Œ ì‚¬ìš©
            slide_number = doc.metadata.get("slide_number")
            page_number = doc.metadata.get("page_number")
            
            if slide_number is not None:
                # PPTX: file_name + slide_number ì¡°í•©ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
                key = f"{file_name}_slide_{slide_number}"
            elif page_number is not None:
                # PDF: file_name + page_number ì¡°í•©ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
                key = f"{file_name}_page_{page_number}"
            else:
                # ë©”íƒ€ë°ì´í„°ê°€ ì—†ìœ¼ë©´ íŒŒì¼ëª…ë§Œ ì‚¬ìš© (ê¸°ì¡´ ë°©ì‹)
                key = file_name
            
            # íŒŒì¼ë‹¹ ìµœëŒ€ ì²­í¬ ìˆ˜ ì œí•œ (ë„ˆë¬´ ë§ì€ ì²­í¬ ë°©ì§€)
            # ëª©ë¡ ë‚˜ì—´ ì§ˆë¬¸ì˜ ê²½ìš° ë” ë§ì´ í—ˆìš©
            max_per_file = 10  # ê¸°ë³¸ê°’: íŒŒì¼ë‹¹ ìµœëŒ€ 10ê°œ ì²­í¬
            
            if key in seen:
                # ì´ë¯¸ ë³¸ ì¡°í•©ì´ë©´ ê±´ë„ˆë›°ê¸° (ë™ì¼ ìŠ¬ë¼ì´ë“œ/í˜ì´ì§€ëŠ” 1ê°œë§Œ)
                continue
            
            # íŒŒì¼ë‹¹ ì²­í¬ ê°œìˆ˜ ì²´í¬
            if file_name in file_chunk_counts:
                if file_chunk_counts[file_name] >= max_per_file:
                    continue
                file_chunk_counts[file_name] = 1
            else:
                file_chunk_counts[file_name] = 1
            
            seen.add(key)
            results.append((doc, score))
            if len(results) >= k:
                break
        return results

    def _search_candidates(self, question: str, search_mode: str = "integrated") -> List[tuple]:
        """
        Hybrid Search ë‹¨ì¼ ì§„ì…ì  (BM25 + Vector Search)

        ìš°ì„ ìˆœìœ„:
        1. search_with_mode (ë“€ì–¼ DB ì§€ì›) - ìµœìš°ì„ , ê°€ì¥ ê¸°ëŠ¥ì´ í’ë¶€
        2. similarity_search_hybrid (í´ë°±) - ë‹¨ì¼ DB í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        """
        try:
            # Question Classifierê°€ ì„¤ì •í•œ ê°’ ì‚¬ìš© (ë™ì  ì¡°ì •)
            # ë¶„ë¥˜ê¸°ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
            if hasattr(self, '_last_classification') and self._last_classification:
                initial_k = self.reranker_initial_k  # ë¶„ë¥˜ê¸°ê°€ ì„¤ì •í•œ ê°’ ì‚¬ìš©
            else:
                initial_k = max(self.reranker_initial_k, max(self.top_k * 8, 60))  # ê¸°ì¡´ ë¡œì§

            # ìš°ì„ ìˆœìœ„ 1: ë“€ì–¼ DB í†µí•© ê²€ìƒ‰ (ìµœì‹ , ê°€ì¥ ê¸°ëŠ¥ í’ë¶€)
            if hasattr(self.vectorstore, 'search_with_mode'):
                print(f"[SEARCH] ë“€ì–¼ DB ê²€ìƒ‰ ëª¨ë“œ: {search_mode}, initial_k={initial_k}")
                hybrid = self.vectorstore.search_with_mode(
                    query=question,
                    search_mode=search_mode,
                    initial_k=initial_k,
                    top_k=initial_k,
                    use_reranker=self.use_reranker,
                    reranker_model=self.reranker_model
                )
            # ìš°ì„ ìˆœìœ„ 2: í´ë°± - ê¸°ë³¸ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
            else:
                print(f"[SEARCH] ê¸°ë³¸ Hybrid Search (BM25+Vector) ì‚¬ìš© (initial_k={initial_k})")
                hybrid = self.vectorstore.similarity_search_hybrid(
                    question, initial_k=initial_k, top_k=initial_k
                )

            # Phase 3: ì—”í‹°í‹° ë§¤ì¹­ ì²­í¬ì— boost ì ìš©
            if hasattr(self.vectorstore, 'entity_index') and self.vectorstore.entity_index:
                hybrid = self._apply_entity_boost(question, hybrid)

            return hybrid
        except Exception as e:
            print(f"[WARN] Hybrid Search ì˜¤ë¥˜: {e}, í´ë°± ëª¨ë“œë¡œ ì „í™˜")
            # í´ë°±: ë²¡í„° ê²€ìƒ‰ (ë¶„ë¥˜ê¸° ì„¤ì •ê°’ ì‚¬ìš©)
            if hasattr(self, '_last_classification') and self._last_classification:
                fallback_k = self.reranker_initial_k  # ë¶„ë¥˜ê¸°ê°€ ì„¤ì •í•œ ê°’
            else:
                fallback_k = max(self.reranker_initial_k, 60)  # ê¸°ì¡´ ë¡œì§
            return self.vectorstore.similarity_search_with_score(question, k=fallback_k)
    
    def _apply_entity_boost(self, question: str, candidates: List[tuple], boost_factor: float = 1.5) -> List[tuple]:
        """ì—”í‹°í‹° ë§¤ì¹­ ì²­í¬ì— boost ì ìˆ˜ ì ìš© (Phase 3)"""
        # ì¿¼ë¦¬ì—ì„œ ì—”í‹°í‹° ê°ì§€ (ë„ë©”ì¸ ìš©ì–´ ì‚¬ì „ í™œìš©)
        detected_entities = []
        question_lower = question.lower()
        
        # ë„ë©”ì¸ ìš©ì–´ ì‚¬ì „ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ
        for key in self._domain_lexicon.keys():
            if key.lower() in question_lower:
                detected_entities.append(key)
        
        # ê°ì§€ëœ ì—”í‹°í‹°ê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if not detected_entities:
            return candidates
        
        # ì—”í‹°í‹° ë§¤ì¹­ ì²­í¬ ID ìˆ˜ì§‘
        matching_chunk_ids = set()
        for entity in detected_entities:
            chunk_ids = self.vectorstore.search_by_entity(entity)
            matching_chunk_ids.update(chunk_ids)
        
        if not matching_chunk_ids:
            return candidates
        
        # ë§¤ì¹­ë˜ëŠ” ì²­í¬ì— boost ì ìš©
        boosted_candidates = []
        boost_count = 0
        for doc, score in candidates:
            chunk_id = doc.metadata.get('chunk_id') or doc.metadata.get('id')
            if chunk_id in matching_chunk_ids:
                boosted_score = score * boost_factor
                boosted_candidates.append((doc, boosted_score))
                boost_count += 1
            else:
                boosted_candidates.append((doc, score))
        
        if boost_count > 0:
            print(f"âœ¨ ì—”í‹°í‹° boost ì ìš©: {boost_count}ê°œ ì²­í¬ (ê°ì§€ëœ ì—”í‹°í‹°: {', '.join(detected_entities)})")
        
        return boosted_candidates

    def rerank_documents(self, query: str, docs: List[tuple]) -> List[tuple]:
        """Re-rankerë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ì¬ìˆœìœ„í™”

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            docs: (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸

        Returns:
            Re-rankingëœ (Document, rerank_score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.use_reranker or not self.reranker:
            print("[INFO] Re-rankerê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆê±°ë‚˜ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì›ë³¸ ë°˜í™˜.")
            return docs

        if not docs:
            return docs

        try:
            # Re-ranker ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            docs_for_rerank = [{
                "document": doc,
                "chunk_id": idx,
                "raw_score": score
            } for idx, (doc, score) in enumerate(docs)]

            # Re-ranking ìˆ˜í–‰
            reranked = self.reranker.rerank(query, docs_for_rerank, top_k=len(docs_for_rerank))

            # ê²°ê³¼ë¥¼ (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            pairs = [(d["document"], d.get("rerank_score", 0.0)) for d in reranked]

            print(f"[Re-ranker] {len(docs)}ê°œ ë¬¸ì„œ ì¬ìˆœìœ„í™” ì™„ë£Œ")
            return pairs

        except Exception as e:
            print(f"[WARN] Re-ranking ì˜¤ë¥˜: {e}, ì›ë³¸ ë°˜í™˜")
            return docs

    def _semantic_similarity_filter(self, query: str, candidates: List[tuple], threshold: float = 0.5) -> List[tuple]:
        """ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ ê¸°ë°˜ í•„í„°ë§ (Solution #1)

        ì¿¼ë¦¬ì™€ ê° ë¬¸ì„œì˜ ì„ë² ë”© ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ threshold ì´í•˜ ë¬¸ì„œ ì œê±°

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            candidates: (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            threshold: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ (0~1, ê¸°ë³¸ê°’ 0.5)

        Returns:
            í•„í„°ë§ëœ (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if not candidates or len(candidates) < 2:
            return candidates

        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.vectorstore.embeddings.embed_query(query)

            filtered = []
            removed_count = 0

            for doc, score in candidates:
                # ë¬¸ì„œ ì„ë² ë”© ìƒì„±
                doc_embedding = self.vectorstore.embeddings.embed_query(doc.page_content)

                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = np.dot(query_embedding, doc_embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)
                )

                if similarity >= threshold:
                    filtered.append((doc, score))
                else:
                    removed_count += 1

            # í•„í„°ë§ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìœ¼ë©´ threshold ì™„í™”
            if len(filtered) < max(2, len(candidates) // 3):
                print(f"[WARN] Semantic í•„í„°ë§ ê²°ê³¼ ë¶€ì¡±, threshold ì™„í™” ({threshold} -> {threshold * 0.7})")
                return self._semantic_similarity_filter(query, candidates, threshold * 0.7)

            if removed_count > 0:
                print(f"[SEMANTIC] ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ í•„í„°ë§: {removed_count}ê°œ ë¬¸ì„œ ì œê±° (threshold={threshold:.2f})")

            return filtered

        except Exception as e:
            print(f"[WARN] Semantic í•„í„°ë§ ì˜¤ë¥˜: {e}, ì›ë³¸ ë°˜í™˜")
            return candidates

    def _keyword_based_filter(self, query: str, candidates: List[tuple], min_overlap: float = 0.2) -> List[tuple]:
        """í‚¤ì›Œë“œ ì¤‘ë³µë„ ê¸°ë°˜ í•„í„°ë§ (Solution #2)

        ì¿¼ë¦¬ì™€ ë¬¸ì„œì˜ í‚¤ì›Œë“œ ì¤‘ë³µë„ë¥¼ ì¸¡ì •í•˜ì—¬ min_overlap ì´í•˜ ë¬¸ì„œ ì œê±°

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            candidates: (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            min_overlap: ìµœì†Œ í‚¤ì›Œë“œ ì¤‘ë³µë„ (0~1, ê¸°ë³¸ê°’ 0.2)

        Returns:
            í•„í„°ë§ëœ (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if not candidates or len(candidates) < 2:
            return candidates

        try:
            # ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ í† í°í™”)
            query_keywords = set(query.lower().split())
            # ë¶ˆìš©ì–´ ì œê±° (ê°„ë‹¨í•œ ì˜ì–´ ë¶ˆìš©ì–´)
            stopwords = {'a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'is', 'are', 'was', 'were'}
            query_keywords = query_keywords - stopwords

            if len(query_keywords) == 0:
                return candidates

            filtered = []
            removed_count = 0

            for doc, score in candidates:
                # ë¬¸ì„œì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                doc_text = doc.page_content.lower()
                doc_keywords = set(doc_text.split()) - stopwords

                # Jaccard ìœ ì‚¬ë„ ê³„ì‚° (êµì§‘í•© / í•©ì§‘í•©)
                if len(doc_keywords) == 0:
                    overlap = 0
                else:
                    intersection = query_keywords & doc_keywords
                    union = query_keywords | doc_keywords
                    overlap = len(intersection) / len(union) if len(union) > 0 else 0

                if overlap >= min_overlap:
                    filtered.append((doc, score))
                else:
                    removed_count += 1

            # í•„í„°ë§ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìœ¼ë©´ threshold ì™„í™”
            if len(filtered) < max(2, len(candidates) // 3):
                print(f"[WARN] Keyword í•„í„°ë§ ê²°ê³¼ ë¶€ì¡±, threshold ì™„í™” ({min_overlap} -> {min_overlap * 0.5})")
                return self._keyword_based_filter(query, candidates, min_overlap * 0.5)

            if removed_count > 0:
                print(f"[KEYWORD] í‚¤ì›Œë“œ ì¤‘ë³µë„ í•„í„°ë§: {removed_count}ê°œ ë¬¸ì„œ ì œê±° (min_overlap={min_overlap:.2f})")

            return filtered

        except Exception as e:
            print(f"[WARN] Keyword í•„í„°ë§ ì˜¤ë¥˜: {e}, ì›ë³¸ ë°˜í™˜")
            return candidates

    def _statistical_outlier_removal(self, candidates: List[tuple], method: str = 'mad', mad_threshold: float = 3.0) -> List[tuple]:
        """í†µê³„ ê¸°ë°˜ ì´ìƒì¹˜ ì œê±° (ê°œì„ ì•ˆ 3)

        Args:
            candidates: (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            method: 'mad' (Median Absolute Deviation) ë˜ëŠ” 'iqr' (Interquartile Range) ë˜ëŠ” 'zscore'
            mad_threshold: MAD ë°©ì‹ì—ì„œ ì‚¬ìš©í•  threshold ë°°ìˆ˜ (ê¸°ë³¸ê°’: 3.0)

        Returns:
            í•„í„°ë§ëœ (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if not candidates or len(candidates) < 3:
            return candidates

        try:
            scores = [float(score) for _, score in candidates]

            if method == 'mad':
                # MAD (Median Absolute Deviation) - ê°€ì¥ ê²¬ê³ í•œ ë°©ë²•
                median = np.median(scores)
                mad = np.median([abs(s - median) for s in scores])

                # MADê°€ 0ì´ë©´ ëª¨ë“  ê°’ì´ ë™ì¼ (í•„í„°ë§ ë¶ˆí•„ìš”)
                if mad < 1e-9:
                    return candidates

                # ì¤‘ì•™ê°’ì—ì„œ mad_threshold * MAD ì´ìƒ ë–¨ì–´ì§„ ê²ƒ ì œê±°
                threshold = median - mad_threshold * mad
                filtered = [(doc, s) for doc, s in candidates if s >= threshold]

            elif method == 'iqr':
                # IQR (Interquartile Range)
                q1 = np.percentile(scores, 25)
                q3 = np.percentile(scores, 75)
                iqr = q3 - q1

                if iqr < 1e-9:
                    return candidates

                lower_bound = q1 - 1.5 * iqr
                upper_bound = q3 + 1.5 * iqr
                filtered = [(doc, s) for doc, s in candidates if lower_bound <= s <= upper_bound]

            elif method == 'zscore':
                # Z-score
                mean = np.mean(scores)
                std = np.std(scores)

                if std < 1e-9:
                    return candidates

                # Z-scoreê°€ 2 ì´ë‚´ì¸ ê²ƒë§Œ ì„ íƒ
                filtered = [(doc, s) for doc, s in candidates if abs((s - mean) / std) < 2]

            else:
                return candidates

            # í•„í„°ë§ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì›ë³¸ ë°˜í™˜ (ìµœì†Œ 3ê°œ ë˜ëŠ” ì›ë³¸ì˜ 50%)
            min_required = max(3, len(candidates) // 2)
            if len(filtered) < min_required:
                print(f"[WARN] í†µê³„ í•„í„°ë§ ê²°ê³¼ ë¶€ì¡± ({len(filtered)}ê°œ), ì›ë³¸ ìœ ì§€")
                return candidates

            removed_count = len(candidates) - len(filtered)
            if removed_count > 0:
                print(f"[STAT] í†µê³„ ê¸°ë°˜ ì´ìƒì¹˜ ì œê±°: {removed_count}ê°œ ë¬¸ì„œ í•„í„°ë§ ({method.upper()} ë°©ì‹)")

            return filtered

        except Exception as e:
            print(f"[WARN] í†µê³„ í•„í„°ë§ ì˜¤ë¥˜: {e}, ì›ë³¸ ë°˜í™˜")
            return candidates

    def _reranker_gap_based_cutoff(self, candidates: List[tuple], min_docs: int = 3, gap_threshold_multiplier: float = 2.0) -> List[tuple]:
        """Re-ranker ì ìˆ˜ Gap ê¸°ë°˜ ë™ì  ì»·ì˜¤í”„ (ê°œì„ ì•ˆ 5)

        ì£¼ì œê°€ ë‹¤ë¥¸ ë¬¸ì„œëŠ” ì ìˆ˜ ì°¨ì´ê°€ í¬ê²Œ ë‚˜íƒ€ë‚˜ëŠ” íŠ¹ì„±ì„ ì´ìš©í•˜ì—¬
        ê°€ì¥ í° ì ìˆ˜ gapì´ ë‚˜íƒ€ë‚˜ëŠ” ì§€ì ì—ì„œ ìë™ìœ¼ë¡œ ì»·ì˜¤í”„

        Args:
            candidates: (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ (ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ ê°€ì •)
            min_docs: ìµœì†Œ ë°˜í™˜ ë¬¸ì„œ ìˆ˜
            gap_threshold_multiplier: Gap threshold ë°°ìˆ˜ (ê¸°ë³¸ê°’: 2.0, ë‚®ì„ìˆ˜ë¡ ë” ì—„ê²©í•˜ê²Œ í•„í„°ë§)

        Returns:
            í•„í„°ë§ëœ (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if not candidates or len(candidates) <= min_docs:
            return candidates

        try:
            scores = [float(score) for _, score in candidates]

            # ì ìˆ˜ ì°¨ì´(gap) ê³„ì‚°
            gaps = [scores[i] - scores[i+1] for i in range(len(scores)-1)]

            if not gaps:
                return candidates

            # ê°€ì¥ í° gap ì°¾ê¸°
            max_gap = max(gaps)
            max_gap_idx = gaps.index(max_gap)

            # Gapì˜ í†µê³„ ë¶„ì„
            mean_gap = statistics.mean(gaps)

            # Gapì´ ì¶©ë¶„íˆ í° ê²½ìš°ì—ë§Œ ì»·ì˜¤í”„ ì ìš©
            # ì¡°ê±´: Gapì´ í‰ê· ì˜ gap_threshold_multiplierë°° ì´ìƒ && ì»·ì˜¤í”„ ìœ„ì¹˜ê°€ min_docs ì´ìƒ
            if max_gap > mean_gap * gap_threshold_multiplier and max_gap_idx >= min_docs - 1:
                cutoff = max_gap_idx + 1
                filtered = candidates[:cutoff]

                removed_count = len(candidates) - cutoff
                print(f"[CUT] Re-ranker Gap ê¸°ë°˜ ì»·ì˜¤í”„: {removed_count}ê°œ ë¬¸ì„œ í•„í„°ë§")
                print(f"   - Gap ìœ„ì¹˜: {cutoff}ë²ˆì§¸ ë¬¸ì„œ (ìµœëŒ€ Gap: {max_gap:.4f}, í‰ê·  Gap: {mean_gap:.4f})")

                return filtered

            # Gapì´ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ì›ë³¸ ë°˜í™˜
            return candidates

        except Exception as e:
            print(f"[WARN] Gap ê¸°ë°˜ í•„í„°ë§ ì˜¤ë¥˜: {e}, ì›ë³¸ ë°˜í™˜")
            return candidates

    def _score_based_filtering(self, candidates: List[tuple], question: str = "") -> List[tuple]:
        """OpenAI ìŠ¤íƒ€ì¼ Score-based Filtering (ì ìˆ˜ + ê°œìˆ˜ í•˜ì´ë¸Œë¦¬ë“œ + Adaptive)

        Args:
            candidates: (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ (ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ ê°€ì •)
            question: ì‚¬ìš©ì ì§ˆë¬¸ (adaptive max results ê³„ì‚°ìš©)

        Returns:
            í•„í„°ë§ëœ (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.enable_score_filtering or not candidates:
            return candidates

        try:
            # 1ë‹¨ê³„: ë™ì  threshold ê³„ì‚° (í™œì„±í™”ëœ ê²½ìš°)
            threshold = self.score_threshold

            if self.enable_adaptive_threshold and len(candidates) > 0:
                scores = [float(score) for _, score in candidates]
                top_score = scores[0]

                # ë™ì  threshold: top1ì˜ N% ë˜ëŠ” ê³ ì • threshold ì¤‘ í° ê°’
                adaptive_threshold = top_score * self.adaptive_threshold_percentile
                threshold = max(self.score_threshold, adaptive_threshold)

                print(f"[SCORE] ë™ì  Threshold: {threshold:.4f} (top1={top_score:.4f} Ã— {self.adaptive_threshold_percentile})")
            else:
                print(f"[SCORE] ê³ ì • Threshold: {threshold:.4f}")

            # 2ë‹¨ê³„: ì ìˆ˜ ê¸°ë°˜ í•„í„°ë§
            filtered = []
            for doc, score in candidates:
                if score >= threshold:
                    filtered.append((doc, score))
                else:
                    # ì ìˆ˜ê°€ threshold ì•„ë˜ë¡œ ë–¨ì–´ì§€ë©´ ì¤‘ë‹¨ (ë‚´ë¦¼ì°¨ìˆœ ê°€ì •)
                    break

            # 3ë‹¨ê³„: Adaptive ìµœëŒ€ ê°œìˆ˜ ê³„ì‚° (ì§ˆë¬¸ ìœ í˜• ê¸°ë°˜)
            if question:
                max_results = self._adaptive_max_results(question, candidates)
            else:
                max_results = self.max_num_results

            # 4ë‹¨ê³„: ìµœëŒ€ ê°œìˆ˜ ì œí•œ
            if len(filtered) > max_results:
                removed = len(filtered) - max_results
                print(f"[SCORE] ìµœëŒ€ ê°œìˆ˜ ì œí•œ: {removed}ê°œ ì œê±° (max={max_results})")
                filtered = filtered[:max_results]

            # 5ë‹¨ê³„: ìµœì†Œ ê°œìˆ˜ ë³´ì¥ (ì•ˆì „ë§)
            if len(filtered) < self.min_num_results and len(candidates) >= self.min_num_results:
                print(f"[SCORE] ìµœì†Œ ê°œìˆ˜ ë³´ì¥: threshold ë¬´ì‹œí•˜ê³  {self.min_num_results}ê°œ ì„ íƒ")
                filtered = candidates[:self.min_num_results]

            # 6ë‹¨ê³„: ê²°ê³¼ ë¡œê¹…
            removed_count = len(candidates) - len(filtered)
            if removed_count > 0:
                print(f"[SCORE] Score-based í•„í„°ë§: {removed_count}ê°œ ë¬¸ì„œ ì œê±° (threshold={threshold:.4f})")
                print(f"       ìµœì¢… ì„ íƒ: {len(filtered)}ê°œ ë¬¸ì„œ (ì ìˆ˜ ë²”ìœ„: {filtered[0][1]:.4f} ~ {filtered[-1][1]:.4f})")

            return filtered

        except Exception as e:
            print(f"[WARN] Score-based í•„í„°ë§ ì˜¤ë¥˜: {e}, ì›ë³¸ ë°˜í™˜")
            import traceback
            traceback.print_exc()
            return candidates

    def _detect_exhaustive_query(self, question: str) -> bool:
        """ì „ì²´ ë¬¸ì„œê°€ í•„ìš”í•œ ì¿¼ë¦¬ì¸ì§€ ê°ì§€ (Option 1: í‚¤ì›Œë“œ ê¸°ë°˜)

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            True if exhaustive retrieval needed
        """
        if not self.enable_exhaustive_retrieval:
            return False

        # ìš°ì„ ìˆœìœ„ ë†’ì€ í‚¤ì›Œë“œ (ëª…í™•í•œ ì „ì²´ ìš”êµ¬)
        high_priority_keywords = [
            "ëª¨ë“  ", "ì „ì²´ ", "ëª¨ë‘ ", "ê°ê°ì˜ ", "ì „ë¶€ ",
            "ëª¨ë“ í˜ì´ì§€", "ëª¨ë“ ìŠ¬ë¼ì´ë“œ", "ì „ì²´ëª©ë¡", "ì „ì²´ë‚´ìš©",
            "ëª¨ë“ ì œëª©", "ê°í˜ì´ì§€", "ê°ìŠ¬ë¼ì´ë“œ"
        ]

        # ì¤‘ê°„ ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ (ë¬¸ë§¥ìƒ ì „ì²´ ì˜ë¯¸)
        medium_priority_keywords = [
            "ì „ì²´ì ìœ¼ë¡œ", "ë¦¬ìŠ¤íŠ¸", "ëª©ë¡", "ê°ê°"
        ]

        question_lower = question.lower()

        # ê³ ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ ì²´í¬ (ê³µë°± í¬í•¨ìœ¼ë¡œ ì˜¤íƒ ë°©ì§€)
        for keyword in high_priority_keywords:
            if keyword in question_lower:
                print(f"[EXHAUSTIVE] í‚¤ì›Œë“œ ê°ì§€: '{keyword}' â†’ ëŒ€ëŸ‰ ë¬¸ì„œ ëª¨ë“œ")
                return True

        # ì¤‘ê°„ ìš°ì„ ìˆœìœ„ í‚¤ì›Œë“œ ì²´í¬ (ë¬¸ë§¥ í™•ì¸)
        for keyword in medium_priority_keywords:
            # í‚¤ì›Œë“œ ì•ë’¤ë¡œ ê³µë°±ì´ ìˆê±°ë‚˜, ì‹œì‘/ëì— ìˆëŠ” ê²½ìš° ë§¤ì¹­
            padded_question = f" {question_lower} "
            if f" {keyword} " in padded_question or f" {keyword}" in padded_question:
                print(f"[EXHAUSTIVE] í‚¤ì›Œë“œ ê°ì§€: '{keyword}' â†’ ëŒ€ëŸ‰ ë¬¸ì„œ ëª¨ë“œ")
                return True

        return False

    def _is_single_file_query(self, question: str, candidates: List[tuple]) -> bool:
        """ë‹¨ì¼ íŒŒì¼ì— ëŒ€í•œ ì „ì²´ ì¡°íšŒì¸ì§€ íŒë‹¨ (Option 2: íŒŒì¼ ê¸°ë°˜)

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            candidates: ê²€ìƒ‰ëœ ë¬¸ì„œ í›„ë³´

        Returns:
            True if single file complete retrieval needed
        """
        if not self.enable_single_file_optimization or not candidates:
            return False

        # "ì´ ìŠ¬ë¼ì´ë“œ", "í•´ë‹¹ íŒŒì¼", "ì´ ë¬¸ì„œ" ë“±ì˜ í‚¤ì›Œë“œ
        file_specific_keywords = [
            "ì´ ìŠ¬ë¼ì´ë“œ", "í•´ë‹¹ ìŠ¬ë¼ì´ë“œ", "í˜„ì¬ ìŠ¬ë¼ì´ë“œ",
            "ì´ íŒŒì¼", "í•´ë‹¹ íŒŒì¼", "í˜„ì¬ íŒŒì¼",
            "ì´ ë¬¸ì„œ", "í•´ë‹¹ ë¬¸ì„œ", "í˜„ì¬ ë¬¸ì„œ",
            "ì´ ë…¼ë¬¸", "í•´ë‹¹ ë…¼ë¬¸"
        ]

        has_file_keyword = any(kw in question for kw in file_specific_keywords)

        # ëª¨ë“  í›„ë³´ê°€ ê°™ì€ íŒŒì¼ì—ì„œ ì˜¨ ê²ƒì¸ì§€ í™•ì¸
        file_names = set()
        for doc, _ in candidates:
            file_name = doc.metadata.get("file_name", "")
            if file_name:
                file_names.add(file_name)

        is_single_file = len(file_names) == 1

        if has_file_keyword and is_single_file:
            file_name = list(file_names)[0]
            print(f"[SINGLE_FILE] ë‹¨ì¼ íŒŒì¼ ì „ì²´ ì¡°íšŒ ê°ì§€: '{file_name}'")
            return True

        return False

    def _count_file_chunks(self, candidates: List[tuple], file_name: str = None) -> int:
        """íŠ¹ì • íŒŒì¼ì˜ ì´ ì²­í¬ ìˆ˜ ê³„ì‚°

        Args:
            candidates: ê²€ìƒ‰ëœ ë¬¸ì„œ í›„ë³´
            file_name: íŒŒì¼ëª… (Noneì´ë©´ ì²« ë²ˆì§¸ ë¬¸ì„œì˜ íŒŒì¼)

        Returns:
            ì²­í¬ ìˆ˜
        """
        if not candidates:
            return 0

        if file_name is None:
            file_name = candidates[0][0].metadata.get("file_name", "")

        chunk_count = sum(
            1 for doc, _ in candidates
            if doc.metadata.get("file_name", "") == file_name
        )

        return chunk_count

    def _adaptive_max_results(self, question: str, candidates: List[tuple]) -> int:
        """ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ìµœëŒ€ ë¬¸ì„œ ìˆ˜ ê²°ì • (3ë‹¨ê³„ í´ë°± ì „ëµ)

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            candidates: ê²€ìƒ‰ëœ ë¬¸ì„œ í›„ë³´

        Returns:
            ìµœëŒ€ ë¬¸ì„œ ìˆ˜
        """
        # ì•ˆì „ì¥ì¹˜: candidatesê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
        if not candidates:
            return self.max_num_results

        # ìš°ì„ ìˆœìœ„ 1: Exhaustive query ê°ì§€ (Option 1)
        if self._detect_exhaustive_query(question):
            max_results = min(self.exhaustive_max_results, len(candidates))
            # ìµœì†Œê°’ ë³´ì¥ (exhaustiveì´ì§€ë§Œ í›„ë³´ê°€ ì ì„ ìˆ˜ ìˆìŒ)
            max_results = max(max_results, self.min_num_results)
            print(f"[ADAPTIVE] Exhaustive mode â†’ max={max_results}")
            return max_results

        # ìš°ì„ ìˆœìœ„ 2: ë‹¨ì¼ íŒŒì¼ ì „ì²´ ì¡°íšŒ (Option 2)
        if self._is_single_file_query(question, candidates):
            file_chunks = self._count_file_chunks(candidates)
            max_results = min(file_chunks, self.exhaustive_max_results)
            # ìµœì†Œê°’ ë³´ì¥
            max_results = max(max_results, self.min_num_results)
            print(f"[ADAPTIVE] Single file mode â†’ max={max_results} (file chunks)")
            return max_results

        # ìš°ì„ ìˆœìœ„ 3: LLM íŒë‹¨ í™œìš© (Option 3)
        # determine_optimal_top_k()ëŠ” ì´ë¯¸ í˜¸ì¶œë˜ì–´ ìˆìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©
        print(f"[ADAPTIVE] Default mode â†’ max={self.max_num_results}")
        return self.max_num_results

    def _detect_query_type(self, question: str) -> str:
        """ì¿¼ë¦¬ íƒ€ì… ê°ì§€ (êµ¬ì²´ì  ì •ë³´ ì¶”ì¶œ, ìš”ì•½, ë¹„êµ, ê´€ê³„ ë¶„ì„ ë“±)"""
        question_lower = question.lower()

        # êµ¬ì²´ì  ì •ë³´ ì¶”ì¶œ í‚¤ì›Œë“œ
        specific_keywords = ["ë¬´ì—‡ì¸ê°€", "ì–¼ë§ˆì¸ê°€", "ëˆ„êµ¬ì¸ê°€", "ì–¸ì œ", "ì–´ë””",
                           "ì–´ë–¤", "ë‚˜ì—´", "ì¶”ì¶œ", "ìˆ˜ì¹˜", "ê°’", "ì´ë¦„", "êµ¬ì¡°"]
        if any(keyword in question_lower for keyword in specific_keywords):
            return "specific_info"

        # ìš”ì•½ í‚¤ì›Œë“œ
        summary_keywords = ["ìš”ì•½", "ì •ë¦¬", "í•µì‹¬", "ì£¼ìš” ë‚´ìš©", "ê°œìš”", "ê°œìš”"]
        if any(keyword in question_lower for keyword in summary_keywords):
            return "summary"

        # ë¹„êµ ë¶„ì„ í‚¤ì›Œë“œ
        comparison_keywords = ["ë¹„êµ", "ì°¨ì´", "ëŒ€ë¹„", "ì–´ëŠ ê²ƒì´", "vs", "versus"]
        if any(keyword in question_lower for keyword in comparison_keywords):
            return "comparison"

        # ê´€ê³„ ë¶„ì„ í‚¤ì›Œë“œ
        relationship_keywords = ["ê´€ê³„", "ìƒê´€ê´€ê³„", "ê²½í–¥", "ì˜í–¥", "ë©”ì»¤ë‹ˆì¦˜", "ì›ì¸"]
        if any(keyword in question_lower for keyword in relationship_keywords):
            return "relationship"

        # ê¸°ë³¸ê°’
        return "general"

    def _detect_question_category(self, question: str) -> List[str]:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ê°ì§€

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ (technical/business/hr/safety/reference)
            ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ê°€ ê´€ë ¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        """
        # TEMPORARY: ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ë¹„í™œì„±í™”
        print(f"  [INFO] ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ë¹„í™œì„±í™”ë¨")
        return []

        # Few-shot í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì´ ì–´ë–¤ ì¹´í…Œê³ ë¦¬ì˜ ë¬¸ì„œë¥¼ í•„ìš”ë¡œ í•˜ëŠ”ì§€ ë¶„ì„í•˜ì„¸ìš”.

**ì¹´í…Œê³ ë¦¬ ì •ì˜:**
- technical: ê³¼í•™, ê¸°ìˆ , ì—°êµ¬, OLED, ë””ìŠ¤í”Œë ˆì´, ê³µí•™, í•™ìˆ  ë‚´ìš©
- business: ì‚¬ì—…, ë‰´ìŠ¤, ì œí’ˆ ë°œí‘œ, ë§ˆì¼€íŒ…, ì‹œì¥ ë¶„ì„
- hr: ì¸ì‚¬, êµìœ¡, ì¶œê²° ê´€ë¦¬, ì§ì› ê´€ë¦¬
- safety: ì•ˆì „, ê·œì •, ìœ„í—˜ ê´€ë¦¬, ë³´ê±´
- reference: ì¼ë°˜ ì°¸ê³  ìë£Œ

**ë¶„ë¥˜ ì˜ˆì‹œ:**
1. ì§ˆë¬¸: "TADF ì¬ë£Œì˜ ì–‘ì íš¨ìœ¨ì€?"
   ì¹´í…Œê³ ë¦¬: technical

2. ì§ˆë¬¸: "LGë””ìŠ¤í”Œë ˆì´ì˜ ì‹ ì œí’ˆ ì¶œì‹œì¼ì€?"
   ì¹´í…Œê³ ë¦¬: business

3. ì§ˆë¬¸: "ì¶œê²° ì‹œìŠ¤í…œ ë¡œê·¸ì¸ ë°©ë²•ì€?"
   ì¹´í…Œê³ ë¦¬: hr

4. ì§ˆë¬¸: "ì‘ì—…ì¥ ì•ˆì „ ìˆ˜ì¹™ì€?"
   ì¹´í…Œê³ ë¦¬: safety

5. ì§ˆë¬¸: "ë¶„ì êµ¬ì¡°ì™€ ì„±ëŠ¥ì˜ ê´€ê³„ëŠ”?"
   ì¹´í…Œê³ ë¦¬: technical

6. ì§ˆë¬¸: "HRD-Net ì‹œìŠ¤í…œ ì‚¬ìš©ë²•ì€?"
   ì¹´í…Œê³ ë¦¬: hr

**ë¶„ì„ ëŒ€ìƒ:**
ì§ˆë¬¸: {question}

**ì§€ì‹œì‚¬í•­:**
1. ì§ˆë¬¸ì˜ ì£¼ì œì™€ ì˜ë„ë¥¼ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”
2. ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ê°€ ê´€ë ¨ë  ìˆ˜ ìˆìœ¼ë©´ ëª¨ë‘ ë‚˜ì—´í•˜ì„¸ìš” (ìµœëŒ€ 2ê°œ)
3. ì‘ë‹µì€ ì¹´í…Œê³ ë¦¬ ì´ë¦„ë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš” (ì†Œë¬¸ì, ì¶”ê°€ ì„¤ëª… ì—†ì´)
4. ì˜ˆ: "technical" ë˜ëŠ” "technical,business"

ì¹´í…Œê³ ë¦¬:"""

        try:
            # LLM í˜¸ì¶œ (LLMì˜ invoke ë©”ì„œë“œ ì‚¬ìš©)
            response = self.llm.invoke(prompt)

            # ì‘ë‹µì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            categories_str = response.strip().lower()
            categories = [c.strip() for c in categories_str.split(",")]

            # ìœ íš¨í•œ ì¹´í…Œê³ ë¦¬ë§Œ í•„í„°ë§
            valid_categories = ["technical", "business", "hr", "safety", "reference"]
            filtered_categories = [c for c in categories if c in valid_categories]

            if filtered_categories:
                print(f"  [OK] ì§ˆë¬¸ ì¹´í…Œê³ ë¦¬ ê°ì§€: {', '.join(filtered_categories)}")
                return filtered_categories
            else:
                # ìœ íš¨í•˜ì§€ ì•Šì€ ì‘ë‹µì´ë©´ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ë°˜í™˜ (í•„í„°ë§ ì—†ìŒ)
                print(f"  [WARN] ì•Œ ìˆ˜ ì—†ëŠ” ì¹´í…Œê³ ë¦¬ ì‘ë‹µ '{categories_str}', í•„í„°ë§ ë¹„í™œì„±í™”")
                return []

        except Exception as e:
            print(f"  [WARN] ì¹´í…Œê³ ë¦¬ ê°ì§€ ì‹¤íŒ¨ ({e}), í•„í„°ë§ ë¹„í™œì„±í™”")
            return []

    def _filter_by_category(self, results: List[tuple], target_categories: List[str]) -> List[tuple]:
        """ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ í•„í„°ë§

        Args:
            results: (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            target_categories: ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸

        Returns:
            í•„í„°ë§ëœ (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        # ì¹´í…Œê³ ë¦¬ê°€ ë¹„ì–´ìˆìœ¼ë©´ í•„í„°ë§ í•˜ì§€ ì•ŠìŒ
        if not target_categories:
            return results

        filtered_results = []
        for doc, score in results:
            doc_category = doc.metadata.get("category", "reference")

            # ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ê°€ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ì™€ ì¼ì¹˜í•˜ë©´ í¬í•¨
            if doc_category in target_categories:
                filtered_results.append((doc, score))

        # í•„í„°ë§ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìœ¼ë©´ (3ê°œ ë¯¸ë§Œ) ì›ë³¸ ë°˜í™˜ (ë„ˆë¬´ ì—„ê²©í•œ í•„í„°ë§ ë°©ì§€)
        if len(filtered_results) < 3:
            print(f"  [WARN] ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ê²°ê³¼ ë¶€ì¡± ({len(filtered_results)}ê°œ), í•„í„°ë§ ë¹„í™œì„±í™”")
            return results

        print(f"  [OK] ì¹´í…Œê³ ë¦¬ í•„í„°ë§: {len(results)}ê°œ â†’ {len(filtered_results)}ê°œ (ì¹´í…Œê³ ë¦¬: {', '.join(target_categories)})")
        return filtered_results

    def _get_context(self, question: str, chat_history: List[Dict] = None, search_mode: str = "integrated") -> str:
        context_start = time.perf_counter()

        # ========== Quick Wins: ì§ˆë¬¸ ë¶„ë¥˜ ë° íŒŒë¼ë¯¸í„° ìµœì í™” ==========
        if hasattr(self, 'question_classifier') and self.question_classifier:
            try:
                classification = self.question_classifier.classify(question)

                # ë¶„ë¥˜ ê²°ê³¼ ì €ì¥ (UI í‘œì‹œìš©)
                self._last_classification = classification

                # ë¡œê¹… (verbose ëª¨ë“œì—ì„œë§Œ ìƒì„¸ ì¶œë ¥)
                logger.info(f"ğŸ¯ ì§ˆë¬¸ ìœ í˜•: {classification['type']} "
                           f"(ì‹ ë¢°ë„: {classification['confidence']:.0%}, "
                           f"ë°©ë²•: {classification['method']})")

                # íŒŒë¼ë¯¸í„° ë™ì  ì¡°ì •
                self.enable_multi_query = classification['multi_query']
                self.max_num_results = classification['max_results']
                self.reranker_initial_k = classification['reranker_k']
                self.max_tokens = classification['max_tokens']

                # LLM max_tokens ì„¤ì • (API íƒ€ì…ë³„)
                if hasattr(self.llm, 'max_tokens'):
                    self.llm.max_tokens = classification['max_tokens']
                elif hasattr(self.llm, 'num_predict'):
                    # Ollamaì˜ ê²½ìš°
                    self.llm.num_predict = classification['max_tokens']

                logger.info(f"âš™ï¸  ìµœì í™”: Multi-Query={classification['multi_query']}, "
                           f"MaxResults={classification['max_results']}, "
                           f"RerankK={classification['reranker_k']}, "
                           f"MaxTokens={classification['max_tokens']}")
            except Exception as e:
                logger.warning(f"ì§ˆë¬¸ ë¶„ë¥˜ ì‹¤íŒ¨, ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©: {e}")
                self._last_classification = None
        else:
            self._last_classification = None
        # ================================================================

        # Chat history ìºì‹œ ì—…ë°ì´íŠ¸
        if chat_history:
            self._chat_history_cache = chat_history

        # ì¹´í…Œê³ ë¦¬ ê°ì§€ (Phase 1: ì£¼ì œ ì¼ê´€ì„± ê²€ì¦)
        categories = self._detect_question_category(question)

        # ì¿¼ë¦¬ íƒ€ì… ê°ì§€
        query_type = self._detect_query_type(question)
        
        # êµ¬ì²´ì  ì •ë³´ ì¶”ì¶œ ëª¨ë“œ: Small-to-Large ê²€ìƒ‰ í™œìš©
        if query_type == "specific_info":
            try:
                # 1ë‹¨ê³„: Small-to-Large ê²€ìƒ‰ìœ¼ë¡œ ì •í™•í•œ ì²­í¬ ì°¾ê¸°
                stl_results = self.small_to_large_search.search_with_context_expansion(
                    question, top_k=20, max_parents=5, partial_context_size=self.small_to_large_context_size
                )
                
                if stl_results:
                    # Small-to-Large ê²°ê³¼ë¥¼ (doc, score) í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
                    weighted_results = []
                    for doc in stl_results:
                        # ì²­í¬ íƒ€ì…ë³„ ê°€ì¤‘ì¹˜ ì ìš©
                        chunk_type_weight = doc.metadata.get("chunk_type_weight", 1.0)
                        # ê¸°ë³¸ ì ìˆ˜ (Small-to-LargeëŠ” ì •í™•í•œ ë§¤ì¹­ì„ ìš°ì„ í•˜ë¯€ë¡œ ë†’ì€ ì ìˆ˜)
                        base_score = 0.8 * chunk_type_weight
                        weighted_results.append((doc, base_score))
                    
                    # ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ì ìš© (Phase 1)
                    weighted_results = self._filter_by_category(weighted_results, categories)

                    # Re-ranking ì ìš© (ìˆëŠ” ê²½ìš°)
                    if self.use_reranker and len(weighted_results) > 0:
                        docs_for_rerank = [{
                            "page_content": d.page_content,
                            "metadata": d.metadata,
                            "vector_score": s,
                            "document": d
                        } for d, s in weighted_results]
                        reranked = self.reranker.rerank(question, docs_for_rerank, top_k=min(15, len(docs_for_rerank)))
                        pairs = [(d["document"], d.get("rerank_score", 0.8)) for d in reranked]
                    else:
                        pairs = weighted_results
                    
                    # ì¤‘ë³µ ì œê±°
                    dedup = self._unique_by_file(pairs, self.top_k * 2)
                    self._last_retrieved_docs = dedup[:self.top_k]
                    docs = [d for d, _ in self._last_retrieved_docs]
                    elapsed = time.perf_counter() - context_start
                    print(f"[Timing] context retrieval (Small-to-Large, type={query_type}): {elapsed:.2f}s")
                    print(f"[SEARCH] êµ¬ì²´ì  ì •ë³´ ì¶”ì¶œ ëª¨ë“œ: Small-to-Large ê²€ìƒ‰ (ì¿¼ë¦¬ íƒ€ì…: {query_type})")
                    return self._format_docs(docs)
            except Exception as e:
                print(f"Small-to-Large ê²€ìƒ‰ ì‹¤íŒ¨, ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±: {e}")
                # í´ë°±: ê¸°ë³¸ ê²€ìƒ‰ ê³„ì† ì§„í–‰
        
        # ìš”ì•½ ëª¨ë“œ: ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
        if query_type == "summary":
            # ìš”ì•½ì€ ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ í•„ìš”
            original_top_k = self.top_k
            self.top_k = min(10, original_top_k * 2)
            try:
                context = self._get_context_standard(question, categories, search_mode)
                elapsed = time.perf_counter() - context_start
                print(f"[Timing] context retrieval (summary, type={query_type}): {elapsed:.2f}s")
                self.top_k = original_top_k
                return context
            except:
                self.top_k = original_top_k
                return ""

        # ê¸°ë³¸ ê²€ìƒ‰ (ê¸°ì¡´ ë¡œì§)
        context = self._get_context_standard(question, categories, search_mode)
        elapsed = time.perf_counter() - context_start
        print(f"[Timing] context retrieval (standard, type={query_type}): {elapsed:.2f}s")
        return context

    def _get_context_standard(self, question: str, categories: List[str] = None, search_mode: str = "integrated") -> str:
        """í‘œì¤€ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
        if categories is None:
            categories = []
        overall_start = time.perf_counter()
        
        # ğŸ†• ë™ì  top_k ê²°ì • (ì§ˆë¬¸ íŠ¹ì„± ë¶„ì„)
        dynamic_top_k = self.determine_optimal_top_k(question)
        print(f"[SEARCH] ì§ˆë¬¸ íŠ¹ì„± ë¶„ì„: top_k = {dynamic_top_k} (ê¸°ë³¸: {self.top_k})")
        
        # Multi-Query Rewriting ì ìš©
        if self.enable_multi_query:
            mq_start = time.perf_counter()
            queries = self.generate_rewritten_queries(question, num_queries=self.multi_query_num)
            print(f"[Timing] multi_query_generate: {time.perf_counter() - mq_start:.2f}s (queries={len(queries)})")
            all_retrieved_chunks = []
            chunk_id_set = set()
            
            # ëª¨ë“  ì¿¼ë¦¬ì— ëŒ€í•´ ê²€ìƒ‰ ìˆ˜í–‰
            for idx, query in enumerate(queries, start=1):
                query_start = time.perf_counter()
                try:
                    results = []
                    if self.use_reranker:
                        base = self._search_candidates(query, search_mode=search_mode)
                        if base:
                            docs_for_rerank = [{
                                "page_content": d.page_content,
                                "metadata": d.metadata,
                                "vector_score": s,
                                "document": d
                            } for d, s in base]
                            reranked = self.reranker.rerank(query, docs_for_rerank, top_k=max(self.top_k * 3, 15))
                            results = [(d["document"], d.get("rerank_score", 0)) for d in reranked]
                        else:
                            results = []
                    else:
                        # ë“€ì–¼ DB ì§€ì›: search_with_mode ì‚¬ìš© ê°€ëŠ¥ ì‹œ ì‚¬ìš©
                        if hasattr(self.vectorstore, 'search_with_mode'):
                            temp_results = self.vectorstore.search_with_mode(
                                query=query,
                                search_mode=search_mode,
                                initial_k=max(self.top_k * 3, 15),
                                top_k=max(self.top_k * 3, 15),
                                use_reranker=False,  # ì´ë¯¸ rerankerëŠ” ì™¸ë¶€ì—ì„œ ì²˜ë¦¬
                                reranker_model=self.reranker_model
                            )
                            results = temp_results if temp_results else []
                        else:
                            results = self.vectorstore.similarity_search_with_score(query, k=max(self.top_k * 3, 15))

                    # ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ì ìš©
                    results = self._filter_by_category(results, categories)

                    print(f"[Timing] retrieval[{idx}/{len(queries)}]: {time.perf_counter() - query_start:.2f}s (docs={len(results)})")
                    
                    # ì¤‘ë³µ ì œê±° (ë¬¸ì„œ ë‚´ìš© ê¸°ì¤€)
                    for doc, score in results:
                        doc_id = f"{doc.metadata.get('source', '')}_{doc.page_content[:50]}"
                        if doc_id not in chunk_id_set:
                            all_retrieved_chunks.append((doc, score))
                            chunk_id_set.add(doc_id)
                            
                except Exception as e:
                    print(f"ì¿¼ë¦¬ '{query}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    continue
            
            if all_retrieved_chunks:
                # ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ì ìš© (ìµœì¢… í†µí•©)
                all_retrieved_chunks = self._filter_by_category(all_retrieved_chunks, categories)

                # ì›ë³¸ ì¿¼ë¦¬ë¡œ ì¬ìˆœìœ„ ë§¤ê¹€
                if self.use_reranker:
                    rerank_start = time.perf_counter()
                    docs_for_final_rerank = [{
                        "page_content": d.page_content,
                        "metadata": d.metadata,
                        "vector_score": s,
                        "document": d
                    } for d, s in all_retrieved_chunks]
                    final_reranked = self.reranker.rerank(question, docs_for_final_rerank, top_k=max(self.top_k * 2, 20))
                    pairs = [(d["document"], d.get("rerank_score", 0)) for d in final_reranked]
                    print(f"[Timing] final_rerank (multi-query): {time.perf_counter() - rerank_start:.2f}s (candidates={len(all_retrieved_chunks)})")
                else:
                    pairs = all_retrieved_chunks

                # ğŸ†• Score-based í•„í„°ë§ íŒŒì´í”„ë¼ì¸ (OpenAI ìŠ¤íƒ€ì¼ + Adaptive)
                filter_start = time.perf_counter()

                # 1ë‹¨ê³„: í†µê³„ ê¸°ë°˜ ì´ìƒì¹˜ ì œê±° (ì´ìƒ ì ìˆ˜ ì œê±°)
                pairs = self._statistical_outlier_removal(pairs, method='mad')

                # 2ë‹¨ê³„: Score-based filtering (ì ìˆ˜ + ê°œìˆ˜ í•˜ì´ë¸Œë¦¬ë“œ + Adaptive)
                pairs = self._score_based_filtering(pairs, question=question)

                print(f"[Timing] score_filtering: {time.perf_counter() - filter_start:.2f}s")

                # ì¤‘ë³µ ì œê±° (íŒŒì¼ ë‹¨ìœ„)
                dedup = self._unique_by_file(pairs, len(pairs))  # score filteringì—ì„œ ì´ë¯¸ ê°œìˆ˜ ì œí•œ
                self._last_retrieved_docs = dedup
                docs = [d for d, _ in dedup]
                print(f"[Timing] context_standard total: {time.perf_counter() - overall_start:.2f}s (mode=multi-query, docs={len(docs)})")
                return self._format_docs(docs)
        
        # í´ë°±: ë‹¨ì¼ ì¿¼ë¦¬ ê²€ìƒ‰ (ë™ì˜ì–´ í™•ì¥ í¬í•¨)
        syn_start = time.perf_counter()
        expanded_question = self.expand_query_with_synonyms(question)
        print(f"[Timing] synonym_expand: {time.perf_counter() - syn_start:.2f}s")
        
        if self.use_reranker:
            retrieval_start = time.perf_counter()
            base = self._search_candidates(expanded_question, search_mode=search_mode)
            if not base:
                self._last_retrieved_docs = []
                print(f"[Timing] context_standard total: {time.perf_counter() - overall_start:.2f}s (mode=fallback, docs=0)")
                return ""
            
            # base ëŠ” (doc, score) í˜•íƒœ
            docs_for_rerank = [{
                "page_content": d.page_content,
                "metadata": d.metadata,
                "vector_score": s,
                "document": d
            } for d, s in base]
            print(f"[Timing] candidate_retrieval (fallback): {time.perf_counter() - retrieval_start:.2f}s (candidates={len(base)})")
            rerank_start = time.perf_counter()
            reranked = self.reranker.rerank(expanded_question, docs_for_rerank, top_k=max(self.top_k * 8, 40))
            pairs = [(d["document"], d.get("rerank_score", 0)) for d in reranked]
            print(f"[Timing] final_rerank (fallback): {time.perf_counter() - rerank_start:.2f}s")

            # ğŸ†• Score-based í•„í„°ë§ íŒŒì´í”„ë¼ì¸ (OpenAI ìŠ¤íƒ€ì¼ + Adaptive)
            filter_start = time.perf_counter()

            # 1ë‹¨ê³„: í†µê³„ ê¸°ë°˜ ì´ìƒì¹˜ ì œê±° (ì´ìƒ ì ìˆ˜ ì œê±°)
            pairs = self._statistical_outlier_removal(pairs, method='mad')

            # 2ë‹¨ê³„: Score-based filtering (ì ìˆ˜ + ê°œìˆ˜ í•˜ì´ë¸Œë¦¬ë“œ + Adaptive)
            pairs = self._score_based_filtering(pairs, question=question)

            print(f"[Timing] score_filtering: {time.perf_counter() - filter_start:.2f}s")

            # ì¤‘ë³µ ì œê±° (íŒŒì¼ ë‹¨ìœ„)
            dedup = self._unique_by_file(pairs, len(pairs))  # score filteringì—ì„œ ì´ë¯¸ ê°œìˆ˜ ì œí•œ

            # ìºì‹œ ì €ì¥: ì‹¤ì œ ì‚¬ìš©ëœ ë¬¸ì„œì™€ ì ìˆ˜
            self._last_retrieved_docs = dedup  # [(doc, score), ...]

            docs = [d for d, _ in dedup]
            print(f"[Timing] deduplication: {time.perf_counter() - rerank_start:.2f}s (selected={len(dedup)})")
        else:
            retrieval_start = time.perf_counter()
            # ë“€ì–¼ DB ì§€ì›: search_with_mode ì‚¬ìš© ê°€ëŠ¥ ì‹œ ì‚¬ìš©
            if hasattr(self.vectorstore, 'search_with_mode'):
                pairs = self.vectorstore.search_with_mode(
                    query=expanded_question,
                    search_mode=search_mode,
                    initial_k=max(self.top_k * 8, 40),
                    top_k=max(self.top_k * 8, 40),
                    use_reranker=False,
                    reranker_model=self.reranker_model
                )
                if not pairs:
                    pairs = []
            else:
                pairs = self.vectorstore.similarity_search_with_score(expanded_question, k=max(self.top_k * 8, 40))
            # ë„ë©”ì¸ í•„í„°ë§ ì ìš©

            # ğŸ†• Score-based í•„í„°ë§ íŒŒì´í”„ë¼ì¸ (OpenAI ìŠ¤íƒ€ì¼ + Adaptive)
            filter_start = time.perf_counter()

            # 1ë‹¨ê³„: í†µê³„ ê¸°ë°˜ ì´ìƒì¹˜ ì œê±° (ì´ìƒ ì ìˆ˜ ì œê±°)
            pairs = self._statistical_outlier_removal(pairs, method='mad')

            # 2ë‹¨ê³„: Score-based filtering (ì ìˆ˜ + ê°œìˆ˜ í•˜ì´ë¸Œë¦¬ë“œ + Adaptive)
            pairs = self._score_based_filtering(pairs, question=question)

            print(f"[Timing] score_filtering: {time.perf_counter() - filter_start:.2f}s")

            # ì¤‘ë³µ ì œê±° (íŒŒì¼ ë‹¨ìœ„)
            dedup = self._unique_by_file(pairs, len(pairs))  # score filteringì—ì„œ ì´ë¯¸ ê°œìˆ˜ ì œí•œ

            # ìºì‹œ ì €ì¥
            self._last_retrieved_docs = dedup

            docs = [d for d, _ in dedup]
            print(f"[Timing] candidate_retrieval (vector fallback): {time.perf_counter() - retrieval_start:.2f}s (selected={len(dedup)})")
        print(f"[Timing] context_standard total: {time.perf_counter() - overall_start:.2f}s (mode=fallback, top_k={dynamic_top_k})")
        return self._format_docs(docs)

    def expand_query_with_synonyms(self, original_query: str) -> str:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ ì¿¼ë¦¬ì— ëŒ€í•œ ë™ì˜ì–´/ì—°ê´€ì–´ë¥¼ ìƒì„±í•˜ê³  í™•ì¥ëœ ì¿¼ë¦¬ë¥¼ ë°˜í™˜"""
        if not self.enable_synonym_expansion:
            return original_query
            
        try:
            prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ ê²€ìƒ‰ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤. ì‚¬ìš©ì ì¿¼ë¦¬ì˜ ê²€ìƒ‰ íš¨ê³¼ë¥¼ ë†’ì´ê¸° ìœ„í•´ ë™ì˜ì–´ì™€ ì—°ê´€ì–´ë¥¼ ìƒì„±í•˜ì„¸ìš”.

**ì›ë³¸ ì¿¼ë¦¬**: "{original_query}"

**ìƒì„± ê·œì¹™**:
1. ë™ì˜ì–´: ì¿¼ë¦¬ì˜ í•µì‹¬ ê°œë…ê³¼ ë™ì¼í•œ ì˜ë¯¸ì˜ ë‹¤ë¥¸ í‘œí˜„
2. ìƒìœ„ì–´: ë” ì¼ë°˜ì ì¸ ê°œë…
3. í•˜ìœ„ì–´: ë” êµ¬ì²´ì ì¸ ê°œë…
4. ê´€ë ¨ì–´: ë°€ì ‘í•˜ê²Œ ì—°ê´€ëœ ê°œë…

**Few-shot ì˜ˆì‹œ**:
[ì˜ˆì‹œ 1]
ì›ë³¸: "OLED íš¨ìœ¨"
ë™ì˜ì–´: ["ìœ ê¸°ë°œê´‘ë‹¤ì´ì˜¤ë“œ íš¨ìœ¨", "OLED ì„±ëŠ¥", "ë°œê´‘ íš¨ìœ¨"]

[ì˜ˆì‹œ 2]
ì›ë³¸: "TADF ì¬ë£Œ"
ë™ì˜ì–´: ["ì—´í™œì„±í™” ì§€ì—° í˜•ê´‘ ì¬ë£Œ", "thermally activated delayed fluorescence", "TADF ì†Œì¬"]

[ì˜ˆì‹œ 3]
ì›ë³¸: "ë°œê´‘ íš¨ìœ¨ ì¸¡ì •"
ë™ì˜ì–´: ["ê´‘ì¶œë ¥ ì¸¡ì •", "luminescence efficiency", "ë°œê´‘ ì„±ëŠ¥ í‰ê°€"]

**ì¶œë ¥ í˜•ì‹**: JSON ë¦¬ìŠ¤íŠ¸
{{"synonyms": ["ìš©ì–´1", "ìš©ì–´2", "ìš©ì–´3"], "related_terms": ["ê´€ë ¨ì–´1", "ê´€ë ¨ì–´2"]}}

**ìƒì„±**:"""
            
            response = self.llm.invoke(prompt)
            
            # ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            if hasattr(response, 'content'):
                response_text = response.content
            elif hasattr(response, 'text'):
                response_text = response.text
            else:
                response_text = str(response)
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ë©€í‹°ë¼ì¸ JSON ì§€ì›)
                json_match = re.search(r'\{.*?\}', response_text, re.DOTALL)
                if json_match:
                    expansion_data = json.loads(json_match.group())
                    synonyms = expansion_data.get("synonyms", [])
                    related_terms = expansion_data.get("related_terms", [])
                    
                    # ëª¨ë“  ìš©ì–´ë¥¼ ê²°í•©
                    all_terms = synonyms + related_terms
                    if all_terms:
                        expanded_query = f"{original_query} ({', '.join(all_terms)})"
                    else:
                        expanded_query = original_query
                    
                    print(f"[SEARCH] ë™ì˜ì–´ í™•ì¥: {original_query} â†’ {expanded_query}")
                    return expanded_query
                else:
                    # JSON í˜•ì‹ì´ ì•„ë‹Œ ê²½ìš° í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
                    lines = response_text.strip().split('\n')
                    all_terms = []
                    for line in lines:
                        line = line.strip().strip('"[],')
                        if line and len(line) > 1 and not line.startswith('ë™ì˜ì–´') and not line.startswith('ê´€ë ¨ì–´'):
                            all_terms.append(line)
                    
                    if all_terms:
                        expanded_query = f"{original_query} ({', '.join(all_terms[:5])})"  # ìµœëŒ€ 5ê°œ
                    else:
                        expanded_query = original_query
                    
                    print(f"[SEARCH] ë™ì˜ì–´ í™•ì¥: {original_query} â†’ {expanded_query}")
                    return expanded_query
                    
            except (json.JSONDecodeError, ValueError) as e:
                print(f"ë™ì˜ì–´ íŒŒì‹± ì‹¤íŒ¨: {e}")
                
        except Exception as e:
            print(f"ë™ì˜ì–´ í™•ì¥ ì‹¤íŒ¨: {e}")
        
        return original_query

    def determine_optimal_top_k(self, question: str) -> int:
        """ì§ˆë¬¸ íŠ¹ì„±ì— ë”°ë¼ ìµœì ì˜ top_k ê°’ì„ ë™ì ìœ¼ë¡œ ê²°ì • (Option 3: LLM íŒë‹¨)"""
        try:
            prompt = f"""ë‹¹ì‹ ì€ RAG ê²€ìƒ‰ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸ íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ë¬¸ì„œ ê²€ìƒ‰ ê°œìˆ˜ë¥¼ ê²°ì •í•˜ì„¸ìš”.

**ì§ˆë¬¸**: "{question}"

**ë¶„ì„ ì ˆì°¨**:
1ë‹¨ê³„ [ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜]:
   - ë‹¨ì¼ ì‚¬ì‹¤ ì°¾ê¸°: "ë¬´ì—‡", "ì–¼ë§ˆ", "ëˆ„êµ¬", "ì–¸ì œ", "ì–´ë””" (ëª…í™•í•œ í•˜ë‚˜ì˜ ë‹µë³€)
   - ëª©ë¡ ë‚˜ì—´ (ì†ŒëŸ‰): "ë‚˜ì—´", "ëª©ë¡" (10~30ê°œ í•­ëª©)
   - ëª©ë¡ ë‚˜ì—´ (ëŒ€ëŸ‰): "ëª¨ë“ ", "ì „ì²´", "ê°ê°" (30ê°œ ì´ìƒ í•­ëª©)
   - ë¹„êµ/ë¶„ì„: "ì°¨ì´", "ë¹„êµ", "vs", "ëŒ€ë¹„", "ê´€ê³„" (ë‹¤ê°ë„ ë¶„ì„)
   - ì¢…í•© ì •ë³´: "ìš”ì•½", "í•µì‹¬", "ê°œìš”", "ì •ë¦¬" (ì „ì²´ ì»¨í…ìŠ¤íŠ¸)
   - ë³µí•© ì§ˆë¬¸: ì—¬ëŸ¬ ìœ í˜•ì´ í˜¼í•©ëœ ê²½ìš°

2ë‹¨ê³„ [ë³µì¡ë„ í‰ê°€]:
   - ë‚®ìŒ: ë‹¨ìˆœí•œ ì‚¬ì‹¤ í™•ì¸ (3-5ê°œ)
   - ì¤‘ê°„: ë¹„êµ/ë¶„ì„, ê¸°ë³¸ ì¢…í•© (10-20ê°œ)
   - ë†’ìŒ: ëª©ë¡ ë‚˜ì—´ (ì†ŒëŸ‰), ë³µí•© ì§ˆë¬¸ (30-50ê°œ)
   - ë§¤ìš° ë†’ìŒ: ì „ì²´ ëª©ë¡ ë‚˜ì—´, ìŠ¬ë¼ì´ë“œ/í˜ì´ì§€ ì „ì²´ (50-100ê°œ)

**Few-shot ì˜ˆì‹œ**:
[ì˜ˆì‹œ 1]
ì§ˆë¬¸: "OLED íš¨ìœ¨ì€ ì–¼ë§ˆì¸ê°€?"
ìœ í˜•: ë‹¨ì¼ ì‚¬ì‹¤ ì°¾ê¸°
ë³µì¡ë„: ë‚®ìŒ
ì¶”ì²œ ê°œìˆ˜: 5

[ì˜ˆì‹œ 2]
ì§ˆë¬¸: "ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ ì¬ë£Œë¥¼ ë‚˜ì—´í•´ì£¼ì„¸ìš”."
ìœ í˜•: ëª©ë¡ ë‚˜ì—´ (ì†ŒëŸ‰)
ë³µì¡ë„: ë†’ìŒ
ì¶”ì²œ ê°œìˆ˜: 30

[ì˜ˆì‹œ 3]
ì§ˆë¬¸: "ëª¨ë“  ìŠ¬ë¼ì´ë“œì˜ ì œëª©ì„ ì•Œë ¤ì¤˜."
ìœ í˜•: ëª©ë¡ ë‚˜ì—´ (ëŒ€ëŸ‰)
ë³µì¡ë„: ë§¤ìš° ë†’ìŒ
ì¶”ì²œ ê°œìˆ˜: 80

[ì˜ˆì‹œ 4]
ì§ˆë¬¸: "ê° í˜ì´ì§€ì˜ ì£¼ìš” ë‚´ìš©ì„ ì •ë¦¬í•´ì¤˜."
ìœ í˜•: ì „ì²´ í˜ì´ì§€ ì¢…í•©
ë³µì¡ë„: ë§¤ìš° ë†’ìŒ
ì¶”ì²œ ê°œìˆ˜: 100

**ì¶œë ¥ í˜•ì‹**: ìˆ«ìë§Œ ì¶œë ¥ (ë²”ìœ„: 3-100)

**ë¶„ì„ ê²°ê³¼**:"""

            response = self.llm.invoke(prompt)
            response_text = response.content if hasattr(response, 'content') else str(response)

            # ìˆ«ì ì¶”ì¶œ
            numbers = re.findall(r'\d+', response_text)
            if numbers:
                top_k = int(numbers[0])
                top_k = max(3, min(100, top_k))  # 3~100 ë²”ìœ„ ì œí•œ (í™•ì¥)
                print(f"[LLM-TOPK] ë™ì  top_k ê²°ì •: {top_k} (ì§ˆë¬¸ ìœ í˜• ë¶„ì„)")
                return top_k
        except Exception as e:
            print(f"[WARN] ë™ì  top_k ê²°ì • ì‹¤íŒ¨: {e}")

        # í´ë°±: ê¸°ë³¸ê°’
        return self.top_k
    
    def generate_rewritten_queries(self, original_query: str, num_queries: int = 3) -> List[str]:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ ì¿¼ë¦¬ë¥¼ ì—¬ëŸ¬ ê´€ì ì—ì„œ ì¬ì‘ì„±í•œ ëŒ€ì•ˆ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±"""
        if not self.enable_multi_query:
            return [original_query]
            
        try:
            prompt = f"""ë‹¹ì‹ ì€ ê²€ìƒ‰ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì›ë³¸ ì¿¼ë¦¬ë¥¼ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì¬ì‘ì„±í•˜ì—¬ ê²€ìƒ‰ ë¦¬ì½œì„ í–¥ìƒì‹œí‚¤ì„¸ìš”.

**ì›ë³¸ ì¿¼ë¦¬**: "{original_query}"

**ì¬ì‘ì„± ì „ëµ** (ê°ê° 1ê°œì”© ìƒì„±):
1. **ê¸°ìˆ ì  ê´€ì **: êµ¬ì²´ì ì¸ ê¸°ìˆ  ìš©ì–´ì™€ ë°©ë²•ë¡  ì¤‘ì‹¬
2. **ê°œë…ì  ê´€ì **: ì¶”ìƒì  ê°œë…ê³¼ ì´ë¡  ì¤‘ì‹¬  
3. **ì‘ìš© ê´€ì **: ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€ì™€ ì ìš© ì¤‘ì‹¬
4. **ë¹„êµ ê´€ì **: ë¹„êµ ë¶„ì„ ì§ˆë¬¸ í˜•íƒœ (ì ìš© ê°€ëŠ¥í•œ ê²½ìš°)
5. **ë¬¸ì œ í•´ê²° ê´€ì **: ë¬¸ì œ ì •ì˜ ë° í•´ê²°ì±… ì¤‘ì‹¬ (ì ìš© ê°€ëŠ¥í•œ ê²½ìš°)

**Few-shot ì˜ˆì‹œ**:
[ì›ë³¸] "OLED íš¨ìœ¨ í–¥ìƒ ë°©ë²•"
[ì¬ì‘ì„±]
1. ê¸°ìˆ ì : "OLED ë°œê´‘ íš¨ìœ¨(luminous efficacy) ê°œì„  ê¸°ìˆ "
2. ê°œë…ì : "ìœ ê¸°ë°œê´‘ë‹¤ì´ì˜¤ë“œì˜ ê´‘ì¶œë ¥ í–¥ìƒ ì›ë¦¬"
3. ì‘ìš©: "OLED ë””ìŠ¤í”Œë ˆì´ íš¨ìœ¨ ìµœì í™” ì‚¬ë¡€"
4. ë¹„êµ: "OLED íš¨ìœ¨ ë¹„êµ: ë‹¤ë¥¸ ë””ìŠ¤í”Œë ˆì´ ê¸°ìˆ  ëŒ€ë¹„"
5. ë¬¸ì œí•´ê²°: "OLED íš¨ìœ¨ ì €í•˜ ì›ì¸ ë° í•´ê²°ì±…"

**ì¶œë ¥ í˜•ì‹**: JSON ë¦¬ìŠ¤íŠ¸
["ì¿¼ë¦¬1", "ì¿¼ë¦¬2", "ì¿¼ë¦¬3"]

**ì¬ì‘ì„±**:"""
            
            response = self.llm.invoke(prompt)
            
            # ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            if hasattr(response, 'content'):
                response_text = response.content
            elif hasattr(response, 'text'):
                response_text = response.text
            else:
                response_text = str(response)
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
                json_match = re.search(r'\[.*?\]', response_text)
                if json_match:
                    rewritten_queries = json.loads(json_match.group())
                else:
                    # JSON í˜•ì‹ì´ ì•„ë‹Œ ê²½ìš° í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
                    lines = response_text.strip().split('\n')
                    rewritten_queries = []
                    for line in lines:
                        line = line.strip().strip('"[]')
                        if line and len(line) > 1:
                            rewritten_queries.append(line)
                    rewritten_queries = rewritten_queries[:num_queries]  # ìµœëŒ€ num_queriesê°œ
                
                # ì›ë³¸ ì¿¼ë¦¬ê°€ í¬í•¨ë˜ì§€ ì•Šì•˜ë‹¤ë©´, ë¦¬ìŠ¤íŠ¸ì˜ ë§¨ ì•ì— ì¶”ê°€
                if original_query not in rewritten_queries:
                    rewritten_queries.insert(0, original_query)
                    
                print(f"[REWRITE] ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„±: {original_query} â†’ {len(rewritten_queries)}ê°œ ì¿¼ë¦¬")
                return rewritten_queries
                    
            except (json.JSONDecodeError, ValueError) as e:
                print(f"ë‹¤ì¤‘ ì¿¼ë¦¬ íŒŒì‹± ì‹¤íŒ¨: {e}")
                
        except Exception as e:
            print(f"ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return [original_query]

    def _format_chat_history(self, messages: List[Dict[str, str]], max_messages: int = 5) -> str:
        if not messages:
            return "ì´ì „ ëŒ€í™” ì—†ìŒ"
        recent_messages = messages[-max_messages * 2:] if len(messages) > max_messages * 2 else messages
        formatted = []
        for msg in recent_messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            if role == "user":
                formatted.append(f"ì‚¬ìš©ì: {content}")
            elif role == "assistant":
                formatted.append(f"ì–´ì‹œìŠ¤í„´íŠ¸: {content}")
        return "\n".join(formatted) if formatted else "ì´ì „ ëŒ€í™” ì—†ìŒ"

    def query(self, question: str, chat_history: List[Dict[str, str]] = None) -> Dict[str, Any]:
        try:
            formatted_history = self._format_chat_history(chat_history or [])
            
            # ì¿¼ë¦¬ íƒ€ì… ê°ì§€ ë° í”„ë¡¬í”„íŠ¸ ì„ íƒ
            query_type = self._detect_query_type(question)
            if query_type in self.prompt_templates:
                selected_template = self.prompt_templates[query_type]
                self.prompt = PromptTemplate(
                    template=selected_template,
                    input_variables=["chat_history", "context", "question"]
                )
                # ì²´ì¸ ì¬êµ¬ì„± (í”„ë¡¬í”„íŠ¸ ë³€ê²½ ë°˜ì˜)
                self.chain = (
                    {
                        "context": lambda x: self._get_context(x["question"]),
                        "chat_history": lambda x: x.get("chat_history", "ì´ì „ ëŒ€í™” ì—†ìŒ"),
                        "question": lambda x: x["question"]
                    }
                    | self.prompt
                    | self.llm
                    | StrOutputParser()
                )
            
            # ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (_last_retrieved_docs ì—…ë°ì´íŠ¸ë¨)
            context = self._get_context(question, chat_history)

            # Phase A-3: Self-Consistency Check ì ìš©
            consistency_score = 1.0  # ê¸°ë³¸ê°’
            if self.enable_self_consistency:
                # Self-Consistency ë‹µë³€ ìƒì„±
                sc_result = self._generate_with_self_consistency(
                    question=question,
                    context=context,
                    chat_history=formatted_history,
                    n=self.self_consistency_n,
                    enable=True
                )
                answer = sc_result['answer']
                consistency_score = sc_result['consistency']

                print(f"  [OK] Self-Consistency ì ìš© ì™„ë£Œ (ì¼ê´€ì„±: {consistency_score:.2%})")

            else:
                # ê¸°ì¡´ ë°©ì‹: ë‹¨ì¼ ë‹µë³€ ìƒì„±
                answer = self.chain.invoke({
                    "question": question,
                    "chat_history": formatted_history
                })

            # Phase 2: ë‹µë³€ ê²€ì¦ ë° ì¬ìƒì„± (ìƒìš© ì„œë¹„ìŠ¤ ìˆ˜ì¤€)
            # Self-Consistencyê°€ í™œì„±í™”ëœ ê²½ìš°, ì¼ê´€ì„±ì´ ë†’ìœ¼ë©´ ê²€ì¦ Skip ê°€ëŠ¥
            skip_verification = self.enable_self_consistency and consistency_score > 0.8

            if not skip_verification:
                docs_for_confidence = [d for d, _ in self._last_retrieved_docs[:self.top_k]]
                verification_result = self._verify_answer_quality(question, answer, docs_for_confidence)

                if not verification_result["is_valid"]:
                    print(f"[WARN] ë‹µë³€ ê²€ì¦ ì‹¤íŒ¨: {verification_result['reason']}")
                    print(f"[INFO] ë¬¸ì„œ ê¸°ë°˜ ì¬ìƒì„± ì‹œë„...")

                    # ë¬¸ì„œ ê¸°ë°˜ ì¬ìƒì„±
                    regenerated_answer = self._regenerate_answer(question, answer, docs_for_confidence, formatted_history)
                    if regenerated_answer:
                        answer = regenerated_answer
                        print(f"[OK] ë‹µë³€ ì¬ìƒì„± ì™„ë£Œ")
                    else:
                        print(f"[WARN] ì¬ìƒì„± ì‹¤íŒ¨, ì›ë³¸ ë‹µë³€ ì‚¬ìš©")
            else:
                print(f"  [OK] ë†’ì€ ì¼ê´€ì„± ({consistency_score:.2%}), ê²€ì¦ Skip")

            # Phase A-2: NotebookLM ìŠ¤íƒ€ì¼ ì¸ë¼ì¸ Citation ì¶”ê°€
            # ìºì‹œëœ ë¬¸ì„œì—ì„œ Document ê°ì²´ ì¶”ì¶œ
            source_docs = [doc for doc, _ in self._last_retrieved_docs[:self.top_k]]

            if source_docs:
                # Citation ìƒì„± ë° ë‹µë³€ì— í†µí•©
                answer = self._generate_source_citations(answer, source_docs)

            # ìºì‹œëœ ë¬¸ì„œì—ì„œ ì¶œì²˜ ì •ë³´ ìƒì„±
            sources = []
            docs_for_confidence = []

            for doc, score in self._last_retrieved_docs[:self.top_k]:
                docs_for_confidence.append(doc)
                source_info = {
                    "file_name": doc.metadata.get("file_name", "Unknown"),
                    "page_number": doc.metadata.get("page_number", "Unknown"),
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "similarity_score": float(round(score * 100, 1))  # 0-100 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
                }
                sources.append(source_info)
            
            # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            confidence = self._calculate_confidence_score(question, answer, docs_for_confidence)
            
            return {
                "answer": answer,
                "sources": sources,
                "confidence": confidence,
                "success": True
            }
        except Exception as e:
            print(f"[ERROR] query() ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return {
                "answer": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "sources": [],
                "confidence": 0.0,
                "success": False
            }
    
    def _verify_answer_quality(self, question: str, answer: str, docs: List[Document]) -> Dict[str, Any]:
        """ë‹µë³€ í’ˆì§ˆ ê²€ì¦ (Phase 2: ìƒìš© ì„œë¹„ìŠ¤ ìˆ˜ì¤€)
        
        Returns:
            {
                "is_valid": bool,
                "reason": str,
                "scores": {
                    "no_forbidden_phrases": float,
                    "has_citation": float,
                    "content_match": float,
                    "specificity": float
                }
            }
        """
        if not docs or not answer:
            return {
                "is_valid": False,
                "reason": "ë¬¸ì„œ ë˜ëŠ” ë‹µë³€ì´ ì—†ìŒ",
                "scores": {}
            }
        
        answer_lower = answer.lower()
        doc_contents = " ".join([d.page_content.lower() for d in docs])
        doc_metadata = [(d.metadata.get("file_name", ""), d.metadata.get("page_number", "")) for d in docs]
        
        scores = {}
        
        # 1. ê¸ˆì§€ êµ¬ë¬¸ ê²€ì‚¬
        forbidden_phrases = [
            "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤",
            "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "ì—†ìŠµë‹ˆë‹¤",
            "ì •ë³´ ë¶€ì¡±",
            "í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "cannot find",
            "not found",
            "no information"
        ]
        has_forbidden = any(phrase in answer_lower for phrase in forbidden_phrases)
        scores["no_forbidden_phrases"] = 0.0 if has_forbidden else 1.0
        
        # 2. ë¬¸ì„œ ì¸ìš© ê²€ì‚¬ (í˜ì´ì§€ ë²ˆí˜¸, íŒŒì¼ëª… ë“± ë©”íƒ€ë°ì´í„° ì–¸ê¸‰)
        has_citation = False
        citation_keywords = ["í˜ì´ì§€", "page", "ë¬¸ì„œ", "íŒŒì¼", "ì„¹ì…˜", "section"]
        has_citation_keywords = any(keyword in answer_lower for keyword in citation_keywords)
        
        # íŒŒì¼ëª…ì´ë‚˜ í˜ì´ì§€ ë²ˆí˜¸ ì§ì ‘ ì–¸ê¸‰ í™•ì¸
        for file_name, page_num in doc_metadata:
            if file_name and file_name.lower() in answer_lower:
                has_citation = True
                break
            if page_num and str(page_num) in answer:
                has_citation = True
                break
        
        scores["has_citation"] = 1.0 if (has_citation or has_citation_keywords) else 0.3
        
        # 3. ë¬¸ì„œ ë‚´ìš©ê³¼ì˜ ì¼ì¹˜ ê²€ì‚¬ (í‚¤ì›Œë“œ ë§¤ì¹­)
        question_keywords = set(re.findall(r'\w+', question.lower()))
        doc_keywords = set(re.findall(r'\w+', doc_contents[:1000]))  # ì²˜ìŒ 1000ìë§Œ
        
        # ë‹µë³€ì— ë¬¸ì„œ í‚¤ì›Œë“œê°€ ì–¼ë§ˆë‚˜ í¬í•¨ë˜ëŠ”ì§€
        matching_keywords = question_keywords.intersection(doc_keywords)
        answer_has_keywords = sum(1 for kw in matching_keywords if kw in answer_lower)
        content_match_score = min(1.0, answer_has_keywords / max(len(matching_keywords), 1))
        scores["content_match"] = content_match_score
        
        # 4. êµ¬ì²´ì„± ê²€ì‚¬ (ì¼ë°˜í™”ëœ ë‹µë³€ ê°ì§€)
        # ì¼ë°˜í™”ëœ êµ¬ë¬¸ íŒ¨í„´
        generic_phrases = [
            "ì¼ë°˜ì ìœ¼ë¡œ",
            "ë³´í†µ",
            "ëŒ€ë¶€ë¶„ì˜ ê²½ìš°",
            "ì¼ë°˜ì ìœ¼ë¡œ ì•Œë ¤ì§„",
            "ì¼ë°˜ì ì¸ ì›ë¦¬",
            "ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ”"
        ]
        has_generic = any(phrase in answer_lower for phrase in generic_phrases)
        
        # ë¬¸ì„œ íŠ¹ì • ë‚´ìš© (ìˆ˜ì¹˜, ì´ë¦„, ê³ ìœ ëª…ì‚¬ ë“±) í¬í•¨ ì—¬ë¶€
        has_specifics = bool(re.search(r'\d+[.%]?', answer)) or len([w for w in answer.split() if len(w) > 5]) > 3
        specificity_score = 0.5 if has_generic else 1.0
        if has_specifics:
            specificity_score = min(1.0, specificity_score + 0.3)
        scores["specificity"] = specificity_score
        
        # ì¢…í•© ê²€ì¦
        total_score = (
            scores["no_forbidden_phrases"] * 0.4 +
            scores["has_citation"] * 0.3 +
            scores["content_match"] * 0.2 +
            scores["specificity"] * 0.1
        )
        
        is_valid = total_score >= 0.6 and scores["no_forbidden_phrases"] > 0
        
        reasons = []
        if not is_valid:
            if scores["no_forbidden_phrases"] == 0:
                reasons.append("ê¸ˆì§€ êµ¬ë¬¸ ì‚¬ìš©")
            if scores["has_citation"] < 0.5:
                reasons.append("ë¬¸ì„œ ì¸ìš© ë¶€ì¡±")
            if scores["content_match"] < 0.3:
                reasons.append("ë¬¸ì„œ ë‚´ìš©ê³¼ ë¶ˆì¼ì¹˜")
            if scores["specificity"] < 0.5:
                reasons.append("ì¼ë°˜í™”ëœ ë‹µë³€")
        
        return {
            "is_valid": is_valid,
            "reason": ", ".join(reasons) if reasons else "ì •ìƒ",
            "total_score": total_score,
            "scores": scores
        }
    
    def _regenerate_answer(self, question: str, original_answer: str, docs: List[Document], 
                          chat_history: str) -> Optional[str]:
        """ê²€ì¦ ì‹¤íŒ¨ ì‹œ ë¬¸ì„œ ê¸°ë°˜ ì¬ìƒì„± (Phase 2)"""
        if not docs:
            return None
        
        try:
            # ì¬ìƒì„± ì „ìš© í”„ë¡¬í”„íŠ¸
            context = self._format_docs(docs)
            
            regeneration_prompt = f"""ì´ì „ì— ìƒì„±ëœ ë‹µë³€ì´ ë¬¸ì„œ ê¸°ë°˜ì´ ì•„ë‹ˆì—ˆìŠµë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì‹œ ë‹µë³€í•˜ì„¸ìš”.

âš ï¸ ì¤‘ìš” ê·œì¹™:
1. **ë¬¸ì„œ ìš°ì„ **: ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œì—ì„œë§Œ ì •ë³´ë¥¼ ì°¾ìœ¼ì„¸ìš”
2. **ê¸ˆì§€ êµ¬ë¬¸**: "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ì—†ìŠµë‹ˆë‹¤"ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
3. **ë¬¸ì„œ ì¸ìš©**: ë°˜ë“œì‹œ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì¸ìš©í•˜ê³  í˜ì´ì§€/íŒŒì¼ ì •ë³´ë¥¼ ëª…ì‹œí•˜ì„¸ìš”
4. **êµ¬ì²´ì„±**: ë¬¸ì„œì˜ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ì´ë¦„, ë‚´ìš©ì„ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”

ì´ì „ ëŒ€í™”:
{chat_history}

ì œê³µëœ ë¬¸ì„œ:
{context}

ì§ˆë¬¸:
{question}

ì´ì „ ë‹µë³€ (ì°¸ê³ ìš©, ê°œì„  í•„ìš”):
{original_answer}

ìœ„ ì´ì „ ë‹µë³€ì„ ê°œì„ í•˜ì—¬, ì œê³µëœ ë¬¸ì„œì— ê·¼ê±°í•œ êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.

ë‹µë³€:"""
            
            # LLM ì¬ìƒì„±
            regenerated = self.llm.invoke(regeneration_prompt)
            
            # ì‘ë‹µ íŒŒì‹±
            if hasattr(regenerated, 'content'):
                return regenerated.content.strip()
            elif hasattr(regenerated, 'text'):
                return regenerated.text.strip()
            else:
                return str(regenerated).strip()
                
        except Exception as e:
            print(f"[WARN] ì¬ìƒì„± ì˜¤ë¥˜: {e}")
            return None
    
    def _calculate_confidence_score(self, question: str, answer: str, docs: List[Document]) -> float:
        """ë‹µë³€ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° (0-100)"""
        if not docs:
            return 0.0
        
        # 1. ë¬¸ì„œ ê°œìˆ˜ ê¸°ë°˜ ì ìˆ˜ (ë” ë§ì€ ì¶œì²˜ = ë†’ì€ ì‹ ë¢°ë„)
        doc_score = min(len(docs) / 5.0, 1.0)  # 5ê°œ ì´ìƒì´ë©´ ë§Œì 
        
        # 2. ë‹µë³€ ê¸¸ì´ ì ìˆ˜ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ë©´ ê°ì )
        answer_length = len(answer)
        if answer_length < 50:
            length_score = 0.3
        elif answer_length < 100:
            length_score = 0.6
        elif answer_length < 500:
            length_score = 1.0
        elif answer_length < 1000:
            length_score = 0.9
        else:
            length_score = 0.8
        
        # 3. "ì •ë³´ ì—†ìŒ" í‚¤ì›Œë“œ ê°ì§€
        negative_keywords = ["ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤", "ì£„ì†¡í•©ë‹ˆë‹¤"]
        has_negative = any(keyword in answer for keyword in negative_keywords)
        negative_penalty = 0.5 if has_negative else 1.0
        
        # ìµœì¢… ì‹ ë¢°ë„ ì ìˆ˜ (0-100)
        confidence = (doc_score * 0.4 + length_score * 0.4 + negative_penalty * 0.2) * 100
        return round(confidence, 1)
    
    def query_stream(self, question: str, chat_history: List[Dict[str, str]] = None, search_mode: str = "integrated") -> Iterator[str]:
        overall_start = time.perf_counter()
        try:
            formatted_history = self._format_chat_history(chat_history or [])

            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ë¡œê·¸ í¬í•¨)
            context = self._get_context(question, chat_history, search_mode)

            # ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¡°í•© í›„ ë¡œê·¸ ì¶œë ¥
            prompt_text = self.prompt.format(
                chat_history=formatted_history,
                context=context,
                question=question
            )
            print("[Prompt] ---------- START ----------")
            print(prompt_text)
            print("[Prompt] ----------- END -----------")

            chain_start = time.perf_counter()
            first_chunk = True
            for chunk in self.llm.stream(prompt_text):
                # chunk íƒ€ì…ë³„ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                if hasattr(chunk, "content") and isinstance(chunk.content, str):
                    text = chunk.content
                elif hasattr(chunk, "text") and isinstance(chunk.text, str):
                    text = chunk.text
                else:
                    text = str(chunk)

                if text:
                    if first_chunk:
                        print(f"[Timing] LLM first token delay: {time.perf_counter() - chain_start:.2f}s")
                        first_chunk = False
                    yield text

            print(f"[Timing] LLM streaming total: {time.perf_counter() - chain_start:.2f}s")
            print(f"[Timing] query_stream total: {time.perf_counter() - overall_start:.2f}s")
        except Exception as e:
            print(f"[Timing] query_stream total: {time.perf_counter() - overall_start:.2f}s (error)")
            yield f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def get_source_documents(self, question: str = None) -> List[Dict[str, Any]]:
        """ìºì‹œëœ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¶œì²˜ë¡œ ë°˜í™˜ (ë‹µë³€ ìƒì„±ì— ì‹¤ì œ ì‚¬ìš©ëœ ë¬¸ì„œ)"""
        try:
            if not self._last_retrieved_docs:
                return []
            
            # ìºì‹œëœ ë¬¸ì„œì— ì ìˆ˜ ì •ê·œí™” ì ìš©
            is_reranker = self.use_reranker
            probs = self._normalize_scores(self._last_retrieved_docs, is_reranker=is_reranker)
            
            sources = []
            for (doc, raw_score), normalized_score in zip(self._last_retrieved_docs, probs):
                # 15% ì„ê³„ê°’ ì œê±° - ì‹¤ì œ ì‚¬ìš©ëœ ë¬¸ì„œëŠ” ëª¨ë‘ í‘œì‹œ
                sources.append({
                    "file_name": doc.metadata.get("file_name", "Unknown"),
                    "page_number": doc.metadata.get("page_number", "Unknown"),
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "similarity_score": float(round(normalized_score, 1)),
                    "raw_score": float(round(raw_score, 4))  # ë””ë²„ê¹…ìš©
                })
            
            return sources
        except Exception as e:
            print(f"ì¶œì²˜ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def get_last_classification(self) -> Optional[Dict[str, Any]]:
        """ë§ˆì§€ë§‰ ì§ˆë¬¸ ë¶„ë¥˜ ê²°ê³¼ ë°˜í™˜ (UI í‘œì‹œìš©)"""
        return getattr(self, '_last_classification', None)

    def clear_memory(self):
        pass
    
    def update_llm(self, llm_api_type: str, llm_base_url: str, llm_model: str, 
                   llm_api_key: str = "", temperature: float = 0.7):
        self.llm_api_type = llm_api_type
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.llm_api_key = llm_api_key
        self.temperature = temperature
        self.llm = self._create_llm()
        self.chain = (
            {
                "context": lambda x: self._get_context(x["question"]),
                "chat_history": lambda x: x.get("chat_history", "ì´ì „ ëŒ€í™” ì—†ìŒ"),
                "question": lambda x: x["question"]
            }
            | self.prompt
            | self.llm
            | StrOutputParser()
        )
    
    def update_retriever(self, vectorstore, top_k: int = 3):
        self.vectorstore = vectorstore
        self.top_k = top_k
        self.retriever = vectorstore.as_retriever(
            search_kwargs={"k": max(top_k * 5, 20)}
        )
        self.chain = (
            {
                "context": lambda x: self._get_context(x["question"]),
                "chat_history": lambda x: x.get("chat_history", "ì´ì „ ëŒ€í™” ì—†ìŒ"),
                "question": lambda x: x["question"]
            }
            | self.prompt
            | self.llm
            | StrOutputParser()
        )

    def _to_percentage(self, scores: List[float], is_reranker: bool) -> List[float]:
        """ì ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¥¼ 0~100%ë¡œ ì •ê·œí™”"""
        if not scores:
            return []
        if is_reranker:
            mn = min(scores)
            mx = max(scores)
            if abs(mx - mn) < 1e-9:
                return [50.0 for _ in scores]
            return [max(0.0, min(100.0, (s - mn) / (mx - mn) * 100.0)) for s in scores]
        # ë²¡í„° ê²€ìƒ‰: ê±°ë¦¬ê°€ 0~2 (ì‘ì„ìˆ˜ë¡ ìœ ì‚¬) ê°€ì • â†’ ìœ ì‚¬ë„ë¡œ ë³€í™˜
        return [max(0.0, min(100.0, (2.0 - s) / 2.0 * 100.0)) for s in scores]

    def _normalize_scores(self, pairs: List[tuple], is_reranker: bool) -> List[float]:
        """(doc, raw_score) -> 0~100% í™•ë¥ í˜• ì ìˆ˜ë¡œ ë³´ì • (ê°œì„  ë²„ì „)
        - reranker: Z-score ì •ê·œí™” í›„ Min-Maxë¡œ [0, 1] ë³€í™˜
        - vector:   í•˜ì´í¼ë³¼ë¦­ ë³€í™˜ + Min-Max ì •ê·œí™” í›„ softmax
        """
        import math
        import statistics
        
        if not pairs:
            return []
        
        raw = [float(s) for _, s in pairs]
        
        if is_reranker:
            # Reranker ì ìˆ˜: ì¼ë°˜ì ìœ¼ë¡œ ì–‘ìˆ˜ì´ë©° í° ê°’ì´ ì¢‹ìŒ
            # ìŒìˆ˜ ê°’ í•„í„°ë§ ë° ì •ê·œí™”
            filtered_scores = [s for s in raw if s > 0]
            
            if not filtered_scores:
                # ëª¨ë“  ì ìˆ˜ê°€ 0 ì´í•˜ì¸ ê²½ìš° ê· ë“± ë¶„ë°°
                return [50.0] * len(pairs)
            
            # Z-score ì •ê·œí™”
            mean_score = statistics.mean(filtered_scores)
            try:
                stdev_score = statistics.stdev(filtered_scores) if len(filtered_scores) > 1 else 1.0
            except:
                stdev_score = 1.0
            
            z_scores = []
            for s in raw:
                if s > 0 and stdev_score > 0:
                    z = (s - mean_score) / stdev_score
                    z_scores.append(z)
                else:
                    z_scores.append(-2.0)  # ìŒìˆ˜ ì ìˆ˜ëŠ” ë‚®ì€ Z-score
            
            # Z-scoreë¥¼ [0, 1] ë²”ìœ„ë¡œ Min-Max ì •ê·œí™”
            min_z = min(z_scores)
            max_z = max(z_scores)
            z_range = max_z - min_z if max_z > min_z else 1.0
            
            normalized = []
            for z in z_scores:
                if z_range > 0:
                    norm_val = (z - min_z) / z_range
                else:
                    norm_val = 0.5
                normalized.append(max(0.0, min(1.0, norm_val)))
            
            # Softmax ì ìš© (ë” ë¶€ë“œëŸ¬ìš´ í™•ë¥  ë¶„í¬)
            mx = max(normalized)
            exps = [math.exp(v - mx) for v in normalized]
            Z = sum(exps) or 1.0
            probs = [min(100.0, max(0.0, 100.0 * v / Z)) for v in exps]
        else:
            # Vector ê²€ìƒ‰: ê±°ë¦¬ ê¸°ë°˜ ì ìˆ˜ (ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ)
            # ìŒìˆ˜ ê°’ ì²˜ë¦¬ ë° í•˜ì´í¼ë³¼ë¦­ ë³€í™˜
            sims = []
            for s in raw:
                if s >= 0:
                    # ê±°ë¦¬ â†’ ìœ ì‚¬ë„ ë³€í™˜: similarity = 1 / (1 + distance)
                    sim = 1.0 / (1.0 + s)
                else:
                    # ìŒìˆ˜ ê±°ë¦¬ëŠ” ë¹„ì •ìƒ â†’ ë‚®ì€ ìœ ì‚¬ë„
                    sim = 0.01
                sims.append(sim)
            
            # Min-Max ì •ê·œí™”
            min_sim = min(sims)
            max_sim = max(sims)
            sim_range = max_sim - min_sim if max_sim > min_sim else 1.0
            
            normalized = []
            for sim in sims:
                if sim_range > 0:
                    norm_val = (sim - min_sim) / sim_range
                else:
                    norm_val = 0.5
                normalized.append(max(0.0, min(1.0, norm_val)))
            
            # Softmax ì ìš©
            mx = max(normalized)
            exps = [math.exp(v - mx) for v in normalized]
            Z = sum(exps) or 1.0
            probs = [min(100.0, max(0.0, 100.0 * v / Z)) for v in exps]

        return probs

    # ========================================
    # Phase A-2: Source Citation Enhancement
    # NotebookLM-style inline citations
    # ========================================

    def _split_sentences(self, text: str) -> List[str]:
        """ë‹µë³€ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬

        í•œê¸€/ì˜ë¬¸ ë¬¸ì¥ êµ¬ë¶„ ê³ ë ¤:
        - ë§ˆì¹¨í‘œ(.), ë¬¼ìŒí‘œ(?), ëŠë‚Œí‘œ(!)
        - ë‹¨, "Dr.", "Mr.", "etc." ë“±ì€ ì œì™¸

        Args:
            text: ë¶„ë¦¬í•  í…ìŠ¤íŠ¸

        Returns:
            ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
        """
        if not text:
            return []

        # 1. íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ë³´í˜¸ (Dr., Mr. ë“±)
        text = re.sub(r'(Dr|Mr|Ms|Mrs|etc)\.', r'\1<DOT>', text)

        # 2. ë¬¸ì¥ ë¶„ë¦¬ (., ?, !) - êµ¬ë¶„ìë„ í•¨ê»˜ ìº¡ì²˜
        sentences = re.split(r'([.!?])\s+', text)

        # 3. ì¬ì¡°í•© (êµ¬ë¶„ìì™€ ë¬¸ì¥ì„ ë‹¤ì‹œ í•©ì¹¨)
        result = []
        for i in range(0, len(sentences)-1, 2):
            if i+1 < len(sentences):
                sentence = sentences[i] + sentences[i+1]
            else:
                sentence = sentences[i]
            result.append(sentence)

        # ë§ˆì§€ë§‰ ë¬¸ì¥ ì¶”ê°€ (êµ¬ë¶„ì ì—†ì´ ëë‚˜ëŠ” ê²½ìš°)
        if len(sentences) % 2 == 1:
            result.append(sentences[-1])

        # 4. <DOT> ë³µì›
        result = [s.replace('<DOT>', '.') for s in result]

        # 5. ë¹ˆ ë¬¸ì¥ ì œê±° ë° trim
        return [s.strip() for s in result if s.strip()]

    def _embed_text(self, text: str) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜

        ê¸°ì¡´ vectorstoreì˜ ì„ë² ë”© ëª¨ë¸ ì¬ì‚¬ìš©

        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° (numpy array)
        """
        if not text:
            return np.zeros(1024)  # ê¸°ë³¸ ì°¨ì› (mxbai-embed-large)

        try:
            # VectorStoreManagerì˜ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
            embedding_model = self.vectorstore.embeddings

            # í…ìŠ¤íŠ¸ ì„ë² ë”©
            embedding = embedding_model.embed_query(text)
            return np.array(embedding)
        except Exception as e:
            print(f"    [WARN] ì„ë² ë”© ì‹¤íŒ¨: {e}")
            return np.zeros(1024)  # ê¸°ë³¸ ì°¨ì›

    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°

        Args:
            vec1: ì²« ë²ˆì§¸ ë²¡í„°
            vec2: ë‘ ë²ˆì§¸ ë²¡í„°

        Returns:
            ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (0.0 ~ 1.0)
        """
        # ì˜ë²¡í„° ì²´í¬
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)

        if norm1 == 0 or norm2 == 0:
            return 0.0

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = np.dot(vec1, vec2) / (norm1 * norm2)

        # 0~1 ë²”ìœ„ë¡œ clipping
        return float(max(0.0, min(1.0, similarity)))

    def _format_citation(self, source: Document) -> str:
        """ì¶œì²˜ë¥¼ NotebookLM ìŠ¤íƒ€ì¼ë¡œ í¬ë§·

        í˜•ì‹: [íŒŒì¼ëª…, p.í˜ì´ì§€, ì‹ ë¢°ë„: ì ìˆ˜]

        Args:
            source: ì¶œì²˜ ë¬¸ì„œ

        Returns:
            í¬ë§·ëœ ì¶œì²˜ ë¬¸ìì—´
        """
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        file_name = source.metadata.get('file_name', 'Unknown')
        page = source.metadata.get('page_number', '?')

        # Document tupleì—ì„œ score ì¶”ì¶œ ì‹œë„
        score = source.metadata.get('score', 0.0)

        # ì§§ì€ íŒŒì¼ëª… ì¶”ì¶œ (í™•ì¥ì ì œê±°)
        short_name = file_name.rsplit('.', 1)[0]

        # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸° (30ì ì œí•œ)
        if len(short_name) > 30:
            short_name = short_name[:27] + "..."

        # ì¶œì²˜ í¬ë§·
        citation = f"[{short_name}, p.{page}]"

        return citation

    def _find_best_source_for_sentence(self, sentence: str, sources: List[Document]) -> Optional[Document]:
        """ë¬¸ì¥ê³¼ ê°€ì¥ ê´€ë ¨ëœ ì¶œì²˜ ì°¾ê¸°

        ë°©ë²•:
        1. ë¬¸ì¥ê³¼ ê° ì¶œì²˜ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        2. ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ ì¶œì²˜ ì„ íƒ
        3. ìœ ì‚¬ë„ê°€ ì„ê³„ê°’(0.4) ì´í•˜ë©´ None ë°˜í™˜

        Args:
            sentence: ë¶„ì„í•  ë¬¸ì¥
            sources: í›„ë³´ ì¶œì²˜ ë¬¸ì„œë“¤

        Returns:
            ê°€ì¥ ê´€ë ¨ëœ ì¶œì²˜ (ë˜ëŠ” None)
        """
        if not sources or not sentence:
            return None

        # 1. ë¬¸ì¥ ì„ë² ë”©
        sentence_embedding = self._embed_text(sentence)

        # 2. ê° ì¶œì²˜ì™€ ìœ ì‚¬ë„ ê³„ì‚°
        best_source = None
        best_similarity = 0.0

        for source in sources:
            # ì¶œì²˜ í…ìŠ¤íŠ¸ ì„ë² ë”© (ì²˜ìŒ 500ìë§Œ - ì„±ëŠ¥ ìµœì í™”)
            source_text = source.page_content[:500] if len(source.page_content) > 500 else source.page_content
            source_embedding = self._embed_text(source_text)

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            similarity = self._cosine_similarity(sentence_embedding, source_embedding)

            # ì„ê³„ê°’ ì²´í¬ (0.4)
            if similarity > best_similarity and similarity > 0.4:
                best_similarity = similarity
                best_source = source

        return best_source

    def _find_multiple_sources_for_sentence(self, sentence: str, sources: List[Document]) -> List[Document]:
        """ë¬¸ì¥ê³¼ ê´€ë ¨ëœ ì—¬ëŸ¬ ì¶œì²˜ ì°¾ê¸° (Phase C: Citation 95%)

        ë°©ë²•:
        1. ëª¨ë“  ì¶œì²˜ì™€ ìœ ì‚¬ë„ ê³„ì‚°
        2. ë™ì  ì„ê³„ê°’ ì´ìƒì¸ ì¶œì²˜ ëª¨ë‘ ì„ íƒ
        3. ìµœëŒ€ 2ê°œê¹Œì§€ ë°˜í™˜ (ê³¼ë„í•œ Citation ë°©ì§€)

        Args:
            sentence: ë¶„ì„í•  ë¬¸ì¥
            sources: í›„ë³´ ì¶œì²˜ ë¬¸ì„œë“¤

        Returns:
            ê´€ë ¨ëœ ì¶œì²˜ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 2ê°œ)
        """
        if not sources or not sentence:
            return []

        # 1. ë¬¸ì¥ ì„ë² ë”©
        sentence_embedding = self._embed_text(sentence)

        # 2. ëª¨ë“  ì¶œì²˜ì™€ ìœ ì‚¬ë„ ê³„ì‚°
        relevant_sources = []

        for source in sources:
            # ì¶œì²˜ í…ìŠ¤íŠ¸ ì„ë² ë”©
            source_text = source.page_content[:500] if len(source.page_content) > 500 else source.page_content
            source_embedding = self._embed_text(source_text)

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            similarity = self._cosine_similarity(sentence_embedding, source_embedding)

            # ë™ì  ì„ê³„ê°’ ì´ìƒì´ë©´ ëª¨ë‘ ì¶”ê°€
            threshold = self._get_adaptive_threshold(sentence, sources)
            if similarity > threshold:
                relevant_sources.append((source, similarity))

        # 3. ìœ ì‚¬ë„ ìˆœ ì •ë ¬
        relevant_sources.sort(key=lambda x: x[1], reverse=True)

        # 4. ìµœëŒ€ 2ê°œê¹Œì§€ ë°˜í™˜
        return [src for src, _ in relevant_sources[:2]]

    def _get_adaptive_threshold(self, sentence: str, sources: List[Document]) -> float:
        """ë™ì  ì„ê³„ê°’ ê³„ì‚° (Phase C)

        ë¬¸ì¥ ê¸¸ì´ì™€ ë³µì¡ë„ì— ë”°ë¼ ì„ê³„ê°’ ì¡°ì •:
        - ì§§ì€ ë¬¸ì¥ (10-20ì): 0.5 (ë†’ì€ í™•ì‹  í•„ìš”)
        - ì¤‘ê°„ ë¬¸ì¥ (20-40ì): 0.4 (ê¸°ë³¸)
        - ê¸´ ë¬¸ì¥ (40+ì): 0.35 (ë” ê´€ëŒ€í•˜ê²Œ)

        Args:
            sentence: ë¶„ì„í•  ë¬¸ì¥
            sources: í›„ë³´ ì¶œì²˜ ë¬¸ì„œë“¤

        Returns:
            ë™ì  ì„ê³„ê°’
        """
        sentence_length = len(sentence)

        if sentence_length < 20:
            return 0.5  # ì§§ì€ ë¬¸ì¥ì€ ë†’ì€ í™•ì‹  í•„ìš”
        elif sentence_length < 40:
            return 0.4  # ê¸°ë³¸ ì„ê³„ê°’
        else:
            return 0.35  # ê¸´ ë¬¸ì¥ì€ ë” ê´€ëŒ€í•˜ê²Œ

    def _generate_source_citations(self, answer: str, sources: List[Document]) -> str:
        """NotebookLM ìŠ¤íƒ€ì¼ ì¶œì²˜ ì¸ë¼ì¸ í‘œì‹œ (Phase C: 95% ëª©í‘œ)

        Args:
            answer: ìƒì„±ëœ ë‹µë³€
            sources: ì‚¬ìš©ëœ ì¶œì²˜ ë¬¸ì„œë“¤

        Returns:
            ì¶œì²˜ê°€ ì¸ë¼ì¸ìœ¼ë¡œ í‘œì‹œëœ ë‹µë³€
        """
        if not sources or not answer:
            return answer

        print(f"  [CITE] Citation ìƒì„± ì¤‘... (ë¬¸ì„œ {len(sources)}ê°œ)")

        # 1. ë‹µë³€ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        sentences = self._split_sentences(answer)
        print(f"    [OK] ë¬¸ì¥ ë¶„ë¦¬: {len(sentences)}ê°œ")

        # 2. ê° ë¬¸ì¥ì— ì¶œì²˜ ë§¤ì¹­
        cited_sentences = []
        citation_count = 0

        for i, sentence in enumerate(sentences):
            # Phase C: ì§§ì€ ë¬¸ì¥ ì„ê³„ê°’ ë‚®ì¶¤ (15 â†’ 10)
            if len(sentence) < 10:
                cited_sentences.append(sentence)
                continue

            # Phase C: ì—¬ëŸ¬ ì¶œì²˜ í—ˆìš© (ìµœëŒ€ 2ê°œ)
            relevant_sources = self._find_multiple_sources_for_sentence(sentence, sources)

            if relevant_sources:
                # ì—¬ëŸ¬ ì¶œì²˜ë¥¼ ì¸ë¼ì¸ìœ¼ë¡œ ê²°í•©
                citations = [self._format_citation(src) for src in relevant_sources]
                cited_sentence = f"{sentence.strip()} {''.join(citations)}"
                citation_count += 1
            else:
                cited_sentence = sentence.strip()

            cited_sentences.append(cited_sentence)

        print(f"    [OK] Citation ì¶”ê°€: {citation_count}/{len(sentences)}ê°œ ë¬¸ì¥")

        return " ".join(cited_sentences)

    # ============================================
    # Phase A-3: Self-Consistency Check
    # ============================================

    def _generate_answer_internal(self, question: str, context: str, chat_history: str = "") -> str:
        """ë‚´ë¶€ ë‹µë³€ ìƒì„± ë©”ì„œë“œ (Self-Consistencyìš©)

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            context: ê²€ìƒ‰ëœ ë¬¸ë§¥
            chat_history: ëŒ€í™” ì´ë ¥ (formatted)

        Returns:
            ìƒì„±ëœ ë‹µë³€ ë¬¸ìì—´
        """
        try:
            # LangChain invoke ì‚¬ìš©
            answer = self.chain.invoke({
                "question": question,
                "context": context,
                "chat_history": chat_history if chat_history else "ì´ì „ ëŒ€í™” ì—†ìŒ"
            })

            # ì‘ë‹µ íƒ€ì…ì— ë”°ë¼ ë¬¸ìì—´ ì¶”ì¶œ
            if hasattr(answer, 'content'):
                return answer.content
            elif hasattr(answer, 'text'):
                return answer.text
            else:
                return str(answer)

        except Exception as e:
            print(f"    [ERROR] ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""

    def _calculate_answer_consistency(self, answers: List[str]) -> float:
        """ë‹µë³€ë“¤ ê°„ì˜ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚° (Jaccard ìœ ì‚¬ë„)

        Args:
            answers: ìƒì„±ëœ ë‹µë³€ë“¤

        Returns:
            ì¼ê´€ì„± ì ìˆ˜ (0.0 ~ 1.0)
        """
        from itertools import combinations

        if len(answers) < 2:
            return 1.0

        # ëª¨ë“  ìŒì˜ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []

        for ans1, ans2 in combinations(answers, 2):
            # í† í°í™” (ë‹¨ìˆœ ê³µë°± ê¸°ì¤€)
            tokens1 = set(ans1.lower().split())
            tokens2 = set(ans2.lower().split())

            # Jaccard ìœ ì‚¬ë„: |êµì§‘í•©| / |í•©ì§‘í•©|
            if len(tokens1.union(tokens2)) == 0:
                similarity = 0.0
            else:
                similarity = len(tokens1.intersection(tokens2)) / len(tokens1.union(tokens2))

            similarities.append(similarity)

        # í‰ê·  ìœ ì‚¬ë„ ë°˜í™˜
        return sum(similarities) / len(similarities) if similarities else 0.0

    def _extract_common_info(self, answers: List[str]) -> str:
        """ì—¬ëŸ¬ ë‹µë³€ì—ì„œ ê³µí†µ ì •ë³´ ì¶”ì¶œ

        Args:
            answers: ìƒì„±ëœ ë‹µë³€ë“¤

        Returns:
            ê³µí†µ ì •ë³´ë¥¼ í†µí•©í•œ ë‹µë³€
        """
        if not answers:
            return ""

        if len(answers) == 1:
            return answers[0]

        # ê°„ë‹¨í•œ ì „ëµ: ê°€ì¥ ê¸´ ë‹µë³€ ì„ íƒ (ì •ë³´ê°€ ë§ìŒ)
        # í–¥í›„ ê°œì„ : ì‹¤ì œ ê³µí†µ ë¬¸ì¥ ì¶”ì¶œ ë¡œì§ êµ¬í˜„ ê°€ëŠ¥
        longest_answer = max(answers, key=lambda a: len(a))

        return longest_answer

    def _generate_with_self_consistency(
        self,
        question: str,
        context: str,
        chat_history: str = "",
        n: int = 3,
        enable: bool = True
    ) -> Dict[str, Any]:
        """Self-Consistency Check: ì—¬ëŸ¬ ë²ˆ ìƒì„± í›„ ì¼ê´€ì„± ê²€ì¦

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            context: ê²€ìƒ‰ëœ ë¬¸ë§¥
            chat_history: ëŒ€í™” ì´ë ¥
            n: ìƒì„± íšŸìˆ˜ (ê¸°ë³¸ 3íšŒ)
            enable: Self-Consistency í™œì„±í™” ì—¬ë¶€

        Returns:
            {
                'answer': ìµœì¢… ë‹µë³€,
                'consistency': ì¼ê´€ì„± ì ìˆ˜ (0-1),
                'variants': ìƒì„±ëœ ë‹µë³€ë“¤,
                'method': 'self_consistency' or 'single'
            }
        """
        # Self-Consistency ë¹„í™œì„±í™” ì‹œ ë‹¨ì¼ ìƒì„±
        if not enable:
            answer = self._generate_answer_internal(question, context, chat_history)
            return {
                'answer': answer,
                'consistency': 1.0,
                'variants': [answer],
                'method': 'single'
            }

        print(f"  [REWRITE] Self-consistency check: {n}íšŒ ìƒì„± ì¤‘...")

        # 1. Në²ˆ ë…ë¦½ì ìœ¼ë¡œ ë‹µë³€ ìƒì„±
        original_temp = self.temperature
        self.temperature = 0.5  # ì•½ê°„ ë‹¤ì–‘ì„± ì¶”ê°€

        answers = []
        for i in range(n):
            answer = self._generate_answer_internal(question, context, chat_history)
            if answer:  # ë¹ˆ ë‹µë³€ ì œì™¸
                answers.append(answer)
                print(f"    [OK] {i+1}ë²ˆì§¸ ìƒì„± ì™„ë£Œ ({len(answer)} chars)")

        self.temperature = original_temp

        # ìƒì„± ì‹¤íŒ¨ ì‹œ
        if not answers:
            print(f"    [ERROR] ëª¨ë“  ìƒì„± ì‹¤íŒ¨")
            return {
                'answer': "ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                'consistency': 0.0,
                'variants': [],
                'method': 'self_consistency_failed'
            }

        # 2. ë‹µë³€ ê°„ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
        consistency_score = self._calculate_answer_consistency(answers)
        print(f"    [OK] ì¼ê´€ì„± ì ìˆ˜: {consistency_score:.2%}")

        # 3. ì¼ê´€ì„±ì— ë”°ë¼ ì²˜ë¦¬
        if consistency_score > 0.8:
            # ë†’ì€ ì¼ê´€ì„±: ê°€ì¥ ìƒì„¸í•œ ë‹µë³€ ì„ íƒ
            best_answer = max(answers, key=lambda a: len(a))
            print(f"    [OK] ë†’ì€ ì¼ê´€ì„±: ìµœìƒ ë‹µë³€ ì„ íƒ")

        elif consistency_score > 0.5:
            # ì¤‘ê°„ ì¼ê´€ì„±: ê³µí†µ ì •ë³´ ì¶”ì¶œ
            best_answer = self._extract_common_info(answers)
            # ì‹ ë¢°ë„ í‘œì‹œëŠ” ì„ íƒì ìœ¼ë¡œ ì¶”ê°€ (ì‚¬ìš©ì í˜¼ë€ ë°©ì§€)
            # best_answer = f"[WARN] ì¤‘ê°„ ì‹ ë¢°ë„ (ì¼ê´€ì„±: {consistency_score:.1%})\n\n{best_answer}"
            print(f"    [WARN] ì¤‘ê°„ ì¼ê´€ì„±: ê³µí†µ ì •ë³´ ì¶”ì¶œ")

        else:
            # ë‚®ì€ ì¼ê´€ì„±: ê²½ê³ ì™€ í•¨ê»˜ ì²« ë²ˆì§¸ ë‹µë³€
            best_answer = answers[0]
            # ì‹ ë¢°ë„ í‘œì‹œëŠ” ì„ íƒì ìœ¼ë¡œ ì¶”ê°€
            # best_answer = f"[WARN] ë‚®ì€ ì‹ ë¢°ë„ (ì¼ê´€ì„±: {consistency_score:.1%})\nì œê³µëœ ë¬¸ì„œì—ì„œ ëª…í™•í•œ ë‹µë³€ì„ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤.\n\n{answers[0]}"
            print(f"    [WARN] ë‚®ì€ ì¼ê´€ì„±: ê²½ê³  í‘œì‹œ")

        return {
            'answer': best_answer,
            'consistency': consistency_score,
            'variants': answers,
            'method': 'self_consistency'
        }
