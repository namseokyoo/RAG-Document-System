from typing import List, Dict, Any, Optional, Iterator
from langchain_ollama import OllamaLLM
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from utils.reranker import get_reranker
from utils.request_llm import RequestLLM
from utils.small_to_large_search import SmallToLargeSearch
import json
import re
import time


class RAGChain:
    def __init__(self, vectorstore, 
                 llm_api_type: str = "ollama",
                 llm_base_url: str = "http://localhost:11434", 
                 llm_model: str = "llama3",
                 llm_api_key: str = "",
                 temperature: float = 0.3,
                 top_k: int = 3,
                 use_reranker: bool = True,
                 reranker_model: str = "multilingual-mini",
                 reranker_initial_k: int = 20,
                 enable_synonym_expansion: bool = True,
                 enable_multi_query: bool = True,
                 multi_query_num: int = 3):
        self.llm_api_type = llm_api_type
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.llm_api_key = llm_api_key
        self.temperature = temperature
        self.top_k = top_k
        self.vectorstore = vectorstore
        
        # Re-ranker ì„¤ì • (ê¸°ë³¸ í™œì„±í™”)
        self.use_reranker = use_reranker
        self.reranker_model = reranker_model
        self.reranker_initial_k = max(reranker_initial_k, top_k * 5)
        
        # Re-ranker ì´ˆê¸°í™” (ì‚¬ìš© ì‹œ)
        if self.use_reranker:
            self.reranker = get_reranker(model_name=reranker_model)
        
        # ë§ˆì§€ë§‰ ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ (ì¶œì²˜ í‘œì‹œìš©)
        self._last_retrieved_docs = []
        
        # LLM ì´ˆê¸°í™” - API íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
        self.llm = self._create_llm()
        
        # ë™ì˜ì–´ í™•ì¥ ì„¤ì •
        self.enable_synonym_expansion = enable_synonym_expansion
        self.multi_query_num = max(0, multi_query_num)
        self.enable_multi_query = enable_multi_query and self.multi_query_num > 0
        
        # Small-to-Large ê²€ìƒ‰ ì´ˆê¸°í™”
        self.small_to_large_search = SmallToLargeSearch(vectorstore)
        
        # ë„ë©”ì¸ ìš©ì–´ ì‚¬ì „ (ì—”í‹°í‹° ê°ì§€ìš©)
        self._domain_lexicon = {
            "TADF", "ACRSA", "DABNA1", "HF", "OLED", "EQE", 
            "FRET", "PLQY", "DMAC-TRZ", "AZB-TRZ", "Î½-DABNA"
        }
        
        # Retriever ì„¤ì • - vectorstoreëŠ” ì´ë¯¸ Chroma ì¸ìŠ¤í„´ìŠ¤
        self.retriever = vectorstore.as_retriever(
            search_kwargs={"k": max(self.top_k * 8, 24)}
        )
        
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ìƒìš© ì„œë¹„ìŠ¤ ìˆ˜ì¤€ ê°œì„ )
        self.base_prompt_template = """ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.

âš ï¸ ì¤‘ìš” ê·œì¹™:
1. **ë¬¸ì„œ ìš°ì„  ì›ì¹™**: ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ì°¾ì•„ ë‹µë³€í•˜ì„¸ìš”.
2. **ì¼ë°˜ ì§€ì‹ ê¸ˆì§€**: ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ì¸¡í•˜ê±°ë‚˜ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.
3. **ì •ë³´ ì—†ìŒ ê¸ˆì§€**: ë¬¸ì„œê°€ ì œê³µëœ ê²½ìš° "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
4. **ë¬¸ì„œ ì¸ìš© ì˜ë¬´**: ë‹µë³€í•  ë•Œ ë°˜ë“œì‹œ ë¬¸ì„œì˜ êµ¬ì²´ì  ë‚´ìš©ì„ ì¸ìš©í•˜ì„¸ìš”.

ì´ì „ ëŒ€í™” ë‚´ìš©:
{chat_history}

ì°¸ê³  ë¬¸ì„œ:
{context}

í˜„ì¬ ì§ˆë¬¸: {question}

ë‹µë³€ ì ˆì°¨:
1ë‹¨ê³„: ì œê³µëœ ë¬¸ì„œë¥¼ ëª¨ë‘ ê¼¼ê¼¼íˆ ì½ìœ¼ì„¸ìš”.
2ë‹¨ê³„: ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ë¬¸ì„œì—ì„œ ëª¨ë‘ ì°¾ìœ¼ì„¸ìš” (ë™ì˜ì–´, ì•½ì–´ë„ ê³ ë ¤).
3ë‹¨ê³„: ì°¾ì€ ì •ë³´ë¥¼ ì •ë¦¬í•˜ì—¬ êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
4ë‹¨ê³„: ë‹µë³€ì— ë°˜ë“œì‹œ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì¸ìš©í•˜ì„¸ìš” (ì˜ˆ: "ë¬¸ì„œì— ë”°ë¥´ë©´...", "ì œê³µëœ ë¬¸ì„œì˜ X í˜ì´ì§€ì—...").

ë‹µë³€ í˜•ì‹:
- ë¬¸ì„œì—ì„œ ì°¾ì€ ì •ë³´ë¥¼ ë¨¼ì € ì œì‹œí•˜ì„¸ìš”
- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ì´ë¦„, ë‚ ì§œ ë“±ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ì„¸ìš”
- ë¬¸ì„œì˜ ì–´ë–¤ ë¶€ë¶„(í˜ì´ì§€/ì„¹ì…˜)ì—ì„œ ì •ë³´ë¥¼ ì–»ì—ˆëŠ”ì§€ ëª…ì‹œí•˜ì„¸ìš”
- ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”

ë‹µë³€:"""
        
        # ì§ˆë¬¸ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.prompt_templates = {
            "specific_info": """ë‹¹ì‹ ì€ ë¬¸ì„œì—ì„œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì—ì„œ ì •í™•í•œ ì‚¬ì‹¤, ìˆ˜ì¹˜, ì´ë¦„, êµ¬ì¡° ë“±ì„ ì°¾ì•„ ë‹µë³€í•´ì£¼ì„¸ìš”.

âš ï¸ í•µì‹¬ ê·œì¹™:
- ì œê³µëœ ë¬¸ì„œì—ì„œë§Œ ì •ë³´ë¥¼ ì°¾ìœ¼ì„¸ìš” (ì¼ë°˜ ì§€ì‹ ì‚¬ìš© ê¸ˆì§€)
- "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- ë¬¸ì„œì— ê´€ë ¨ ì •ë³´ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì°¾ì•„ ì œì‹œí•˜ì„¸ìš”

ì´ì „ ëŒ€í™” ë‚´ìš©:
{chat_history}

ì°¸ê³  ë¬¸ì„œ:
{context}

í˜„ì¬ ì§ˆë¬¸: {question}

Few-Shot ì˜ˆì‹œ:

[ì¢‹ì€ ë‹µë³€ ì˜ˆì‹œ]
ì§ˆë¬¸: "ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ TADF ì¬ë£ŒëŠ” ë¬´ì—‡ì¸ê°€?"
ë‹µë³€: "ì œê³µëœ ë¬¸ì„œì— ë”°ë¥´ë©´, ë…¼ë¬¸ì—ì„œ ACRSA (spiro-linked TADF molecule)ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ë¬¸ì„œì˜ ì¼ë¶€ì—ì„œ 'ACRSA-based device'ë¼ê³  ëª…ì‹œë˜ì–´ ìˆìœ¼ë©°, ë¹„êµ ì‹¤í—˜ì„ ìœ„í•´ DABNA1ë„ ì–¸ê¸‰ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì •í™•í•œ í™”í•©ë¬¼ëª…ì€ ë¬¸ì„œì—ì„œ í™•ì¸ëœ ê·¸ëŒ€ë¡œ ì¸ìš©í•©ë‹ˆë‹¤."

[ë‚˜ìœ ë‹µë³€ ì˜ˆì‹œ]
ì§ˆë¬¸: "ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ TADF ì¬ë£ŒëŠ” ë¬´ì—‡ì¸ê°€?"
ë‹µë³€: "ì£„ì†¡í•˜ì§€ë§Œ ì œê³µëœ ë¬¸ì„œì—ì„œëŠ” TADF ì¬ë£Œì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì–¸ê¸‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." (âŒ ì´ë ‡ê²Œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”)

ë‹µë³€ ì ˆì°¨:
1. ì œê³µëœ ë¬¸ì„œì˜ ëª¨ë“  ë‚´ìš©ì„ ê¼¼ê¼¼íˆ ì½ìœ¼ì„¸ìš”.
2. ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì‹ë³„í•˜ê³ , ë™ì˜ì–´ë‚˜ ì•½ì–´ë„ ê³ ë ¤í•˜ì—¬ ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•˜ì„¸ìš”.
3. ê´€ë ¨ëœ ëª¨ë“  ì •ë³´ë¥¼ ì°¾ì•„ ë‚˜ì—´í•˜ì„¸ìš” (ì—¬ëŸ¬ ê³³ì— ìˆìœ¼ë©´ ëª¨ë‘ í¬í•¨).
4. ê° ì •ë³´ë§ˆë‹¤ ì›ë¬¸ì„ ì¸ìš©í•˜ê³ , í˜ì´ì§€/ì„¹ì…˜ ì •ë³´ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
5. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ì´ë¦„, ë‚ ì§œëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”.
6. ë¶€ë¶„ì ìœ¼ë¡œë§Œ ì°¾ì€ ê²½ìš°, ì°¾ì€ ë¶€ë¶„ì„ ëª…ì‹œí•˜ê³  "ì¶”ê°€ ì •ë³´ëŠ” ë¬¸ì„œì˜ ë‹¤ë¥¸ ë¶€ë¶„ì— ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤"ë¼ê³  ì–¸ê¸‰í•˜ì„¸ìš”.

ë‹µë³€:""",
            
            "summary": """ë‹¹ì‹ ì€ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ì²´ê³„ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

âš ï¸ í•µì‹¬ ê·œì¹™:
- ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš” (ì¼ë°˜ ì§€ì‹ ì¶”ê°€ ê¸ˆì§€)
- ë¬¸ì„œì˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ì¸ìš©í•˜ì—¬ ìš”ì•½í•˜ì„¸ìš”
- "ë¬¸ì„œì— ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”

ì´ì „ ëŒ€í™” ë‚´ìš©:
{chat_history}

ì°¸ê³  ë¬¸ì„œ:
{context}

í˜„ì¬ ì§ˆë¬¸: {question}

ë‹µë³€ ì ˆì°¨:
1. ì œê³µëœ ë¬¸ì„œì˜ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ì„¸ìš” (ì œëª©, ì„¹ì…˜, í•˜ìœ„ ì„¹ì…˜ ë“±).
2. ê° ì„¹ì…˜ì˜ í•µì‹¬ ë‚´ìš©ì„ ë¬¸ì„œì—ì„œ ì§ì ‘ ì¶”ì¶œí•˜ì„¸ìš”.
3. ì£¼ìš” ë°œê²¬, ê²°ë¡ , ìˆ˜ì¹˜ ë“±ì€ ë¬¸ì„œì˜ ì›ë¬¸ì„ ì¸ìš©í•˜ì„¸ìš”.
4. ë…¼ë¦¬ì  ìˆœì„œë¡œ êµ¬ì„±í•˜ë˜, ëª¨ë“  ë‚´ìš©ì€ ë¬¸ì„œì— ê·¼ê±°í•´ì•¼ í•©ë‹ˆë‹¤.
5. ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.

ë‹µë³€:""",
            
            "comparison": """ë‹¹ì‹ ì€ ë¬¸ì„œë¥¼ ë¹„êµ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì—ì„œ ë¹„êµ ëŒ€ìƒë“¤ì„ ì°¾ì•„ ì°¨ì´ì ê³¼ íŠ¹ì§•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.

âš ï¸ í•µì‹¬ ê·œì¹™:
- ì œê³µëœ ë¬¸ì„œì—ì„œë§Œ ë¹„êµ ì •ë³´ë¥¼ ì°¾ìœ¼ì„¸ìš”
- ë¬¸ì„œì— ëª…ì‹œëœ ë¹„êµ ë‚´ìš©ì„ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”
- ì¼ë°˜ì ì¸ ë¹„êµ ì§€ì‹ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”

ì´ì „ ëŒ€í™” ë‚´ìš©:
{chat_history}

ì°¸ê³  ë¬¸ì„œ:
{context}

í˜„ì¬ ì§ˆë¬¸: {question}

ë‹µë³€ ì ˆì°¨:
1. ë¬¸ì„œì—ì„œ ë¹„êµ ëŒ€ìƒë“¤ì„ ì‹ë³„í•˜ì„¸ìš” (ëª…ì‹œì ìœ¼ë¡œ ë¹„êµëœ í•­ëª©ë“¤).
2. ê° ëŒ€ìƒì˜ íŠ¹ì§•ì„ ë¬¸ì„œì—ì„œ ì§ì ‘ ì¶”ì¶œí•˜ì—¬ ë‚˜ì—´í•˜ì„¸ìš”.
3. ë¬¸ì„œì—ì„œ ì–¸ê¸‰ëœ ì°¨ì´ì ê³¼ ê³µí†µì ì„ ì •í™•íˆ ì¸ìš©í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”.
4. ìˆ˜ì¹˜ì  ë¹„êµê°€ ë¬¸ì„œì— ìˆìœ¼ë©´ í•´ë‹¹ ìˆ˜ì¹˜ë¥¼ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ì„¸ìš”.
5. ì§ì ‘ì ì¸ ë¹„êµ ì •ë³´ê°€ ì—†ìœ¼ë©´, ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ ì œì‹œí•˜ì„¸ìš”.

ë‹µë³€:""",
            
            "relationship": """ë‹¹ì‹ ì€ ë¬¸ì„œì—ì„œ ê´€ê³„ì™€ ì¸ê³¼ê´€ê³„ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì—ì„œ ìš”ì†Œë“¤ ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬ ì„¤ëª…í•´ì£¼ì„¸ìš”.

âš ï¸ í•µì‹¬ ê·œì¹™:
- ì œê³µëœ ë¬¸ì„œì—ì„œë§Œ ê´€ê³„ ì •ë³´ë¥¼ ì°¾ìœ¼ì„¸ìš”
- ë¬¸ì„œì— ëª…ì‹œëœ ê´€ê³„ë¥¼ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”
- ì¼ë°˜ì ì¸ ì›ë¦¬ë‚˜ ì¶”ë¡ ì€ ë¬¸ì„œ ê·¼ê±° ì—†ì´ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”

ì´ì „ ëŒ€í™” ë‚´ìš©:
{chat_history}

ì°¸ê³  ë¬¸ì„œ:
{context}

í˜„ì¬ ì§ˆë¬¸: {question}

ë‹µë³€ ì ˆì°¨:
1. ë¬¸ì„œì—ì„œ ê´€ë ¨ëœ ìš”ì†Œë“¤ì„ ì‹ë³„í•˜ì„¸ìš”.
2. ë¬¸ì„œì—ì„œ ëª…ì‹œëœ ìš”ì†Œë“¤ ê°„ì˜ ê´€ê³„, ì˜í–¥, ë©”ì»¤ë‹ˆì¦˜ì„ ì°¾ì•„ ì¸ìš©í•˜ì„¸ìš”.
3. ë¬¸ì„œì—ì„œ ì–¸ê¸‰ëœ ì¸ê³¼ê´€ê³„ë‚˜ ìƒê´€ê´€ê³„ë¥¼ ì •í™•íˆ ì„¤ëª…í•˜ì„¸ìš”.
4. ë¬¸ì„œì—ì„œ ë°œê²¬ëœ ê²½í–¥ì„±ì´ë‚˜ íŒ¨í„´ì„ ì›ë¬¸ì„ ì¸ìš©í•˜ë©° ì„¤ëª…í•˜ì„¸ìš”.
5. ì§ì ‘ì ì¸ ê´€ê³„ ì •ë³´ê°€ ì—†ìœ¼ë©´, ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ ì œì‹œí•˜ë˜, ì¼ë°˜ì ì¸ ì¶”ë¡ ì€ ìµœì†Œí™”í•˜ì„¸ìš”.

ë‹µë³€:""",
            
            "general": self.base_prompt_template
        }
        
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ (ë‚˜ì¤‘ì— ì§ˆë¬¸ íƒ€ì…ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì„ íƒ)
        self.prompt_template = self.base_prompt_template
        
        self.prompt = PromptTemplate(
            template=self.prompt_template,
            input_variables=["chat_history", "context", "question"]
        )
        
        # LCEL ë°©ì‹ìœ¼ë¡œ ì²´ì¸ êµ¬ì„± (ëŒ€í™” ì´ë ¥ í¬í•¨)
        self.chain = (
            {
                "context": lambda x: self._get_context(x["question"]),
                "chat_history": lambda x: x.get("chat_history", "ì´ì „ ëŒ€í™” ì—†ìŒ"),
                "question": lambda x: x["question"]
            }
            | self.prompt
            | self.llm
            | StrOutputParser()
        )

    def _create_llm(self):
        """API íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
        if self.llm_api_type == "request":
            return RequestLLM(
                base_url=self.llm_base_url,
                model=self.llm_model,
                temperature=self.temperature,
                timeout=60
            )
        elif self.llm_api_type == "ollama":
            return OllamaLLM(
                base_url=self.llm_base_url,
                model=self.llm_model,
                temperature=self.temperature
            )
        elif self.llm_api_type == "openai":
            kwargs = {
                "model": self.llm_model,
                "temperature": self.temperature,
                "api_key": self.llm_api_key if self.llm_api_key else "not-needed"
            }
            return ChatOpenAI(**kwargs)
        elif self.llm_api_type == "openai-compatible":
            kwargs = {
                "model": self.llm_model,
                "temperature": self.temperature,
                "base_url": self.llm_base_url,
                "api_key": self.llm_api_key if self.llm_api_key else "not-needed"
            }
            return ChatOpenAI(**kwargs)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” API íƒ€ì…: {self.llm_api_type}")

    def _format_docs(self, docs: List[Document]) -> str:
        """ë¬¸ì„œë¥¼ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ… (ìƒìš© ì„œë¹„ìŠ¤ ìˆ˜ì¤€ ê°œì„ )"""
        formatted_sections = []
        
        for i, doc in enumerate(docs, 1):
            metadata = doc.metadata or {}
            file_name = metadata.get('file_name', 'Unknown')
            page_number = metadata.get('page_number', 'Unknown')
            chunk_type = metadata.get('chunk_type', 'unknown')
            section_title = metadata.get('section_title', '')
            
            # ë¬¸ì„œ ë²ˆí˜¸ì™€ ë©”íƒ€ë°ì´í„°
            header = f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            header += f"ğŸ“„ ë¬¸ì„œ #{i}\n"
            header += f"   íŒŒì¼ëª…: {file_name}\n"
            header += f"   í˜ì´ì§€: {page_number}\n"
            if chunk_type != 'unknown':
                header += f"   ì²­í¬ íƒ€ì…: {chunk_type}\n"
            if section_title:
                header += f"   ì„¹ì…˜: {section_title}\n"
            header += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
            
            # ë¬¸ì„œ ë‚´ìš©
            content = doc.page_content.strip()
            
            formatted_sections.append(header + content)
        
        return "\n\n".join(formatted_sections)

    def _unique_by_file(self, pairs: List[tuple], k: int) -> List[tuple]:
        """(Document, score) ë¦¬ìŠ¤íŠ¸ì—ì„œ íŒŒì¼ëª… ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µì„ ì œê±°í•˜ë©° ìµœëŒ€ kê°œ ë°˜í™˜
        ê°œì„ : PPTXëŠ” ìŠ¬ë¼ì´ë“œ ë‹¨ìœ„, PDFëŠ” í˜ì´ì§€ ë‹¨ìœ„ë¡œ ì¤‘ë³µ ì œê±°"""
        seen = set()
        results: List[tuple] = []
        file_chunk_counts = {}  # íŒŒì¼ë³„ ì²­í¬ ê°œìˆ˜ ì¶”ì 
        
        for doc, score in pairs:
            file_name = doc.metadata.get("file_name", "")
            chunk_type = doc.metadata.get("chunk_type", "")
            
            # PPTX: ìŠ¬ë¼ì´ë“œ ë‹¨ìœ„ ì¤‘ë³µ ì œê±°, PDF: í˜ì´ì§€ ë‹¨ìœ„ ì¤‘ë³µ ì œê±°
            # slide_number ë˜ëŠ” page_numberê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ file_nameë§Œ ì‚¬ìš©
            slide_number = doc.metadata.get("slide_number")
            page_number = doc.metadata.get("page_number")
            
            if slide_number is not None:
                # PPTX: file_name + slide_number ì¡°í•©ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
                key = f"{file_name}_slide_{slide_number}"
            elif page_number is not None:
                # PDF: file_name + page_number ì¡°í•©ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
                key = f"{file_name}_page_{page_number}"
            else:
                # ë©”íƒ€ë°ì´í„°ê°€ ì—†ìœ¼ë©´ íŒŒì¼ëª…ë§Œ ì‚¬ìš© (ê¸°ì¡´ ë°©ì‹)
                key = file_name
            
            # íŒŒì¼ë‹¹ ìµœëŒ€ ì²­í¬ ìˆ˜ ì œí•œ (ë„ˆë¬´ ë§ì€ ì²­í¬ ë°©ì§€)
            # ëª©ë¡ ë‚˜ì—´ ì§ˆë¬¸ì˜ ê²½ìš° ë” ë§ì´ í—ˆìš©
            max_per_file = 10  # ê¸°ë³¸ê°’: íŒŒì¼ë‹¹ ìµœëŒ€ 10ê°œ ì²­í¬
            
            if key in seen:
                # ì´ë¯¸ ë³¸ ì¡°í•©ì´ë©´ ê±´ë„ˆë›°ê¸° (ë™ì¼ ìŠ¬ë¼ì´ë“œ/í˜ì´ì§€ëŠ” 1ê°œë§Œ)
                continue
            
            # íŒŒì¼ë‹¹ ì²­í¬ ê°œìˆ˜ ì²´í¬
            if file_name in file_chunk_counts:
                if file_chunk_counts[file_name] >= max_per_file:
                    continue
                file_chunk_counts[file_name] += 1
            else:
                file_chunk_counts[file_name] = 1
            
            seen.add(key)
            results.append((doc, score))
            if len(results) >= k:
                break
        return results

    def _search_candidates(self, question: str) -> List[tuple]:
        """í•˜ì´ë¸Œë¦¬ë“œ(í‚¤ì›Œë“œ+ë²¡í„°) â†’ Re-ranker ì…ë ¥ í›„ë³´ í™•ë³´ (Phase 3: ì—”í‹°í‹° boost ì¶”ê°€)"""
        try:
            # í•˜ì´ë¸Œë¦¬ë“œë¡œ ë„‰ë„‰íˆ í›„ë³´ í™•ë³´ (ì„¤ì •ëœ reranker_initial_k ì‚¬ìš©)
            initial_k = max(self.reranker_initial_k, max(self.top_k * 8, 60))
            hybrid = self.vectorstore.similarity_search_hybrid(
                question, initial_k=initial_k, top_k=initial_k
            )
            
            # Phase 3: ì—”í‹°í‹° ë§¤ì¹­ ì²­í¬ì— boost ì ìš©
            if hasattr(self.vectorstore, 'entity_index') and self.vectorstore.entity_index:
                hybrid = self._apply_entity_boost(question, hybrid)
            
            return hybrid
        except Exception:
            # í´ë°±: ë²¡í„° ê²€ìƒ‰
            return self.vectorstore.similarity_search_with_score(question, k=max(self.reranker_initial_k, 60))
    
    def _apply_entity_boost(self, question: str, candidates: List[tuple], boost_factor: float = 1.5) -> List[tuple]:
        """ì—”í‹°í‹° ë§¤ì¹­ ì²­í¬ì— boost ì ìˆ˜ ì ìš© (Phase 3)"""
        # ì¿¼ë¦¬ì—ì„œ ì—”í‹°í‹° ê°ì§€ (ë„ë©”ì¸ ìš©ì–´ ì‚¬ì „ í™œìš©)
        detected_entities = []
        question_lower = question.lower()
        
        # ë„ë©”ì¸ ìš©ì–´ ì‚¬ì „ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ
        for key in self._domain_lexicon.keys():
            if key.lower() in question_lower:
                detected_entities.append(key)
        
        # ê°ì§€ëœ ì—”í‹°í‹°ê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if not detected_entities:
            return candidates
        
        # ì—”í‹°í‹° ë§¤ì¹­ ì²­í¬ ID ìˆ˜ì§‘
        matching_chunk_ids = set()
        for entity in detected_entities:
            chunk_ids = self.vectorstore.search_by_entity(entity)
            matching_chunk_ids.update(chunk_ids)
        
        if not matching_chunk_ids:
            return candidates
        
        # ë§¤ì¹­ë˜ëŠ” ì²­í¬ì— boost ì ìš©
        boosted_candidates = []
        boost_count = 0
        for doc, score in candidates:
            chunk_id = doc.metadata.get('chunk_id') or doc.metadata.get('id')
            if chunk_id in matching_chunk_ids:
                boosted_score = score * boost_factor
                boosted_candidates.append((doc, boosted_score))
                boost_count += 1
            else:
                boosted_candidates.append((doc, score))
        
        if boost_count > 0:
            print(f"âœ¨ ì—”í‹°í‹° boost ì ìš©: {boost_count}ê°œ ì²­í¬ (ê°ì§€ëœ ì—”í‹°í‹°: {', '.join(detected_entities)})")
        
        return boosted_candidates

    def _detect_query_type(self, question: str) -> str:
        """ì¿¼ë¦¬ íƒ€ì… ê°ì§€ (êµ¬ì²´ì  ì •ë³´ ì¶”ì¶œ, ìš”ì•½, ë¹„êµ, ê´€ê³„ ë¶„ì„ ë“±)"""
        question_lower = question.lower()
        
        # êµ¬ì²´ì  ì •ë³´ ì¶”ì¶œ í‚¤ì›Œë“œ
        specific_keywords = ["ë¬´ì—‡ì¸ê°€", "ì–¼ë§ˆì¸ê°€", "ëˆ„êµ¬ì¸ê°€", "ì–¸ì œ", "ì–´ë””", 
                           "ì–´ë–¤", "ë‚˜ì—´", "ì¶”ì¶œ", "ìˆ˜ì¹˜", "ê°’", "ì´ë¦„", "êµ¬ì¡°"]
        if any(keyword in question_lower for keyword in specific_keywords):
            return "specific_info"
        
        # ìš”ì•½ í‚¤ì›Œë“œ
        summary_keywords = ["ìš”ì•½", "ì •ë¦¬", "í•µì‹¬", "ì£¼ìš” ë‚´ìš©", "ê°œìš”", "ê°œìš”"]
        if any(keyword in question_lower for keyword in summary_keywords):
            return "summary"
        
        # ë¹„êµ ë¶„ì„ í‚¤ì›Œë“œ
        comparison_keywords = ["ë¹„êµ", "ì°¨ì´", "ëŒ€ë¹„", "ì–´ëŠ ê²ƒì´", "vs", "versus"]
        if any(keyword in question_lower for keyword in comparison_keywords):
            return "comparison"
        
        # ê´€ê³„ ë¶„ì„ í‚¤ì›Œë“œ
        relationship_keywords = ["ê´€ê³„", "ìƒê´€ê´€ê³„", "ê²½í–¥", "ì˜í–¥", "ë©”ì»¤ë‹ˆì¦˜", "ì›ì¸"]
        if any(keyword in question_lower for keyword in relationship_keywords):
            return "relationship"
        
        # ê¸°ë³¸ê°’
        return "general"
    
    def _get_context(self, question: str) -> str:
        context_start = time.perf_counter()
        # ì¿¼ë¦¬ íƒ€ì… ê°ì§€
        query_type = self._detect_query_type(question)
        
        # êµ¬ì²´ì  ì •ë³´ ì¶”ì¶œ ëª¨ë“œ: Small-to-Large ê²€ìƒ‰ í™œìš©
        if query_type == "specific_info":
            try:
                # 1ë‹¨ê³„: Small-to-Large ê²€ìƒ‰ìœ¼ë¡œ ì •í™•í•œ ì²­í¬ ì°¾ê¸°
                stl_results = self.small_to_large_search.search_with_context_expansion(
                    question, top_k=20, max_parents=5, partial_context_size=300
                )
                
                if stl_results:
                    # Small-to-Large ê²°ê³¼ë¥¼ (doc, score) í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
                    weighted_results = []
                    for doc in stl_results:
                        # ì²­í¬ íƒ€ì…ë³„ ê°€ì¤‘ì¹˜ ì ìš©
                        chunk_type_weight = doc.metadata.get("chunk_type_weight", 1.0)
                        # ê¸°ë³¸ ì ìˆ˜ (Small-to-LargeëŠ” ì •í™•í•œ ë§¤ì¹­ì„ ìš°ì„ í•˜ë¯€ë¡œ ë†’ì€ ì ìˆ˜)
                        base_score = 0.8 * chunk_type_weight
                        weighted_results.append((doc, base_score))
                    
                    # Re-ranking ì ìš© (ìˆëŠ” ê²½ìš°)
                    if self.use_reranker and len(weighted_results) > 0:
                        docs_for_rerank = [{
                            "page_content": d.page_content,
                            "metadata": d.metadata,
                            "vector_score": s,
                            "document": d
                        } for d, s in weighted_results]
                        reranked = self.reranker.rerank(question, docs_for_rerank, top_k=min(15, len(docs_for_rerank)))
                        pairs = [(d["document"], d.get("rerank_score", 0.8)) for d in reranked]
                    else:
                        pairs = weighted_results
                    
                    # ì¤‘ë³µ ì œê±°
                    dedup = self._unique_by_file(pairs, self.top_k * 2)
                    self._last_retrieved_docs = dedup[:self.top_k]
                    docs = [d for d, _ in self._last_retrieved_docs]
                    elapsed = time.perf_counter() - context_start
                    print(f"[Timing] context retrieval (Small-to-Large, type={query_type}): {elapsed:.2f}s")
                    print(f"ğŸ” êµ¬ì²´ì  ì •ë³´ ì¶”ì¶œ ëª¨ë“œ: Small-to-Large ê²€ìƒ‰ (ì¿¼ë¦¬ íƒ€ì…: {query_type})")
                    return self._format_docs(docs)
            except Exception as e:
                print(f"Small-to-Large ê²€ìƒ‰ ì‹¤íŒ¨, ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±: {e}")
                # í´ë°±: ê¸°ë³¸ ê²€ìƒ‰ ê³„ì† ì§„í–‰
        
        # ìš”ì•½ ëª¨ë“œ: ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
        if query_type == "summary":
            # ìš”ì•½ì€ ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ í•„ìš”
            original_top_k = self.top_k
            self.top_k = min(10, original_top_k * 2)
            try:
                context = self._get_context_standard(question)
                elapsed = time.perf_counter() - context_start
                print(f"[Timing] context retrieval (summary, type={query_type}): {elapsed:.2f}s")
                self.top_k = original_top_k
                return context
            except:
                self.top_k = original_top_k
                return ""
        
        # ê¸°ë³¸ ê²€ìƒ‰ (ê¸°ì¡´ ë¡œì§)
        context = self._get_context_standard(question)
        elapsed = time.perf_counter() - context_start
        print(f"[Timing] context retrieval (standard, type={query_type}): {elapsed:.2f}s")
        return context
    
    def _get_context_standard(self, question: str) -> str:
        """í‘œì¤€ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ê¸°ì¡´ ë¡œì§)"""
        overall_start = time.perf_counter()
        
        # ğŸ†• ë™ì  top_k ê²°ì • (ì§ˆë¬¸ íŠ¹ì„± ë¶„ì„)
        dynamic_top_k = self.determine_optimal_top_k(question)
        print(f"ğŸ” ì§ˆë¬¸ íŠ¹ì„± ë¶„ì„: top_k = {dynamic_top_k} (ê¸°ë³¸: {self.top_k})")
        
        # Multi-Query Rewriting ì ìš©
        if self.enable_multi_query:
            mq_start = time.perf_counter()
            queries = self.generate_rewritten_queries(question, num_queries=self.multi_query_num)
            print(f"[Timing] multi_query_generate: {time.perf_counter() - mq_start:.2f}s (queries={len(queries)})")
            all_retrieved_chunks = []
            chunk_id_set = set()
            
            # ëª¨ë“  ì¿¼ë¦¬ì— ëŒ€í•´ ê²€ìƒ‰ ìˆ˜í–‰
            for idx, query in enumerate(queries, start=1):
                query_start = time.perf_counter()
                try:
                    results = []
                    if self.use_reranker:
                        base = self._search_candidates(query)
                        if base:
                            docs_for_rerank = [{
                                "page_content": d.page_content,
                                "metadata": d.metadata,
                                "vector_score": s,
                                "document": d
                            } for d, s in base]
                            reranked = self.reranker.rerank(query, docs_for_rerank, top_k=max(self.top_k * 3, 15))
                            results = [(d["document"], d.get("rerank_score", 0)) for d in reranked]
                        else:
                            results = []
                    else:
                        results = self.vectorstore.similarity_search_with_score(query, k=max(self.top_k * 3, 15))
                    print(f"[Timing] retrieval[{idx}/{len(queries)}]: {time.perf_counter() - query_start:.2f}s (docs={len(results)})")
                    
                    # ì¤‘ë³µ ì œê±° (ë¬¸ì„œ ë‚´ìš© ê¸°ì¤€)
                    for doc, score in results:
                        doc_id = f"{doc.metadata.get('source', '')}_{doc.page_content[:50]}"
                        if doc_id not in chunk_id_set:
                            all_retrieved_chunks.append((doc, score))
                            chunk_id_set.add(doc_id)
                            
                except Exception as e:
                    print(f"ì¿¼ë¦¬ '{query}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    continue
            
            if all_retrieved_chunks:
                # ì›ë³¸ ì¿¼ë¦¬ë¡œ ì¬ìˆœìœ„ ë§¤ê¹€
                if self.use_reranker:
                    rerank_start = time.perf_counter()
                    docs_for_final_rerank = [{
                        "page_content": d.page_content,
                        "metadata": d.metadata,
                        "vector_score": s,
                        "document": d
                    } for d, s in all_retrieved_chunks]
                    final_reranked = self.reranker.rerank(question, docs_for_final_rerank, top_k=max(self.top_k * 2, 20))
                    pairs = [(d["document"], d.get("rerank_score", 0)) for d in final_reranked]
                    print(f"[Timing] final_rerank (multi-query): {time.perf_counter() - rerank_start:.2f}s (candidates={len(all_retrieved_chunks)})")
                else:
                    pairs = all_retrieved_chunks
                
                dedup = self._unique_by_file(pairs, dynamic_top_k)  # ë™ì  top_k ì‚¬ìš©
                self._last_retrieved_docs = dedup
                docs = [d for d, _ in dedup]
                print(f"[Timing] context_standard total: {time.perf_counter() - overall_start:.2f}s (mode=multi-query, top_k={dynamic_top_k})")
                return self._format_docs(docs)
        
        # í´ë°±: ë‹¨ì¼ ì¿¼ë¦¬ ê²€ìƒ‰ (ë™ì˜ì–´ í™•ì¥ í¬í•¨)
        syn_start = time.perf_counter()
        expanded_question = self.expand_query_with_synonyms(question)
        print(f"[Timing] synonym_expand: {time.perf_counter() - syn_start:.2f}s")
        
        if self.use_reranker:
            retrieval_start = time.perf_counter()
            base = self._search_candidates(expanded_question)
            if not base:
                self._last_retrieved_docs = []
                print(f"[Timing] context_standard total: {time.perf_counter() - overall_start:.2f}s (mode=fallback, docs=0)")
                return ""
            # base ëŠ” (doc, score) í˜•íƒœ
            docs_for_rerank = [{
                "page_content": d.page_content,
                "metadata": d.metadata,
                "vector_score": s,
                "document": d
            } for d, s in base]
            print(f"[Timing] candidate_retrieval (fallback): {time.perf_counter() - retrieval_start:.2f}s (candidates={len(base)})")
            rerank_start = time.perf_counter()
            reranked = self.reranker.rerank(expanded_question, docs_for_rerank, top_k=max(self.top_k * 8, 40))
            pairs = [(d["document"], d.get("rerank_score", 0)) for d in reranked]
            dedup = self._unique_by_file(pairs, dynamic_top_k)  # ë™ì  top_k ì‚¬ìš©
            
            # ìºì‹œ ì €ì¥: ì‹¤ì œ ì‚¬ìš©ëœ ë¬¸ì„œì™€ ì ìˆ˜
            self._last_retrieved_docs = dedup  # [(doc, score), ...]
            
            docs = [d for d, _ in dedup]
            print(f"[Timing] final_rerank (fallback): {time.perf_counter() - rerank_start:.2f}s (selected={len(dedup)})")
        else:
            retrieval_start = time.perf_counter()
            pairs = self.vectorstore.similarity_search_with_score(expanded_question, k=max(self.top_k * 8, 40))
            dedup = self._unique_by_file(pairs, dynamic_top_k)  # ë™ì  top_k ì‚¬ìš©
            
            # ìºì‹œ ì €ì¥
            self._last_retrieved_docs = dedup
            
            docs = [d for d, _ in dedup]
            print(f"[Timing] candidate_retrieval (vector fallback): {time.perf_counter() - retrieval_start:.2f}s (selected={len(dedup)})")
        print(f"[Timing] context_standard total: {time.perf_counter() - overall_start:.2f}s (mode=fallback, top_k={dynamic_top_k})")
        return self._format_docs(docs)

    def expand_query_with_synonyms(self, original_query: str) -> str:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ ì¿¼ë¦¬ì— ëŒ€í•œ ë™ì˜ì–´/ì—°ê´€ì–´ë¥¼ ìƒì„±í•˜ê³  í™•ì¥ëœ ì¿¼ë¦¬ë¥¼ ë°˜í™˜"""
        if not self.enable_synonym_expansion:
            return original_query
            
        try:
            prompt = f"""
ì‚¬ìš©ìì˜ ê²€ìƒ‰ ì¿¼ë¦¬: "{original_query}"

ì´ ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë™ì˜ì–´ ë˜ëŠ” ë°€ì ‘í•˜ê²Œ ì—°ê´€ëœ ê²€ìƒ‰ ìš©ì–´ 3ê°œë¥¼ ìƒì„±í•´ ì¤˜.
ê²°ê³¼ëŠ” JSON ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ ì¤˜.
ì˜ˆì‹œ: ["ìš©ì–´1", "ìš©ì–´2", "ìš©ì–´3"]
"""
            
            response = self.llm.invoke(prompt)
            
            # ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            if hasattr(response, 'content'):
                response_text = response.content
            elif hasattr(response, 'text'):
                response_text = response.text
            else:
                response_text = str(response)
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
                json_match = re.search(r'\[.*?\]', response_text)
                if json_match:
                    related_terms = json.loads(json_match.group())
                else:
                    # JSON í˜•ì‹ì´ ì•„ë‹Œ ê²½ìš° í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
                    lines = response_text.strip().split('\n')
                    related_terms = []
                    for line in lines:
                        line = line.strip().strip('"[]')
                        if line and len(line) > 1:
                            related_terms.append(line)
                    related_terms = related_terms[:3]  # ìµœëŒ€ 3ê°œ
                
                # ì›ë³¸ ì¿¼ë¦¬ì™€ ì—°ê´€ì–´ë¥¼ ê²°í•©
                if related_terms:
                    expanded_query = f"{original_query} (ê´€ë ¨ ìš©ì–´: {', '.join(related_terms)})"
                    print(f"ğŸ” ë™ì˜ì–´ í™•ì¥: {original_query} â†’ {expanded_query}")
                    return expanded_query
                    
            except (json.JSONDecodeError, ValueError) as e:
                print(f"ë™ì˜ì–´ íŒŒì‹± ì‹¤íŒ¨: {e}")
                
        except Exception as e:
            print(f"ë™ì˜ì–´ í™•ì¥ ì‹¤íŒ¨: {e}")
        
        return original_query

    def determine_optimal_top_k(self, question: str) -> int:
        """ì§ˆë¬¸ íŠ¹ì„±ì— ë”°ë¼ ìµœì ì˜ top_k ê°’ì„ ë™ì ìœ¼ë¡œ ê²°ì •"""
        try:
            prompt = f"""ë‹¹ì‹ ì€ RAG ê²€ìƒ‰ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ í•„ìš”í•œ ë¬¸ì„œ ê°œìˆ˜ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: "{question}"

ë‹¤ìŒ ì§ˆë¬¸ë“¤ì— ëŒ€í•´ ë¶„ì„í•˜ì„¸ìš”:
1. ì´ ì§ˆë¬¸ì€ ë‹¨ì¼ ì‚¬ì‹¤ ì°¾ê¸°ì¸ê°€? (ì˜ˆ: "ë¬´ì—‡", "ì–¼ë§ˆ", "ëˆ„êµ¬" - ê°„ë‹¨í•œ ë‹µë³€)
2. ì´ ì§ˆë¬¸ì€ ëª©ë¡ ë‚˜ì—´ì¸ê°€? (ì˜ˆ: "ëª¨ë‘", "ì „ì²´", "ë‚˜ì—´", "ëª©ë¡", "ì œëª©ë“¤" - ì—¬ëŸ¬ í•­ëª©)
3. ì´ ì§ˆë¬¸ì€ ë¹„êµ/ë¶„ì„ì¸ê°€? (ì˜ˆ: "ì°¨ì´", "ë¹„êµ", "ê´€ê³„" - ì—¬ëŸ¬ ê´€ì )
4. ì´ ì§ˆë¬¸ì€ ì¢…í•© ì •ë³´ì¸ê°€? (ì˜ˆ: "ìš”ì•½", "í•µì‹¬", "ê°œìš”" - ì „ì²´ ì»¨í…ìŠ¤íŠ¸)

í•„ìš”í•œ ë¬¸ì„œ ê°œìˆ˜:
- ë‹¨ì¼ ì‚¬ì‹¤: 3-5ê°œ
- ëª©ë¡ ë‚˜ì—´: 20-30ê°œ (ì¤‘ë³µ ì œê±° í›„ ì ì ˆí•œ ìˆ˜)
- ë¹„êµ/ë¶„ì„: 10-20ê°œ
- ì¢…í•© ì •ë³´: 10-15ê°œ

ê²°ê³¼ëŠ” ìˆ«ìë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë²”ìœ„: 3~30
ì˜ˆì‹œ: 5, 15, 20"""

            response = self.llm.invoke(prompt)
            response_text = response.content if hasattr(response, 'content') else str(response)
            
            # ìˆ«ì ì¶”ì¶œ
            numbers = re.findall(r'\d+', response_text)
            if numbers:
                top_k = int(numbers[0])
                top_k = max(3, min(30, top_k))  # 3~30 ë²”ìœ„ ì œí•œ
                print(f"ğŸ¯ ë™ì  top_k ê²°ì •: {top_k} (ì§ˆë¬¸ ìœ í˜• ë¶„ì„)")
                return top_k
        except Exception as e:
            print(f"ë™ì  top_k ê²°ì • ì‹¤íŒ¨: {e}")
        
        # í´ë°±: ê¸°ë³¸ê°’
        return self.top_k
    
    def generate_rewritten_queries(self, original_query: str, num_queries: int = 3) -> List[str]:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ ì¿¼ë¦¬ë¥¼ ì—¬ëŸ¬ ê´€ì ì—ì„œ ì¬ì‘ì„±í•œ ëŒ€ì•ˆ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±"""
        if not self.enable_multi_query:
            return [original_query]
            
        try:
            prompt = f"""
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë” ë‚˜ì€ ê²€ìƒ‰ ê²°ê³¼ë¡œ ì´ë„ëŠ” ì „ë¬¸ ê²€ìƒ‰ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì›ë³¸ ì¿¼ë¦¬ë¥¼ {num_queries}ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì—ì„œ ì¬ì‘ì„±í•´ ì£¼ì‹­ì‹œì˜¤.

- ì›ë³¸ ì¿¼ë¦¬ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì‹­ì‹œì˜¤.
- ì¿¼ë¦¬ë“¤ì€ ì„œë¡œ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹(ì˜ˆ: ê¸°ìˆ ì  ì§ˆë¬¸, ê°œë…ì  ì§ˆë¬¸, ë¬¸ì œ í•´ê²°)ì„ ë°˜ì˜í•´ì•¼ í•©ë‹ˆë‹¤.

ì›ë³¸ ì¿¼ë¦¬: "{original_query}"

ê²°ê³¼ëŠ” JSON ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ ì£¼ì‹­ì‹œì˜¤. 
ì˜ˆì‹œ: ["ì¿¼ë¦¬1", "ì¿¼ë¦¬2", "ì¿¼ë¦¬3"]
"""
            
            response = self.llm.invoke(prompt)
            
            # ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            if hasattr(response, 'content'):
                response_text = response.content
            elif hasattr(response, 'text'):
                response_text = response.text
            else:
                response_text = str(response)
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
                json_match = re.search(r'\[.*?\]', response_text)
                if json_match:
                    rewritten_queries = json.loads(json_match.group())
                else:
                    # JSON í˜•ì‹ì´ ì•„ë‹Œ ê²½ìš° í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
                    lines = response_text.strip().split('\n')
                    rewritten_queries = []
                    for line in lines:
                        line = line.strip().strip('"[]')
                        if line and len(line) > 1:
                            rewritten_queries.append(line)
                    rewritten_queries = rewritten_queries[:num_queries]  # ìµœëŒ€ num_queriesê°œ
                
                # ì›ë³¸ ì¿¼ë¦¬ê°€ í¬í•¨ë˜ì§€ ì•Šì•˜ë‹¤ë©´, ë¦¬ìŠ¤íŠ¸ì˜ ë§¨ ì•ì— ì¶”ê°€
                if original_query not in rewritten_queries:
                    rewritten_queries.insert(0, original_query)
                    
                print(f"ğŸ”„ ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„±: {original_query} â†’ {len(rewritten_queries)}ê°œ ì¿¼ë¦¬")
                return rewritten_queries
                    
            except (json.JSONDecodeError, ValueError) as e:
                print(f"ë‹¤ì¤‘ ì¿¼ë¦¬ íŒŒì‹± ì‹¤íŒ¨: {e}")
                
        except Exception as e:
            print(f"ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return [original_query]

    def _format_chat_history(self, messages: List[Dict[str, str]], max_messages: int = 5) -> str:
        if not messages:
            return "ì´ì „ ëŒ€í™” ì—†ìŒ"
        recent_messages = messages[-max_messages * 2:] if len(messages) > max_messages * 2 else messages
        formatted = []
        for msg in recent_messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            if role == "user":
                formatted.append(f"ì‚¬ìš©ì: {content}")
            elif role == "assistant":
                formatted.append(f"ì–´ì‹œìŠ¤í„´íŠ¸: {content}")
        return "\n".join(formatted) if formatted else "ì´ì „ ëŒ€í™” ì—†ìŒ"

    def query(self, question: str, chat_history: List[Dict[str, str]] = None) -> Dict[str, Any]:
        try:
            formatted_history = self._format_chat_history(chat_history or [])
            
            # ì¿¼ë¦¬ íƒ€ì… ê°ì§€ ë° í”„ë¡¬í”„íŠ¸ ì„ íƒ
            query_type = self._detect_query_type(question)
            if query_type in self.prompt_templates:
                selected_template = self.prompt_templates[query_type]
                self.prompt = PromptTemplate(
                    template=selected_template,
                    input_variables=["chat_history", "context", "question"]
                )
                # ì²´ì¸ ì¬êµ¬ì„± (í”„ë¡¬í”„íŠ¸ ë³€ê²½ ë°˜ì˜)
                self.chain = (
                    {
                        "context": lambda x: self._get_context(x["question"]),
                        "chat_history": lambda x: x.get("chat_history", "ì´ì „ ëŒ€í™” ì—†ìŒ"),
                        "question": lambda x: x["question"]
                    }
                    | self.prompt
                    | self.llm
                    | StrOutputParser()
                )
            
            # ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (_last_retrieved_docs ì—…ë°ì´íŠ¸ë¨)
            context = self._get_context(question)
            
            # ë‹µë³€ ìƒì„±
            answer = self.chain.invoke({
                "question": question,
                "chat_history": formatted_history
            })
            
            # Phase 2: ë‹µë³€ ê²€ì¦ ë° ì¬ìƒì„± (ìƒìš© ì„œë¹„ìŠ¤ ìˆ˜ì¤€)
            docs_for_confidence = [d for d, _ in self._last_retrieved_docs[:self.top_k]]
            verification_result = self._verify_answer_quality(question, answer, docs_for_confidence)
            
            if not verification_result["is_valid"]:
                print(f"âš ï¸ ë‹µë³€ ê²€ì¦ ì‹¤íŒ¨: {verification_result['reason']}")
                print(f"ğŸ”„ ë¬¸ì„œ ê¸°ë°˜ ì¬ìƒì„± ì‹œë„...")
                
                # ë¬¸ì„œ ê¸°ë°˜ ì¬ìƒì„±
                regenerated_answer = self._regenerate_answer(question, answer, docs_for_confidence, formatted_history)
                if regenerated_answer:
                    answer = regenerated_answer
                    print(f"âœ… ë‹µë³€ ì¬ìƒì„± ì™„ë£Œ")
                else:
                    print(f"âš ï¸ ì¬ìƒì„± ì‹¤íŒ¨, ì›ë³¸ ë‹µë³€ ì‚¬ìš©")
            
            # ìºì‹œëœ ë¬¸ì„œì—ì„œ ì¶œì²˜ ì •ë³´ ìƒì„±
            sources = []
            docs_for_confidence = []
            
            for doc, score in self._last_retrieved_docs[:self.top_k]:
                docs_for_confidence.append(doc)
                source_info = {
                    "file_name": doc.metadata.get("file_name", "Unknown"),
                    "page_number": doc.metadata.get("page_number", "Unknown"),
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "similarity_score": float(round(score * 100, 1))  # 0-100 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
                }
                sources.append(source_info)
            
            # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            confidence = self._calculate_confidence_score(question, answer, docs_for_confidence)
            
            return {
                "answer": answer,
                "sources": sources,
                "confidence": confidence,
                "success": True
            }
        except Exception as e:
            print(f"âŒ query() ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return {
                "answer": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "sources": [],
                "confidence": 0.0,
                "success": False
            }
    
    def _verify_answer_quality(self, question: str, answer: str, docs: List[Document]) -> Dict[str, Any]:
        """ë‹µë³€ í’ˆì§ˆ ê²€ì¦ (Phase 2: ìƒìš© ì„œë¹„ìŠ¤ ìˆ˜ì¤€)
        
        Returns:
            {
                "is_valid": bool,
                "reason": str,
                "scores": {
                    "no_forbidden_phrases": float,
                    "has_citation": float,
                    "content_match": float,
                    "specificity": float
                }
            }
        """
        if not docs or not answer:
            return {
                "is_valid": False,
                "reason": "ë¬¸ì„œ ë˜ëŠ” ë‹µë³€ì´ ì—†ìŒ",
                "scores": {}
            }
        
        answer_lower = answer.lower()
        doc_contents = " ".join([d.page_content.lower() for d in docs])
        doc_metadata = [(d.metadata.get("file_name", ""), d.metadata.get("page_number", "")) for d in docs]
        
        scores = {}
        
        # 1. ê¸ˆì§€ êµ¬ë¬¸ ê²€ì‚¬
        forbidden_phrases = [
            "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤",
            "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "ì—†ìŠµë‹ˆë‹¤",
            "ì •ë³´ ë¶€ì¡±",
            "í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "cannot find",
            "not found",
            "no information"
        ]
        has_forbidden = any(phrase in answer_lower for phrase in forbidden_phrases)
        scores["no_forbidden_phrases"] = 0.0 if has_forbidden else 1.0
        
        # 2. ë¬¸ì„œ ì¸ìš© ê²€ì‚¬ (í˜ì´ì§€ ë²ˆí˜¸, íŒŒì¼ëª… ë“± ë©”íƒ€ë°ì´í„° ì–¸ê¸‰)
        has_citation = False
        citation_keywords = ["í˜ì´ì§€", "page", "ë¬¸ì„œ", "íŒŒì¼", "ì„¹ì…˜", "section"]
        has_citation_keywords = any(keyword in answer_lower for keyword in citation_keywords)
        
        # íŒŒì¼ëª…ì´ë‚˜ í˜ì´ì§€ ë²ˆí˜¸ ì§ì ‘ ì–¸ê¸‰ í™•ì¸
        for file_name, page_num in doc_metadata:
            if file_name and file_name.lower() in answer_lower:
                has_citation = True
                break
            if page_num and str(page_num) in answer:
                has_citation = True
                break
        
        scores["has_citation"] = 1.0 if (has_citation or has_citation_keywords) else 0.3
        
        # 3. ë¬¸ì„œ ë‚´ìš©ê³¼ì˜ ì¼ì¹˜ ê²€ì‚¬ (í‚¤ì›Œë“œ ë§¤ì¹­)
        question_keywords = set(re.findall(r'\w+', question.lower()))
        doc_keywords = set(re.findall(r'\w+', doc_contents[:1000]))  # ì²˜ìŒ 1000ìë§Œ
        
        # ë‹µë³€ì— ë¬¸ì„œ í‚¤ì›Œë“œê°€ ì–¼ë§ˆë‚˜ í¬í•¨ë˜ëŠ”ì§€
        matching_keywords = question_keywords.intersection(doc_keywords)
        answer_has_keywords = sum(1 for kw in matching_keywords if kw in answer_lower)
        content_match_score = min(1.0, answer_has_keywords / max(len(matching_keywords), 1))
        scores["content_match"] = content_match_score
        
        # 4. êµ¬ì²´ì„± ê²€ì‚¬ (ì¼ë°˜í™”ëœ ë‹µë³€ ê°ì§€)
        # ì¼ë°˜í™”ëœ êµ¬ë¬¸ íŒ¨í„´
        generic_phrases = [
            "ì¼ë°˜ì ìœ¼ë¡œ",
            "ë³´í†µ",
            "ëŒ€ë¶€ë¶„ì˜ ê²½ìš°",
            "ì¼ë°˜ì ìœ¼ë¡œ ì•Œë ¤ì§„",
            "ì¼ë°˜ì ì¸ ì›ë¦¬",
            "ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ”"
        ]
        has_generic = any(phrase in answer_lower for phrase in generic_phrases)
        
        # ë¬¸ì„œ íŠ¹ì • ë‚´ìš© (ìˆ˜ì¹˜, ì´ë¦„, ê³ ìœ ëª…ì‚¬ ë“±) í¬í•¨ ì—¬ë¶€
        has_specifics = bool(re.search(r'\d+[.%]?', answer)) or len([w for w in answer.split() if len(w) > 5]) > 3
        specificity_score = 0.5 if has_generic else 1.0
        if has_specifics:
            specificity_score = min(1.0, specificity_score + 0.3)
        scores["specificity"] = specificity_score
        
        # ì¢…í•© ê²€ì¦
        total_score = (
            scores["no_forbidden_phrases"] * 0.4 +
            scores["has_citation"] * 0.3 +
            scores["content_match"] * 0.2 +
            scores["specificity"] * 0.1
        )
        
        is_valid = total_score >= 0.6 and scores["no_forbidden_phrases"] > 0
        
        reasons = []
        if not is_valid:
            if scores["no_forbidden_phrases"] == 0:
                reasons.append("ê¸ˆì§€ êµ¬ë¬¸ ì‚¬ìš©")
            if scores["has_citation"] < 0.5:
                reasons.append("ë¬¸ì„œ ì¸ìš© ë¶€ì¡±")
            if scores["content_match"] < 0.3:
                reasons.append("ë¬¸ì„œ ë‚´ìš©ê³¼ ë¶ˆì¼ì¹˜")
            if scores["specificity"] < 0.5:
                reasons.append("ì¼ë°˜í™”ëœ ë‹µë³€")
        
        return {
            "is_valid": is_valid,
            "reason": ", ".join(reasons) if reasons else "ì •ìƒ",
            "total_score": total_score,
            "scores": scores
        }
    
    def _regenerate_answer(self, question: str, original_answer: str, docs: List[Document], 
                          chat_history: str) -> Optional[str]:
        """ê²€ì¦ ì‹¤íŒ¨ ì‹œ ë¬¸ì„œ ê¸°ë°˜ ì¬ìƒì„± (Phase 2)"""
        if not docs:
            return None
        
        try:
            # ì¬ìƒì„± ì „ìš© í”„ë¡¬í”„íŠ¸
            context = self._format_docs(docs)
            
            regeneration_prompt = f"""ì´ì „ì— ìƒì„±ëœ ë‹µë³€ì´ ë¬¸ì„œ ê¸°ë°˜ì´ ì•„ë‹ˆì—ˆìŠµë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì‹œ ë‹µë³€í•˜ì„¸ìš”.

âš ï¸ ì¤‘ìš” ê·œì¹™:
1. **ë¬¸ì„œ ìš°ì„ **: ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œì—ì„œë§Œ ì •ë³´ë¥¼ ì°¾ìœ¼ì„¸ìš”
2. **ê¸ˆì§€ êµ¬ë¬¸**: "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ì—†ìŠµë‹ˆë‹¤"ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
3. **ë¬¸ì„œ ì¸ìš©**: ë°˜ë“œì‹œ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì¸ìš©í•˜ê³  í˜ì´ì§€/íŒŒì¼ ì •ë³´ë¥¼ ëª…ì‹œí•˜ì„¸ìš”
4. **êµ¬ì²´ì„±**: ë¬¸ì„œì˜ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ì´ë¦„, ë‚´ìš©ì„ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”

ì´ì „ ëŒ€í™”:
{chat_history}

ì œê³µëœ ë¬¸ì„œ:
{context}

ì§ˆë¬¸:
{question}

ì´ì „ ë‹µë³€ (ì°¸ê³ ìš©, ê°œì„  í•„ìš”):
{original_answer}

ìœ„ ì´ì „ ë‹µë³€ì„ ê°œì„ í•˜ì—¬, ì œê³µëœ ë¬¸ì„œì— ê·¼ê±°í•œ êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.

ë‹µë³€:"""
            
            # LLM ì¬ìƒì„±
            regenerated = self.llm.invoke(regeneration_prompt)
            
            # ì‘ë‹µ íŒŒì‹±
            if hasattr(regenerated, 'content'):
                return regenerated.content.strip()
            elif hasattr(regenerated, 'text'):
                return regenerated.text.strip()
            else:
                return str(regenerated).strip()
                
        except Exception as e:
            print(f"âš ï¸ ì¬ìƒì„± ì˜¤ë¥˜: {e}")
            return None
    
    def _calculate_confidence_score(self, question: str, answer: str, docs: List[Document]) -> float:
        """ë‹µë³€ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° (0-100)"""
        if not docs:
            return 0.0
        
        # 1. ë¬¸ì„œ ê°œìˆ˜ ê¸°ë°˜ ì ìˆ˜ (ë” ë§ì€ ì¶œì²˜ = ë†’ì€ ì‹ ë¢°ë„)
        doc_score = min(len(docs) / 5.0, 1.0)  # 5ê°œ ì´ìƒì´ë©´ ë§Œì 
        
        # 2. ë‹µë³€ ê¸¸ì´ ì ìˆ˜ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ë©´ ê°ì )
        answer_length = len(answer)
        if answer_length < 50:
            length_score = 0.3
        elif answer_length < 100:
            length_score = 0.6
        elif answer_length < 500:
            length_score = 1.0
        elif answer_length < 1000:
            length_score = 0.9
        else:
            length_score = 0.8
        
        # 3. "ì •ë³´ ì—†ìŒ" í‚¤ì›Œë“œ ê°ì§€
        negative_keywords = ["ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤", "ì£„ì†¡í•©ë‹ˆë‹¤"]
        has_negative = any(keyword in answer for keyword in negative_keywords)
        negative_penalty = 0.5 if has_negative else 1.0
        
        # ìµœì¢… ì‹ ë¢°ë„ ì ìˆ˜ (0-100)
        confidence = (doc_score * 0.4 + length_score * 0.4 + negative_penalty * 0.2) * 100
        return round(confidence, 1)
    
    def query_stream(self, question: str, chat_history: List[Dict[str, str]] = None) -> Iterator[str]:
        overall_start = time.perf_counter()
        try:
            formatted_history = self._format_chat_history(chat_history or [])

            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ë¡œê·¸ í¬í•¨)
            context = self._get_context(question)

            # ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¡°í•© í›„ ë¡œê·¸ ì¶œë ¥
            prompt_text = self.prompt.format(
                chat_history=formatted_history,
                context=context,
                question=question
            )
            print("[Prompt] ---------- START ----------")
            print(prompt_text)
            print("[Prompt] ----------- END -----------")

            chain_start = time.perf_counter()
            first_chunk = True
            for chunk in self.llm.stream(prompt_text):
                # chunk íƒ€ì…ë³„ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                if hasattr(chunk, "content") and isinstance(chunk.content, str):
                    text = chunk.content
                elif hasattr(chunk, "text") and isinstance(chunk.text, str):
                    text = chunk.text
                else:
                    text = str(chunk)

                if text:
                    if first_chunk:
                        print(f"[Timing] LLM first token delay: {time.perf_counter() - chain_start:.2f}s")
                        first_chunk = False
                    yield text

            print(f"[Timing] LLM streaming total: {time.perf_counter() - chain_start:.2f}s")
            print(f"[Timing] query_stream total: {time.perf_counter() - overall_start:.2f}s")
        except Exception as e:
            print(f"[Timing] query_stream total: {time.perf_counter() - overall_start:.2f}s (error)")
            yield f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def get_source_documents(self, question: str = None) -> List[Dict[str, Any]]:
        """ìºì‹œëœ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¶œì²˜ë¡œ ë°˜í™˜ (ë‹µë³€ ìƒì„±ì— ì‹¤ì œ ì‚¬ìš©ëœ ë¬¸ì„œ)"""
        try:
            if not self._last_retrieved_docs:
                return []
            
            # ìºì‹œëœ ë¬¸ì„œì— ì ìˆ˜ ì •ê·œí™” ì ìš©
            is_reranker = self.use_reranker
            probs = self._normalize_scores(self._last_retrieved_docs, is_reranker=is_reranker)
            
            sources = []
            for (doc, raw_score), normalized_score in zip(self._last_retrieved_docs, probs):
                # 15% ì„ê³„ê°’ ì œê±° - ì‹¤ì œ ì‚¬ìš©ëœ ë¬¸ì„œëŠ” ëª¨ë‘ í‘œì‹œ
                sources.append({
                    "file_name": doc.metadata.get("file_name", "Unknown"),
                    "page_number": doc.metadata.get("page_number", "Unknown"),
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "similarity_score": float(round(normalized_score, 1)),
                    "raw_score": float(round(raw_score, 4))  # ë””ë²„ê¹…ìš©
                })
            
            return sources
        except Exception as e:
            print(f"ì¶œì²˜ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def clear_memory(self):
        pass
    
    def update_llm(self, llm_api_type: str, llm_base_url: str, llm_model: str, 
                   llm_api_key: str = "", temperature: float = 0.7):
        self.llm_api_type = llm_api_type
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.llm_api_key = llm_api_key
        self.temperature = temperature
        self.llm = self._create_llm()
        self.chain = (
            {
                "context": lambda x: self._get_context(x["question"]),
                "chat_history": lambda x: x.get("chat_history", "ì´ì „ ëŒ€í™” ì—†ìŒ"),
                "question": lambda x: x["question"]
            }
            | self.prompt
            | self.llm
            | StrOutputParser()
        )
    
    def update_retriever(self, vectorstore, top_k: int = 3):
        self.vectorstore = vectorstore
        self.top_k = top_k
        self.retriever = vectorstore.as_retriever(
            search_kwargs={"k": max(top_k * 5, 20)}
        )
        self.chain = (
            {
                "context": lambda x: self._get_context(x["question"]),
                "chat_history": lambda x: x.get("chat_history", "ì´ì „ ëŒ€í™” ì—†ìŒ"),
                "question": lambda x: x["question"]
            }
            | self.prompt
            | self.llm
            | StrOutputParser()
        )

    def _to_percentage(self, scores: List[float], is_reranker: bool) -> List[float]:
        """ì ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¥¼ 0~100%ë¡œ ì •ê·œí™”"""
        if not scores:
            return []
        if is_reranker:
            mn = min(scores)
            mx = max(scores)
            if abs(mx - mn) < 1e-9:
                return [50.0 for _ in scores]
            return [max(0.0, min(100.0, (s - mn) / (mx - mn) * 100.0)) for s in scores]
        # ë²¡í„° ê²€ìƒ‰: ê±°ë¦¬ê°€ 0~2 (ì‘ì„ìˆ˜ë¡ ìœ ì‚¬) ê°€ì • â†’ ìœ ì‚¬ë„ë¡œ ë³€í™˜
        return [max(0.0, min(100.0, (2.0 - s) / 2.0 * 100.0)) for s in scores]

    def _normalize_scores(self, pairs: List[tuple], is_reranker: bool) -> List[float]:
        """(doc, raw_score) -> 0~100% í™•ë¥ í˜• ì ìˆ˜ë¡œ ë³´ì • (ê°œì„  ë²„ì „)
        - reranker: Z-score ì •ê·œí™” í›„ Min-Maxë¡œ [0, 1] ë³€í™˜
        - vector:   í•˜ì´í¼ë³¼ë¦­ ë³€í™˜ + Min-Max ì •ê·œí™” í›„ softmax
        """
        import math
        import statistics
        
        if not pairs:
            return []
        
        raw = [float(s) for _, s in pairs]
        
        if is_reranker:
            # Reranker ì ìˆ˜: ì¼ë°˜ì ìœ¼ë¡œ ì–‘ìˆ˜ì´ë©° í° ê°’ì´ ì¢‹ìŒ
            # ìŒìˆ˜ ê°’ í•„í„°ë§ ë° ì •ê·œí™”
            filtered_scores = [s for s in raw if s > 0]
            
            if not filtered_scores:
                # ëª¨ë“  ì ìˆ˜ê°€ 0 ì´í•˜ì¸ ê²½ìš° ê· ë“± ë¶„ë°°
                return [50.0] * len(pairs)
            
            # Z-score ì •ê·œí™”
            mean_score = statistics.mean(filtered_scores)
            try:
                stdev_score = statistics.stdev(filtered_scores) if len(filtered_scores) > 1 else 1.0
            except:
                stdev_score = 1.0
            
            z_scores = []
            for s in raw:
                if s > 0 and stdev_score > 0:
                    z = (s - mean_score) / stdev_score
                    z_scores.append(z)
                else:
                    z_scores.append(-2.0)  # ìŒìˆ˜ ì ìˆ˜ëŠ” ë‚®ì€ Z-score
            
            # Z-scoreë¥¼ [0, 1] ë²”ìœ„ë¡œ Min-Max ì •ê·œí™”
            min_z = min(z_scores)
            max_z = max(z_scores)
            z_range = max_z - min_z if max_z > min_z else 1.0
            
            normalized = []
            for z in z_scores:
                if z_range > 0:
                    norm_val = (z - min_z) / z_range
                else:
                    norm_val = 0.5
                normalized.append(max(0.0, min(1.0, norm_val)))
            
            # Softmax ì ìš© (ë” ë¶€ë“œëŸ¬ìš´ í™•ë¥  ë¶„í¬)
            mx = max(normalized)
            exps = [math.exp(v - mx) for v in normalized]
            Z = sum(exps) or 1.0
            probs = [min(100.0, max(0.0, 100.0 * v / Z)) for v in exps]
        else:
            # Vector ê²€ìƒ‰: ê±°ë¦¬ ê¸°ë°˜ ì ìˆ˜ (ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ)
            # ìŒìˆ˜ ê°’ ì²˜ë¦¬ ë° í•˜ì´í¼ë³¼ë¦­ ë³€í™˜
            sims = []
            for s in raw:
                if s >= 0:
                    # ê±°ë¦¬ â†’ ìœ ì‚¬ë„ ë³€í™˜: similarity = 1 / (1 + distance)
                    sim = 1.0 / (1.0 + s)
                else:
                    # ìŒìˆ˜ ê±°ë¦¬ëŠ” ë¹„ì •ìƒ â†’ ë‚®ì€ ìœ ì‚¬ë„
                    sim = 0.01
                sims.append(sim)
            
            # Min-Max ì •ê·œí™”
            min_sim = min(sims)
            max_sim = max(sims)
            sim_range = max_sim - min_sim if max_sim > min_sim else 1.0
            
            normalized = []
            for sim in sims:
                if sim_range > 0:
                    norm_val = (sim - min_sim) / sim_range
                else:
                    norm_val = 0.5
                normalized.append(max(0.0, min(1.0, norm_val)))
            
            # Softmax ì ìš©
            mx = max(normalized)
            exps = [math.exp(v - mx) for v in normalized]
            Z = sum(exps) or 1.0
            probs = [min(100.0, max(0.0, 100.0 * v / Z)) for v in exps]
        
        return probs

