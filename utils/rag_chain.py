from typing import List, Dict, Any, Optional, Iterator
from langchain_ollama import OllamaLLM
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from utils.reranker import get_reranker
from utils.request_llm import RequestLLM
from utils.small_to_large_search import SmallToLargeSearch
from utils.hybrid_retriever import HybridRetriever  # Phase 4: Hybrid Search
import json
import re
import time
import logging
import statistics
import numpy as np

logger = logging.getLogger(__name__)


class RAGChain:
    def __init__(self, vectorstore,
                 llm_api_type: str = "ollama",
                 llm_base_url: str = "http://localhost:11434",
                 llm_model: str = "llama3",
                 llm_api_key: str = "",
                 temperature: float = 0.3,
                 top_k: int = 3,
                 use_reranker: bool = True,
                 reranker_model: str = "multilingual-mini",
                 reranker_initial_k: int = 20,
                 enable_synonym_expansion: bool = True,
                 enable_multi_query: bool = True,
                 multi_query_num: int = 3,
                 # Phase 4: Hybrid Search (BM25 + Vector)
                 enable_hybrid_search: bool = True,
                 hybrid_bm25_weight: float = 0.5):
        self.llm_api_type = llm_api_type
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.llm_api_key = llm_api_key
        self.temperature = temperature
        self.top_k = top_k
        self.vectorstore = vectorstore
        
        # Re-ranker ì„¤ì • (ê¸°ë³¸ í™œì„±í™”)
        self.use_reranker = use_reranker
        self.reranker_model = reranker_model
        self.reranker_initial_k = max(reranker_initial_k, top_k * 5)
        
        # Re-ranker ì´ˆê¸°í™” (ì‚¬ìš© ì‹œ)
        self.reranker = None
        if self.use_reranker:
            try:
                self.reranker = get_reranker(model_name=reranker_model)
                logger.info(f"Re-ranker ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {reranker_model}")
            except Exception as e:
                # ì—ëŸ¬ ë©”ì‹œì§€ì—ì„œ ì¤‘ë³µ ì œê±° (reranker.pyì—ì„œ ì´ë¯¸ ìƒì„¸ ë©”ì‹œì§€ ì¶œë ¥)
                error_msg = str(e)
                if "ì˜¤í”„ë¼ì¸ ëª¨ë“œì—ì„œ" in error_msg or "ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in error_msg:
                    # ì´ë¯¸ í¬ë§·ëœ ì—ëŸ¬ ë©”ì‹œì§€ì´ë¯€ë¡œ ê°„ë‹¨íˆë§Œ ë¡œê¹…
                    logger.warning(f"Re-ranker ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ({reranker_model}): ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    logger.warning(f"Re-ranker ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ({reranker_model}): {error_msg}")
                logger.warning("Re-ranker ì—†ì´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                self.use_reranker = False
                self.reranker = None
        
        # ë§ˆì§€ë§‰ ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ (ì¶œì²˜ í‘œì‹œìš©)
        self._last_retrieved_docs = []
        
        # Chat history ìºì‹œ (ë„ë©”ì¸ ê°ì§€ìš©)
        self._chat_history_cache = []
        
        # LLM ì´ˆê¸°í™” - API íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
        self.llm = self._create_llm()
        
        # ë™ì˜ì–´ í™•ì¥ ì„¤ì •
        self.enable_synonym_expansion = enable_synonym_expansion
        self.multi_query_num = max(0, multi_query_num)
        self.enable_multi_query = enable_multi_query and self.multi_query_num > 0
        
        # Small-to-Large ê²€ìƒ‰ ì´ˆê¸°í™”
        self.small_to_large_search = SmallToLargeSearch(vectorstore)

        # Phase 4: Hybrid Search (BM25 + Vector) ì´ˆê¸°í™”
        self.enable_hybrid_search = enable_hybrid_search
        self.hybrid_retriever = None
        if self.enable_hybrid_search:
            try:
                self.hybrid_retriever = HybridRetriever(
                    vector_manager=vectorstore,
                    bm25_weight=hybrid_bm25_weight
                )
                # BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
                self.hybrid_retriever.build_bm25_index()
                logger.info(f"Hybrid Search ì´ˆê¸°í™” ì™„ë£Œ (BM25: {hybrid_bm25_weight}, Vector: {1-hybrid_bm25_weight})")
            except Exception as e:
                logger.warning(f"Hybrid Search ì´ˆê¸°í™” ì‹¤íŒ¨: {e}, ê¸°ë³¸ ê²€ìƒ‰ ëª¨ë“œë¡œ ì§„í–‰")
                self.enable_hybrid_search = False
                self.hybrid_retriever = None

        # ë„ë©”ì¸ ìš©ì–´ ì‚¬ì „ (ì—”í‹°í‹° ê°ì§€ìš©)
        self._domain_lexicon = {
            "TADF", "ACRSA", "DABNA1", "HF", "OLED", "EQE",
            "FRET", "PLQY", "DMAC-TRZ", "AZB-TRZ", "Î½-DABNA"
        }
        
        # Retriever ì„¤ì • - vectorstoreëŠ” VectorStoreManager ì¸ìŠ¤í„´ìŠ¤
        self.retriever = vectorstore.vectorstore.as_retriever(
            search_kwargs={"k": max(self.top_k * 8, 24)}
        )
        
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ìƒìš© ì„œë¹„ìŠ¤ ìˆ˜ì¤€ ê°œì„ )
        self.base_prompt_template = """ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.

âš ï¸ ì¤‘ìš” ê·œì¹™:
1. **ë¬¸ì„œ ìš°ì„  ì›ì¹™**: ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ì°¾ì•„ ë‹µë³€í•˜ì„¸ìš”.
2. **ì¼ë°˜ ì§€ì‹ ê¸ˆì§€**: ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ì¸¡í•˜ê±°ë‚˜ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.
3. **ì •ë³´ ì—†ìŒ ê¸ˆì§€**: ë¬¸ì„œê°€ ì œê³µëœ ê²½ìš° "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
4. **ë¬¸ì„œ ì¸ìš© ì˜ë¬´**: ë‹µë³€í•  ë•Œ ë°˜ë“œì‹œ ë¬¸ì„œì˜ êµ¬ì²´ì  ë‚´ìš©ì„ ì¸ìš©í•˜ì„¸ìš”.

ì´ì „ ëŒ€í™” ë‚´ìš©:
{chat_history}

ì°¸ê³  ë¬¸ì„œ:
{context}

í˜„ì¬ ì§ˆë¬¸: {question}

ë‹µë³€ ì ˆì°¨ (ë‹¨ê³„ë³„ë¡œ ìƒê° ê³¼ì •ì„ ëª…ì‹œí•˜ì„¸ìš”):
1ë‹¨ê³„ [ë¬¸ì„œ ë¶„ì„]:
   - ê° ë¬¸ì„œì˜ ì£¼ì œì™€ í•µì‹¬ ë‚´ìš©ì„ íŒŒì•…í•˜ì„¸ìš”.
   - ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í‚¤ì›Œë“œë¥¼ ë¬¸ì„œì—ì„œ ì°¾ìœ¼ì„¸ìš”.
   - ë™ì˜ì–´, ì•½ì–´, ì „ë¬¸ ìš©ì–´ ë³€í˜•ë„ ê³ ë ¤í•˜ì„¸ìš”.

2ë‹¨ê³„ [ì •ë³´ ì¶”ì¶œ]:
   - ì§ˆë¬¸ì— ì§ì ‘ ë‹µí•˜ëŠ” ì •ë³´ë¥¼ ì‹ë³„í•˜ì„¸ìš”.
   - ìˆ˜ì¹˜, ë‚ ì§œ, ì´ë¦„ ë“± êµ¬ì²´ì  ì‚¬ì‹¤ì„ ì¶”ì¶œí•˜ì„¸ìš”.
   - ì—¬ëŸ¬ ë¬¸ì„œì— ê´€ë ¨ ì •ë³´ê°€ ìˆìœ¼ë©´ ëª¨ë‘ í¬í•¨í•˜ì„¸ìš”.

3ë‹¨ê³„ [ì •ë³´ í†µí•©]:
   - ì¶”ì¶œí•œ ì •ë³´ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”.
   - ê´€ë ¨ì„±ì´ ë†’ì€ ì •ë³´ë¥¼ ìš°ì„  ë°°ì¹˜í•˜ì„¸ìš”.
   - ì •ë³´ ê°„ì˜ ê´€ê³„ë‚˜ ë¹„êµê°€ í•„ìš”í•œì§€ íŒë‹¨í•˜ì„¸ìš”.

4ë‹¨ê³„ [ë‹µë³€ ìƒì„±]:
   - ë¬¸ì„œì—ì„œ í™•ì¸ëœ ì‚¬ì‹¤ë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
   - ê° ì‚¬ì‹¤ë§ˆë‹¤ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš” (ë¬¸ì„œ ë²ˆí˜¸, í˜ì´ì§€/ì„¹ì…˜).
   - ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ ì§€ì‹ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

5ë‹¨ê³„ [ê²€ì¦]:
   - ë‹µë³€ì´ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µí•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
   - ëª¨ë“  ì‚¬ì‹¤ì´ ë¬¸ì„œì— ê·¼ê±°í•˜ëŠ”ì§€ ì¬ê²€í† í•˜ì„¸ìš”.
   - ì¶œì²˜ ì •ë³´ê°€ ì¶©ë¶„í•œì§€ í™•ì¸í•˜ì„¸ìš”.

ë‹µë³€ í˜•ì‹ (ë‹¤ìŒ êµ¬ì¡°ë¥¼ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”):

## ë‹µë³€
[ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€ì„ 1-2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½]

## ìƒì„¸ ì„¤ëª…
[ë‹µë³€ì˜ ê·¼ê±°ì™€ ë°°ê²½ ì„¤ëª…]

## ì°¸ì¡° ì •ë³´
ê° ì‚¬ì‹¤ë§ˆë‹¤ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”:
- [ì‚¬ì‹¤ 1]: ë¬¸ì„œ #1, í˜ì´ì§€ X / ì„¹ì…˜ "Y"
- [ì‚¬ì‹¤ 2]: ë¬¸ì„œ #2, í˜ì´ì§€ Z / ì„¹ì…˜ "W"

## ê´€ë ¨ ì¶”ê°€ ì •ë³´
[ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆì§€ë§Œ ì§ì ‘ ë‹µë³€ì— í¬í•¨ë˜ì§€ ì•Šì€ ì¶”ê°€ ì •ë³´ (ì„ íƒì )]

ë‹µë³€:"""
        
        # ì§ˆë¬¸ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.prompt_templates = {
            "specific_info": """ë‹¹ì‹ ì€ ë¬¸ì„œì—ì„œ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì—ì„œ ì •í™•í•œ ì‚¬ì‹¤, ìˆ˜ì¹˜, ì´ë¦„, êµ¬ì¡° ë“±ì„ ì°¾ì•„ ë‹µë³€í•´ì£¼ì„¸ìš”.

âš ï¸ í•µì‹¬ ê·œì¹™:
- ì œê³µëœ ë¬¸ì„œì—ì„œë§Œ ì •ë³´ë¥¼ ì°¾ìœ¼ì„¸ìš” (ì¼ë°˜ ì§€ì‹ ì‚¬ìš© ê¸ˆì§€)
- "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- ë¬¸ì„œì— ê´€ë ¨ ì •ë³´ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì°¾ì•„ ì œì‹œí•˜ì„¸ìš”

ì´ì „ ëŒ€í™” ë‚´ìš©:
{chat_history}

ì°¸ê³  ë¬¸ì„œ:
{context}

í˜„ì¬ ì§ˆë¬¸: {question}

Few-Shot ì˜ˆì‹œ:

[ì¢‹ì€ ë‹µë³€ ì˜ˆì‹œ]
ì§ˆë¬¸: "ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ TADF ì¬ë£ŒëŠ” ë¬´ì—‡ì¸ê°€?"
ë‹µë³€: 
## ë‹µë³€
ì œê³µëœ ë¬¸ì„œì— ë”°ë¥´ë©´, ë…¼ë¬¸ì—ì„œ ACRSA (spiro-linked TADF molecule)ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ë¹„êµ ì‹¤í—˜ì„ ìœ„í•´ DABNA1ë„ ì–¸ê¸‰ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ìƒì„¸ ì„¤ëª…
ë¬¸ì„œì—ì„œ TADF ì¬ë£Œë¡œ ACRSAê°€ ëª…ì‹œì ìœ¼ë¡œ ì‚¬ìš©ë˜ì—ˆë‹¤ê³  ë‚˜ì™€ ìˆìœ¼ë©°, 'ACRSA-based device'ë¼ëŠ” í‘œí˜„ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì°¸ì¡° ì •ë³´
- ACRSA ì‚¬ìš©: ë¬¸ì„œ #1, í˜ì´ì§€ 3 / ì„¹ì…˜ "ì¬ë£Œ ë° ë°©ë²•"
- DABNA1 ë¹„êµ: ë¬¸ì„œ #1, í˜ì´ì§€ 4 / ì„¹ì…˜ "ì‹¤í—˜ ê²°ê³¼"

[ë‚˜ìœ ë‹µë³€ ì˜ˆì‹œ]
ì§ˆë¬¸: "ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ TADF ì¬ë£ŒëŠ” ë¬´ì—‡ì¸ê°€?"
ë‹µë³€: "ì£„ì†¡í•˜ì§€ë§Œ ì œê³µëœ ë¬¸ì„œì—ì„œëŠ” TADF ì¬ë£Œì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì–¸ê¸‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." (âŒ ì´ë ‡ê²Œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”)

ë‹µë³€ ì ˆì°¨:
1ë‹¨ê³„ [í‚¤ì›Œë“œ ì‹ë³„]: ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì‹ë³„í•˜ê³ , ë™ì˜ì–´ë‚˜ ì•½ì–´ë„ ê³ ë ¤í•˜ì—¬ ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•˜ì„¸ìš”.
2ë‹¨ê³„ [ì •ë³´ ê²€ìƒ‰]: ì œê³µëœ ë¬¸ì„œì˜ ëª¨ë“  ë‚´ìš©ì„ ê¼¼ê¼¼íˆ ì½ê³  ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ìœ¼ì„¸ìš”.
3ë‹¨ê³„ [ì •ë³´ ì¶”ì¶œ]: ê´€ë ¨ëœ ëª¨ë“  ì •ë³´ë¥¼ ì°¾ì•„ ë‚˜ì—´í•˜ì„¸ìš” (ì—¬ëŸ¬ ê³³ì— ìˆìœ¼ë©´ ëª¨ë‘ í¬í•¨).
4ë‹¨ê³„ [ì¶œì²˜ ëª…ì‹œ]: ê° ì •ë³´ë§ˆë‹¤ ì›ë¬¸ì„ ì¸ìš©í•˜ê³ , í˜ì´ì§€/ì„¹ì…˜ ì •ë³´ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
5ë‹¨ê³„ [ì •í™•ì„± í™•ì¸]: êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ì´ë¦„, ë‚ ì§œëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”.
6ë‹¨ê³„ [ë¶€ë¶„ ì •ë³´ ì²˜ë¦¬]: ë¶€ë¶„ì ìœ¼ë¡œë§Œ ì°¾ì€ ê²½ìš°, ì°¾ì€ ë¶€ë¶„ì„ ëª…ì‹œí•˜ê³  "ì¶”ê°€ ì •ë³´ëŠ” ë¬¸ì„œì˜ ë‹¤ë¥¸ ë¶€ë¶„ì— ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤"ë¼ê³  ì–¸ê¸‰í•˜ì„¸ìš”.

ë‹µë³€ í˜•ì‹ (êµ¬ì¡°í™”ëœ í˜•ì‹ ì‚¬ìš©):
## ë‹µë³€
[1-2ë¬¸ì¥ ìš”ì•½]

## ìƒì„¸ ì„¤ëª…
[êµ¬ì²´ì  ì‚¬ì‹¤ ë‚˜ì—´]

## ì°¸ì¡° ì •ë³´
- [ì‚¬ì‹¤ 1]: [ì¶œì²˜]
- [ì‚¬ì‹¤ 2]: [ì¶œì²˜]

ë‹µë³€:""",
            
            "summary": """ë‹¹ì‹ ì€ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ì²´ê³„ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

âš ï¸ í•µì‹¬ ê·œì¹™:
- ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš” (ì¼ë°˜ ì§€ì‹ ì¶”ê°€ ê¸ˆì§€)
- ë¬¸ì„œì˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ì¸ìš©í•˜ì—¬ ìš”ì•½í•˜ì„¸ìš”
- "ë¬¸ì„œì— ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”

ì´ì „ ëŒ€í™” ë‚´ìš©:
{chat_history}

ì°¸ê³  ë¬¸ì„œ:
{context}

í˜„ì¬ ì§ˆë¬¸: {question}

Few-Shot ì˜ˆì‹œ:

[ì¢‹ì€ ìš”ì•½ ì˜ˆì‹œ]
ì§ˆë¬¸: "ì´ ë…¼ë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”."
ë‹µë³€:
## ë‹µë³€
ì´ ë…¼ë¬¸ì€ TADF ì¬ë£Œë¥¼ ì‚¬ìš©í•œ OLED ë””ë°”ì´ìŠ¤ì˜ íš¨ìœ¨ ê°œì„ ì— ê´€í•œ ì—°êµ¬ë¡œ, ACRSA ê¸°ë°˜ ë””ë°”ì´ìŠ¤ë¥¼ í†µí•´ ë†’ì€ ë°œê´‘ íš¨ìœ¨ì„ ë‹¬ì„±í–ˆë‹¤ê³  ë³´ê³ í•©ë‹ˆë‹¤.

## ìƒì„¸ ì„¤ëª…
ë…¼ë¬¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ì£¼ìš” ë‚´ìš©ì„ í¬í•¨í•©ë‹ˆë‹¤:
1. TADF ì¬ë£Œì˜ íŠ¹ì„±ê³¼ ì¥ì 
2. ACRSA ì¬ë£Œë¥¼ ì‚¬ìš©í•œ ë””ë°”ì´ìŠ¤ ì œì‘
3. ë°œê´‘ íš¨ìœ¨ ì¸¡ì • ê²°ê³¼ ë° ë¶„ì„
4. ê¸°ì¡´ ì¬ë£Œ ëŒ€ë¹„ ì„±ëŠ¥ ë¹„êµ

## ì°¸ì¡° ì •ë³´
- TADF ì¬ë£Œ íŠ¹ì„±: ë¬¸ì„œ #1, í˜ì´ì§€ 2 / ì„¹ì…˜ "ì„œë¡ "
- ACRSA ë””ë°”ì´ìŠ¤: ë¬¸ì„œ #1, í˜ì´ì§€ 3 / ì„¹ì…˜ "ì¬ë£Œ ë° ë°©ë²•"
- íš¨ìœ¨ ì¸¡ì •: ë¬¸ì„œ #1, í˜ì´ì§€ 5 / ì„¹ì…˜ "ê²°ê³¼ ë° í† ë¡ "

ë‹µë³€ ì ˆì°¨:
1ë‹¨ê³„ [êµ¬ì¡° íŒŒì•…]: ì œê³µëœ ë¬¸ì„œì˜ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ì„¸ìš” (ì œëª©, ì„¹ì…˜, í•˜ìœ„ ì„¹ì…˜ ë“±).
2ë‹¨ê³„ [í•µì‹¬ ì¶”ì¶œ]: ê° ì„¹ì…˜ì˜ í•µì‹¬ ë‚´ìš©ì„ ë¬¸ì„œì—ì„œ ì§ì ‘ ì¶”ì¶œí•˜ì„¸ìš”.
3ë‹¨ê³„ [ë‚´ìš© ì •ë¦¬]: ì£¼ìš” ë°œê²¬, ê²°ë¡ , ìˆ˜ì¹˜ ë“±ì„ ë…¼ë¦¬ì  ìˆœì„œë¡œ ì •ë¦¬í•˜ì„¸ìš”.
4ë‹¨ê³„ [ì¶œì²˜ ëª…ì‹œ]: ëª¨ë“  ë‚´ìš©ì— ì›ë¬¸ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš” (í˜ì´ì§€/ì„¹ì…˜).
5ë‹¨ê³„ [ì¼ê´€ì„± í™•ì¸]: ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•Šì€ ë‚´ìš©ì´ í¬í•¨ë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

ë‹µë³€ í˜•ì‹:
## ë‹µë³€
[í•µì‹¬ ë‚´ìš© 1-2ë¬¸ì¥ ìš”ì•½]

## ìƒì„¸ ì„¤ëª…
1. [ì£¼ìš” ë‚´ìš© 1]
2. [ì£¼ìš” ë‚´ìš© 2]
3. [ì£¼ìš” ë‚´ìš© 3]

## ì°¸ì¡° ì •ë³´
- [ë‚´ìš© 1]: [ì¶œì²˜]
- [ë‚´ìš© 2]: [ì¶œì²˜]

ë‹µë³€:""",
            
            "comparison": """ë‹¹ì‹ ì€ ë¬¸ì„œë¥¼ ë¹„êµ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì—ì„œ ë¹„êµ ëŒ€ìƒë“¤ì„ ì°¾ì•„ ì°¨ì´ì ê³¼ íŠ¹ì§•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.

âš ï¸ í•µì‹¬ ê·œì¹™:
- ì œê³µëœ ë¬¸ì„œì—ì„œë§Œ ë¹„êµ ì •ë³´ë¥¼ ì°¾ìœ¼ì„¸ìš”
- ë¬¸ì„œì— ëª…ì‹œëœ ë¹„êµ ë‚´ìš©ì„ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”
- ì¼ë°˜ì ì¸ ë¹„êµ ì§€ì‹ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”

ì´ì „ ëŒ€í™” ë‚´ìš©:
{chat_history}

ì°¸ê³  ë¬¸ì„œ:
{context}

í˜„ì¬ ì§ˆë¬¸: {question}

Few-Shot ì˜ˆì‹œ:

[ì¢‹ì€ ë¹„êµ ì˜ˆì‹œ]
ì§ˆë¬¸: "ACRSAì™€ DABNA1ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€?"
ë‹µë³€:
## ë‹µë³€
ACRSAì™€ DABNA1ì€ ë‘˜ ë‹¤ TADF ì¬ë£Œì´ì§€ë§Œ, ACRSAëŠ” spiro-linked êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆê³ , DABNA1ì€ ë‹¤ë¥¸ ë¶„ì êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ë°œê´‘ íš¨ìœ¨ ì¸¡ë©´ì—ì„œë„ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤.

## ìƒì„¸ ì„¤ëª…
ë¬¸ì„œì— ë”°ë¥´ë©´:
- **ACRSA**: spiro-linked TADF molecule êµ¬ì¡°, ë†’ì€ ë°œê´‘ íš¨ìœ¨ ë‹¬ì„±
- **DABNA1**: ë‹¤ë¥¸ ë¶„ì êµ¬ì¡°, ë¹„êµ ì‹¤í—˜ì—ì„œ ì‚¬ìš©

## ì°¸ì¡° ì •ë³´
- ACRSA êµ¬ì¡°: ë¬¸ì„œ #1, í˜ì´ì§€ 3 / ì„¹ì…˜ "ì¬ë£Œ íŠ¹ì„±"
- DABNA1 ë¹„êµ: ë¬¸ì„œ #1, í˜ì´ì§€ 4 / ì„¹ì…˜ "ê²°ê³¼ ë¹„êµ"

ë‹µë³€ ì ˆì°¨:
1ë‹¨ê³„ [ëŒ€ìƒ ì‹ë³„]: ë¬¸ì„œì—ì„œ ë¹„êµ ëŒ€ìƒë“¤ì„ ì‹ë³„í•˜ì„¸ìš” (ëª…ì‹œì ìœ¼ë¡œ ë¹„êµëœ í•­ëª©ë“¤).
2ë‹¨ê³„ [íŠ¹ì§• ì¶”ì¶œ]: ê° ëŒ€ìƒì˜ íŠ¹ì§•ì„ ë¬¸ì„œì—ì„œ ì§ì ‘ ì¶”ì¶œí•˜ì—¬ ë‚˜ì—´í•˜ì„¸ìš”.
3ë‹¨ê³„ [ì°¨ì´ì  ë¶„ì„]: ë¬¸ì„œì—ì„œ ì–¸ê¸‰ëœ ì°¨ì´ì ê³¼ ê³µí†µì ì„ ì •í™•íˆ ì¸ìš©í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”.
4ë‹¨ê³„ [ìˆ˜ì¹˜ ë¹„êµ]: ìˆ˜ì¹˜ì  ë¹„êµê°€ ë¬¸ì„œì— ìˆìœ¼ë©´ í•´ë‹¹ ìˆ˜ì¹˜ë¥¼ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ì„¸ìš”.
5ë‹¨ê³„ [ì •ë³´ ë³´ì™„]: ì§ì ‘ì ì¸ ë¹„êµ ì •ë³´ê°€ ì—†ìœ¼ë©´, ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ ì œì‹œí•˜ì„¸ìš”.

ë‹µë³€ í˜•ì‹:
## ë‹µë³€
[ë¹„êµ ëŒ€ìƒê³¼ ì£¼ìš” ì°¨ì´ì  ìš”ì•½]

## ìƒì„¸ ì„¤ëª…
**ë¹„êµ ëŒ€ìƒ 1**: [íŠ¹ì§•]
**ë¹„êµ ëŒ€ìƒ 2**: [íŠ¹ì§•]
**ì£¼ìš” ì°¨ì´ì **: [ì°¨ì´ì ]
**ê³µí†µì **: [ê³µí†µì ]

## ì°¸ì¡° ì •ë³´
- [ë¹„êµ ë‚´ìš© 1]: [ì¶œì²˜]
- [ë¹„êµ ë‚´ìš© 2]: [ì¶œì²˜]

ë‹µë³€:""",
            
            "relationship": """ë‹¹ì‹ ì€ ë¬¸ì„œì—ì„œ ê´€ê³„ì™€ ì¸ê³¼ê´€ê³„ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì—ì„œ ìš”ì†Œë“¤ ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬ ì„¤ëª…í•´ì£¼ì„¸ìš”.

âš ï¸ í•µì‹¬ ê·œì¹™:
- ì œê³µëœ ë¬¸ì„œì—ì„œë§Œ ê´€ê³„ ì •ë³´ë¥¼ ì°¾ìœ¼ì„¸ìš”
- ë¬¸ì„œì— ëª…ì‹œëœ ê´€ê³„ë¥¼ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”
- ì¼ë°˜ì ì¸ ì›ë¦¬ë‚˜ ì¶”ë¡ ì€ ë¬¸ì„œ ê·¼ê±° ì—†ì´ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”

ì´ì „ ëŒ€í™” ë‚´ìš©:
{chat_history}

ì°¸ê³  ë¬¸ì„œ:
{context}

í˜„ì¬ ì§ˆë¬¸: {question}

Few-Shot ì˜ˆì‹œ:

[ì¢‹ì€ ê´€ê³„ ë¶„ì„ ì˜ˆì‹œ]
ì§ˆë¬¸: "TADF ì¬ë£Œì˜ êµ¬ì¡°ê°€ ë°œê´‘ íš¨ìœ¨ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€?"
ë‹µë³€:
## ë‹µë³€
ë¬¸ì„œì— ë”°ë¥´ë©´, TADF ì¬ë£Œì˜ spiro-linked êµ¬ì¡°ëŠ” ë¶„ì ê°„ ìƒí˜¸ì‘ìš©ì„ ìµœì†Œí™”í•˜ì—¬ ë°œê´‘ íš¨ìœ¨ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

## ìƒì„¸ ì„¤ëª…
ë¬¸ì„œì—ì„œ í™•ì¸ëœ ê´€ê³„:
1. **êµ¬ì¡°ì  íŠ¹ì„±**: spiro-linked êµ¬ì¡° â†’ ë¶„ì ê°„ ìƒí˜¸ì‘ìš© ê°ì†Œ
2. **íš¨ìœ¨ í–¥ìƒ**: ìƒí˜¸ì‘ìš© ê°ì†Œ â†’ ë†’ì€ ë°œê´‘ íš¨ìœ¨ ë‹¬ì„±
3. **ë©”ì»¤ë‹ˆì¦˜**: TADF ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•œ ì—ë„ˆì§€ ì „ë‹¬ ìµœì í™”

## ì°¸ì¡° ì •ë³´
- êµ¬ì¡°-íš¨ìœ¨ ê´€ê³„: ë¬¸ì„œ #1, í˜ì´ì§€ 3 / ì„¹ì…˜ "ì¬ë£Œ ì„¤ê³„"
- ë©”ì»¤ë‹ˆì¦˜ ì„¤ëª…: ë¬¸ì„œ #1, í˜ì´ì§€ 4 / ì„¹ì…˜ "ì‘ë™ ì›ë¦¬"

ë‹µë³€ ì ˆì°¨:
1ë‹¨ê³„ [ìš”ì†Œ ì‹ë³„]: ë¬¸ì„œì—ì„œ ê´€ë ¨ëœ ìš”ì†Œë“¤ì„ ì‹ë³„í•˜ì„¸ìš”.
2ë‹¨ê³„ [ê´€ê³„ ì°¾ê¸°]: ë¬¸ì„œì—ì„œ ëª…ì‹œëœ ìš”ì†Œë“¤ ê°„ì˜ ê´€ê³„, ì˜í–¥, ë©”ì»¤ë‹ˆì¦˜ì„ ì°¾ì•„ ì¸ìš©í•˜ì„¸ìš”.
3ë‹¨ê³„ [ì¸ê³¼ê´€ê³„ ë¶„ì„]: ë¬¸ì„œì—ì„œ ì–¸ê¸‰ëœ ì¸ê³¼ê´€ê³„ë‚˜ ìƒê´€ê´€ê³„ë¥¼ ì •í™•íˆ ì„¤ëª…í•˜ì„¸ìš”.
4ë‹¨ê³„ [íŒ¨í„´ íŒŒì•…]: ë¬¸ì„œì—ì„œ ë°œê²¬ëœ ê²½í–¥ì„±ì´ë‚˜ íŒ¨í„´ì„ ì›ë¬¸ì„ ì¸ìš©í•˜ë©° ì„¤ëª…í•˜ì„¸ìš”.
5ë‹¨ê³„ [ì •ë³´ ë³´ì™„]: ì§ì ‘ì ì¸ ê´€ê³„ ì •ë³´ê°€ ì—†ìœ¼ë©´, ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ ì œì‹œí•˜ë˜, ì¼ë°˜ì ì¸ ì¶”ë¡ ì€ ìµœì†Œí™”í•˜ì„¸ìš”.

ë‹µë³€ í˜•ì‹:
## ë‹µë³€
[ìš”ì†Œ ê°„ ê´€ê³„ ìš”ì•½]

## ìƒì„¸ ì„¤ëª…
**ìš”ì†Œ 1**: [ì„¤ëª…]
**ìš”ì†Œ 2**: [ì„¤ëª…]
**ê´€ê³„/ë©”ì»¤ë‹ˆì¦˜**: [ê´€ê³„ ì„¤ëª…]
**ì˜í–¥/ê²°ê³¼**: [ì˜í–¥ ì„¤ëª…]

## ì°¸ì¡° ì •ë³´
- [ê´€ê³„ ì„¤ëª… 1]: [ì¶œì²˜]
- [ê´€ê³„ ì„¤ëª… 2]: [ì¶œì²˜]

ë‹µë³€:""",
            
            "general": self.base_prompt_template
        }
        
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ (ë‚˜ì¤‘ì— ì§ˆë¬¸ íƒ€ì…ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì„ íƒ)
        self.prompt_template = self.base_prompt_template
        
        self.prompt = PromptTemplate(
            template=self.prompt_template,
            input_variables=["chat_history", "context", "question"]
        )
        
        # LCEL ë°©ì‹ìœ¼ë¡œ ì²´ì¸ êµ¬ì„± (ëŒ€í™” ì´ë ¥ í¬í•¨)
        self.chain = (
            {
                "context": lambda x: self._get_context(x["question"]),
                "chat_history": lambda x: x.get("chat_history", "ì´ì „ ëŒ€í™” ì—†ìŒ"),
                "question": lambda x: x["question"]
            }
            | self.prompt
            | self.llm
            | StrOutputParser()
        )

    def _create_llm(self):
        """API íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
        if self.llm_api_type == "request":
            return RequestLLM(
                base_url=self.llm_base_url,
                model=self.llm_model,
                temperature=self.temperature,
                timeout=60
            )
        elif self.llm_api_type == "ollama":
            return OllamaLLM(
                base_url=self.llm_base_url,
                model=self.llm_model,
                temperature=self.temperature
            )
        elif self.llm_api_type == "openai":
            kwargs = {
                "model": self.llm_model,
                "temperature": self.temperature,
                "api_key": self.llm_api_key if self.llm_api_key else "not-needed"
            }
            return ChatOpenAI(**kwargs)
        elif self.llm_api_type == "openai-compatible":
            kwargs = {
                "model": self.llm_model,
                "temperature": self.temperature,
                "base_url": self.llm_base_url,
                "api_key": self.llm_api_key if self.llm_api_key else "not-needed"
            }
            return ChatOpenAI(**kwargs)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” API íƒ€ì…: {self.llm_api_type}")

    def _format_docs(self, docs: List[Document]) -> str:
        """ë¬¸ì„œë¥¼ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ… (ìƒìš© ì„œë¹„ìŠ¤ ìˆ˜ì¤€ ê°œì„ )"""
        formatted_sections = []
        
        for i, doc in enumerate(docs, 1):
            metadata = doc.metadata or {}
            file_name = metadata.get('file_name', 'Unknown')
            page_number = metadata.get('page_number', 'Unknown')
            chunk_type = metadata.get('chunk_type', 'unknown')
            section_title = metadata.get('section_title', '')
            
            # ë¬¸ì„œ ë²ˆí˜¸ì™€ ë©”íƒ€ë°ì´í„°
            header = f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            header += f"ğŸ“„ ë¬¸ì„œ #{i}\n"
            header += f"   íŒŒì¼ëª…: {file_name}\n"
            header += f"   í˜ì´ì§€: {page_number}\n"
            if chunk_type != 'unknown':
                header += f"   ì²­í¬ íƒ€ì…: {chunk_type}\n"
            if section_title:
                header += f"   ì„¹ì…˜: {section_title}\n"
            header += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
            
            # ë¬¸ì„œ ë‚´ìš©
            content = doc.page_content.strip()
            
            formatted_sections.append(header + content)
        
        return "\n\n".join(formatted_sections)

    def _unique_by_file(self, pairs: List[tuple], k: int) -> List[tuple]:
        """(Document, score) ë¦¬ìŠ¤íŠ¸ì—ì„œ íŒŒì¼ëª… ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µì„ ì œê±°í•˜ë©° ìµœëŒ€ kê°œ ë°˜í™˜
        ê°œì„ : PPTXëŠ” ìŠ¬ë¼ì´ë“œ ë‹¨ìœ„, PDFëŠ” í˜ì´ì§€ ë‹¨ìœ„ë¡œ ì¤‘ë³µ ì œê±°"""
        seen = set()
        results: List[tuple] = []
        file_chunk_counts = {}  # íŒŒì¼ë³„ ì²­í¬ ê°œìˆ˜ ì¶”ì 
        
        for doc, score in pairs:
            file_name = doc.metadata.get("file_name", "")
            chunk_type = doc.metadata.get("chunk_type", "")
            
            # PPTX: ìŠ¬ë¼ì´ë“œ ë‹¨ìœ„ ì¤‘ë³µ ì œê±°, PDF: í˜ì´ì§€ ë‹¨ìœ„ ì¤‘ë³µ ì œê±°
            # slide_number ë˜ëŠ” page_numberê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ file_nameë§Œ ì‚¬ìš©
            slide_number = doc.metadata.get("slide_number")
            page_number = doc.metadata.get("page_number")
            
            if slide_number is not None:
                # PPTX: file_name + slide_number ì¡°í•©ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
                key = f"{file_name}_slide_{slide_number}"
            elif page_number is not None:
                # PDF: file_name + page_number ì¡°í•©ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
                key = f"{file_name}_page_{page_number}"
            else:
                # ë©”íƒ€ë°ì´í„°ê°€ ì—†ìœ¼ë©´ íŒŒì¼ëª…ë§Œ ì‚¬ìš© (ê¸°ì¡´ ë°©ì‹)
                key = file_name
            
            # íŒŒì¼ë‹¹ ìµœëŒ€ ì²­í¬ ìˆ˜ ì œí•œ (ë„ˆë¬´ ë§ì€ ì²­í¬ ë°©ì§€)
            # ëª©ë¡ ë‚˜ì—´ ì§ˆë¬¸ì˜ ê²½ìš° ë” ë§ì´ í—ˆìš©
            max_per_file = 10  # ê¸°ë³¸ê°’: íŒŒì¼ë‹¹ ìµœëŒ€ 10ê°œ ì²­í¬
            
            if key in seen:
                # ì´ë¯¸ ë³¸ ì¡°í•©ì´ë©´ ê±´ë„ˆë›°ê¸° (ë™ì¼ ìŠ¬ë¼ì´ë“œ/í˜ì´ì§€ëŠ” 1ê°œë§Œ)
                continue
            
            # íŒŒì¼ë‹¹ ì²­í¬ ê°œìˆ˜ ì²´í¬
            if file_name in file_chunk_counts:
                if file_chunk_counts[file_name] >= max_per_file:
                    continue
                file_chunk_counts[file_name] = 1
            else:
                file_chunk_counts[file_name] = 1
            
            seen.add(key)
            results.append((doc, score))
            if len(results) >= k:
                break
        return results

    def _search_candidates(self, question: str) -> List[tuple]:
        """í•˜ì´ë¸Œë¦¬ë“œ(í‚¤ì›Œë“œ+ë²¡í„°) â†’ Re-ranker ì…ë ¥ í›„ë³´ í™•ë³´ (Phase 3: ì—”í‹°í‹° boost, Phase 4: BM25+Vector)"""
        try:
            # Phase 4: Hybrid Search (BM25 + Vector) ì‚¬ìš©
            if self.enable_hybrid_search and self.hybrid_retriever:
                initial_k = max(self.reranker_initial_k, max(self.top_k * 8, 60))
                print(f"ğŸ” [Phase 4] Hybrid Search (BM25+Vector) ì‚¬ìš© (top_k={initial_k})")

                # HybridRetriever.search() ê²°ê³¼: List[(doc_dict, score)]
                hybrid_results = self.hybrid_retriever.search(question, top_k=initial_k)

                # Convert to (Document, score) format
                hybrid = []
                for doc_dict, score in hybrid_results:
                    # doc_dictëŠ” {'id', 'content', 'metadata'} í˜•ì‹
                    doc = Document(
                        page_content=doc_dict['content'],
                        metadata=doc_dict['metadata']
                    )
                    hybrid.append((doc, score))
            else:
                # ê¸°ì¡´ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (vectorstoreì˜ ë©”ì„œë“œ)
                initial_k = max(self.reranker_initial_k, max(self.top_k * 8, 60))
                hybrid = self.vectorstore.similarity_search_hybrid(
                    question, initial_k=initial_k, top_k=initial_k
                )

            # Phase 3: ì—”í‹°í‹° ë§¤ì¹­ ì²­í¬ì— boost ì ìš©
            if hasattr(self.vectorstore, 'entity_index') and self.vectorstore.entity_index:
                hybrid = self._apply_entity_boost(question, hybrid)

            return hybrid
        except Exception as e:
            print(f"âš ï¸ Hybrid Search ì˜¤ë¥˜: {e}, í´ë°± ëª¨ë“œë¡œ ì „í™˜")
            # í´ë°±: ë²¡í„° ê²€ìƒ‰
            return self.vectorstore.similarity_search_with_score(question, k=max(self.reranker_initial_k, 60))
    
    def _apply_entity_boost(self, question: str, candidates: List[tuple], boost_factor: float = 1.5) -> List[tuple]:
        """ì—”í‹°í‹° ë§¤ì¹­ ì²­í¬ì— boost ì ìˆ˜ ì ìš© (Phase 3)"""
        # ì¿¼ë¦¬ì—ì„œ ì—”í‹°í‹° ê°ì§€ (ë„ë©”ì¸ ìš©ì–´ ì‚¬ì „ í™œìš©)
        detected_entities = []
        question_lower = question.lower()
        
        # ë„ë©”ì¸ ìš©ì–´ ì‚¬ì „ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ
        for key in self._domain_lexicon.keys():
            if key.lower() in question_lower:
                detected_entities.append(key)
        
        # ê°ì§€ëœ ì—”í‹°í‹°ê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if not detected_entities:
            return candidates
        
        # ì—”í‹°í‹° ë§¤ì¹­ ì²­í¬ ID ìˆ˜ì§‘
        matching_chunk_ids = set()
        for entity in detected_entities:
            chunk_ids = self.vectorstore.search_by_entity(entity)
            matching_chunk_ids.update(chunk_ids)
        
        if not matching_chunk_ids:
            return candidates
        
        # ë§¤ì¹­ë˜ëŠ” ì²­í¬ì— boost ì ìš©
        boosted_candidates = []
        boost_count = 0
        for doc, score in candidates:
            chunk_id = doc.metadata.get('chunk_id') or doc.metadata.get('id')
            if chunk_id in matching_chunk_ids:
                boosted_score = score * boost_factor
                boosted_candidates.append((doc, boosted_score))
                boost_count += 1
            else:
                boosted_candidates.append((doc, score))
        
        if boost_count > 0:
            print(f"âœ¨ ì—”í‹°í‹° boost ì ìš©: {boost_count}ê°œ ì²­í¬ (ê°ì§€ëœ ì—”í‹°í‹°: {', '.join(detected_entities)})")
        
        return boosted_candidates

    def rerank_documents(self, query: str, docs: List[tuple]) -> List[tuple]:
        """Re-rankerë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ì¬ìˆœìœ„í™”

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            docs: (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸

        Returns:
            Re-rankingëœ (Document, rerank_score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.use_reranker or not self.reranker:
            print("[INFO] Re-rankerê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆê±°ë‚˜ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì›ë³¸ ë°˜í™˜.")
            return docs

        if not docs:
            return docs

        try:
            # Re-ranker ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            docs_for_rerank = [{
                "document": doc,
                "chunk_id": idx,
                "raw_score": score
            } for idx, (doc, score) in enumerate(docs)]

            # Re-ranking ìˆ˜í–‰
            reranked = self.reranker.rerank(query, docs_for_rerank, top_k=len(docs_for_rerank))

            # ê²°ê³¼ë¥¼ (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            pairs = [(d["document"], d.get("rerank_score", 0.0)) for d in reranked]

            print(f"[Re-ranker] {len(docs)}ê°œ ë¬¸ì„œ ì¬ìˆœìœ„í™” ì™„ë£Œ")
            return pairs

        except Exception as e:
            print(f"[WARN] Re-ranking ì˜¤ë¥˜: {e}, ì›ë³¸ ë°˜í™˜")
            return docs

    def _semantic_similarity_filter(self, query: str, candidates: List[tuple], threshold: float = 0.5) -> List[tuple]:
        """ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ ê¸°ë°˜ í•„í„°ë§ (Solution #1)

        ì¿¼ë¦¬ì™€ ê° ë¬¸ì„œì˜ ì„ë² ë”© ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ threshold ì´í•˜ ë¬¸ì„œ ì œê±°

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            candidates: (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            threshold: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ (0~1, ê¸°ë³¸ê°’ 0.5)

        Returns:
            í•„í„°ë§ëœ (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if not candidates or len(candidates) < 2:
            return candidates

        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.vectorstore.embeddings.embed_query(query)

            filtered = []
            removed_count = 0

            for doc, score in candidates:
                # ë¬¸ì„œ ì„ë² ë”© ìƒì„±
                doc_embedding = self.vectorstore.embeddings.embed_query(doc.page_content)

                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = np.dot(query_embedding, doc_embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)
                )

                if similarity >= threshold:
                    filtered.append((doc, score))
                else:
                    removed_count += 1

            # í•„í„°ë§ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìœ¼ë©´ threshold ì™„í™”
            if len(filtered) < max(2, len(candidates) // 3):
                print(f"[WARN] Semantic í•„í„°ë§ ê²°ê³¼ ë¶€ì¡±, threshold ì™„í™” ({threshold} -> {threshold * 0.7})")
                return self._semantic_similarity_filter(query, candidates, threshold * 0.7)

            if removed_count > 0:
                print(f"[SEMANTIC] ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ í•„í„°ë§: {removed_count}ê°œ ë¬¸ì„œ ì œê±° (threshold={threshold:.2f})")

            return filtered

        except Exception as e:
            print(f"[WARN] Semantic í•„í„°ë§ ì˜¤ë¥˜: {e}, ì›ë³¸ ë°˜í™˜")
            return candidates

    def _keyword_based_filter(self, query: str, candidates: List[tuple], min_overlap: float = 0.2) -> List[tuple]:
        """í‚¤ì›Œë“œ ì¤‘ë³µë„ ê¸°ë°˜ í•„í„°ë§ (Solution #2)

        ì¿¼ë¦¬ì™€ ë¬¸ì„œì˜ í‚¤ì›Œë“œ ì¤‘ë³µë„ë¥¼ ì¸¡ì •í•˜ì—¬ min_overlap ì´í•˜ ë¬¸ì„œ ì œê±°

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            candidates: (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            min_overlap: ìµœì†Œ í‚¤ì›Œë“œ ì¤‘ë³µë„ (0~1, ê¸°ë³¸ê°’ 0.2)

        Returns:
            í•„í„°ë§ëœ (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if not candidates or len(candidates) < 2:
            return candidates

        try:
            # ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ í† í°í™”)
            query_keywords = set(query.lower().split())
            # ë¶ˆìš©ì–´ ì œê±° (ê°„ë‹¨í•œ ì˜ì–´ ë¶ˆìš©ì–´)
            stopwords = {'a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'is', 'are', 'was', 'were'}
            query_keywords = query_keywords - stopwords

            if len(query_keywords) == 0:
                return candidates

            filtered = []
            removed_count = 0

            for doc, score in candidates:
                # ë¬¸ì„œì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                doc_text = doc.page_content.lower()
                doc_keywords = set(doc_text.split()) - stopwords

                # Jaccard ìœ ì‚¬ë„ ê³„ì‚° (êµì§‘í•© / í•©ì§‘í•©)
                if len(doc_keywords) == 0:
                    overlap = 0
                else:
                    intersection = query_keywords & doc_keywords
                    union = query_keywords | doc_keywords
                    overlap = len(intersection) / len(union) if len(union) > 0 else 0

                if overlap >= min_overlap:
                    filtered.append((doc, score))
                else:
                    removed_count += 1

            # í•„í„°ë§ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìœ¼ë©´ threshold ì™„í™”
            if len(filtered) < max(2, len(candidates) // 3):
                print(f"[WARN] Keyword í•„í„°ë§ ê²°ê³¼ ë¶€ì¡±, threshold ì™„í™” ({min_overlap} -> {min_overlap * 0.5})")
                return self._keyword_based_filter(query, candidates, min_overlap * 0.5)

            if removed_count > 0:
                print(f"[KEYWORD] í‚¤ì›Œë“œ ì¤‘ë³µë„ í•„í„°ë§: {removed_count}ê°œ ë¬¸ì„œ ì œê±° (min_overlap={min_overlap:.2f})")

            return filtered

        except Exception as e:
            print(f"[WARN] Keyword í•„í„°ë§ ì˜¤ë¥˜: {e}, ì›ë³¸ ë°˜í™˜")
            return candidates

    def _statistical_outlier_removal(self, candidates: List[tuple], method: str = 'mad', mad_threshold: float = 3.0) -> List[tuple]:
        """í†µê³„ ê¸°ë°˜ ì´ìƒì¹˜ ì œê±° (ê°œì„ ì•ˆ 3)

        Args:
            candidates: (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            method: 'mad' (Median Absolute Deviation) ë˜ëŠ” 'iqr' (Interquartile Range) ë˜ëŠ” 'zscore'
            mad_threshold: MAD ë°©ì‹ì—ì„œ ì‚¬ìš©í•  threshold ë°°ìˆ˜ (ê¸°ë³¸ê°’: 3.0)

        Returns:
            í•„í„°ë§ëœ (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if not candidates or len(candidates) < 3:
            return candidates

        try:
            scores = [float(score) for _, score in candidates]

            if method == 'mad':
                # MAD (Median Absolute Deviation) - ê°€ì¥ ê²¬ê³ í•œ ë°©ë²•
                median = np.median(scores)
                mad = np.median([abs(s - median) for s in scores])

                # MADê°€ 0ì´ë©´ ëª¨ë“  ê°’ì´ ë™ì¼ (í•„í„°ë§ ë¶ˆí•„ìš”)
                if mad < 1e-9:
                    return candidates

                # ì¤‘ì•™ê°’ì—ì„œ mad_threshold * MAD ì´ìƒ ë–¨ì–´ì§„ ê²ƒ ì œê±°
                threshold = median - mad_threshold * mad
                filtered = [(doc, s) for doc, s in candidates if s >= threshold]

            elif method == 'iqr':
                # IQR (Interquartile Range)
                q1 = np.percentile(scores, 25)
                q3 = np.percentile(scores, 75)
                iqr = q3 - q1

                if iqr < 1e-9:
                    return candidates

                lower_bound = q1 - 1.5 * iqr
                upper_bound = q3 + 1.5 * iqr
                filtered = [(doc, s) for doc, s in candidates if lower_bound <= s <= upper_bound]

            elif method == 'zscore':
                # Z-score
                mean = np.mean(scores)
                std = np.std(scores)

                if std < 1e-9:
                    return candidates

                # Z-scoreê°€ 2 ì´ë‚´ì¸ ê²ƒë§Œ ì„ íƒ
                filtered = [(doc, s) for doc, s in candidates if abs((s - mean) / std) < 2]

            else:
                return candidates

            # í•„í„°ë§ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì›ë³¸ ë°˜í™˜ (ìµœì†Œ 3ê°œ ë˜ëŠ” ì›ë³¸ì˜ 50%)
            min_required = max(3, len(candidates) // 2)
            if len(filtered) < min_required:
                print(f"[WARN] í†µê³„ í•„í„°ë§ ê²°ê³¼ ë¶€ì¡± ({len(filtered)}ê°œ), ì›ë³¸ ìœ ì§€")
                return candidates

            removed_count = len(candidates) - len(filtered)
            if removed_count > 0:
                print(f"[STAT] í†µê³„ ê¸°ë°˜ ì´ìƒì¹˜ ì œê±°: {removed_count}ê°œ ë¬¸ì„œ í•„í„°ë§ ({method.upper()} ë°©ì‹)")

            return filtered

        except Exception as e:
            print(f"[WARN] í†µê³„ í•„í„°ë§ ì˜¤ë¥˜: {e}, ì›ë³¸ ë°˜í™˜")
            return candidates

    def _reranker_gap_based_cutoff(self, candidates: List[tuple], min_docs: int = 3, gap_threshold_multiplier: float = 2.0) -> List[tuple]:
        """Re-ranker ì ìˆ˜ Gap ê¸°ë°˜ ë™ì  ì»·ì˜¤í”„ (ê°œì„ ì•ˆ 5)

        ì£¼ì œê°€ ë‹¤ë¥¸ ë¬¸ì„œëŠ” ì ìˆ˜ ì°¨ì´ê°€ í¬ê²Œ ë‚˜íƒ€ë‚˜ëŠ” íŠ¹ì„±ì„ ì´ìš©í•˜ì—¬
        ê°€ì¥ í° ì ìˆ˜ gapì´ ë‚˜íƒ€ë‚˜ëŠ” ì§€ì ì—ì„œ ìë™ìœ¼ë¡œ ì»·ì˜¤í”„

        Args:
            candidates: (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ (ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ ê°€ì •)
            min_docs: ìµœì†Œ ë°˜í™˜ ë¬¸ì„œ ìˆ˜
            gap_threshold_multiplier: Gap threshold ë°°ìˆ˜ (ê¸°ë³¸ê°’: 2.0, ë‚®ì„ìˆ˜ë¡ ë” ì—„ê²©í•˜ê²Œ í•„í„°ë§)

        Returns:
            í•„í„°ë§ëœ (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if not candidates or len(candidates) <= min_docs:
            return candidates

        try:
            scores = [float(score) for _, score in candidates]

            # ì ìˆ˜ ì°¨ì´(gap) ê³„ì‚°
            gaps = [scores[i] - scores[i+1] for i in range(len(scores)-1)]

            if not gaps:
                return candidates

            # ê°€ì¥ í° gap ì°¾ê¸°
            max_gap = max(gaps)
            max_gap_idx = gaps.index(max_gap)

            # Gapì˜ í†µê³„ ë¶„ì„
            mean_gap = statistics.mean(gaps)

            # Gapì´ ì¶©ë¶„íˆ í° ê²½ìš°ì—ë§Œ ì»·ì˜¤í”„ ì ìš©
            # ì¡°ê±´: Gapì´ í‰ê· ì˜ gap_threshold_multiplierë°° ì´ìƒ && ì»·ì˜¤í”„ ìœ„ì¹˜ê°€ min_docs ì´ìƒ
            if max_gap > mean_gap * gap_threshold_multiplier and max_gap_idx >= min_docs - 1:
                cutoff = max_gap_idx + 1
                filtered = candidates[:cutoff]

                removed_count = len(candidates) - cutoff
                print(f"[CUT] Re-ranker Gap ê¸°ë°˜ ì»·ì˜¤í”„: {removed_count}ê°œ ë¬¸ì„œ í•„í„°ë§")
                print(f"   - Gap ìœ„ì¹˜: {cutoff}ë²ˆì§¸ ë¬¸ì„œ (ìµœëŒ€ Gap: {max_gap:.4f}, í‰ê·  Gap: {mean_gap:.4f})")

                return filtered

            # Gapì´ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ì›ë³¸ ë°˜í™˜
            return candidates

        except Exception as e:
            print(f"[WARN] Gap ê¸°ë°˜ í•„í„°ë§ ì˜¤ë¥˜: {e}, ì›ë³¸ ë°˜í™˜")
            return candidates

    def _detect_query_type(self, question: str) -> str:
        """ì¿¼ë¦¬ íƒ€ì… ê°ì§€ (êµ¬ì²´ì  ì •ë³´ ì¶”ì¶œ, ìš”ì•½, ë¹„êµ, ê´€ê³„ ë¶„ì„ ë“±)"""
        question_lower = question.lower()

        # êµ¬ì²´ì  ì •ë³´ ì¶”ì¶œ í‚¤ì›Œë“œ
        specific_keywords = ["ë¬´ì—‡ì¸ê°€", "ì–¼ë§ˆì¸ê°€", "ëˆ„êµ¬ì¸ê°€", "ì–¸ì œ", "ì–´ë””",
                           "ì–´ë–¤", "ë‚˜ì—´", "ì¶”ì¶œ", "ìˆ˜ì¹˜", "ê°’", "ì´ë¦„", "êµ¬ì¡°"]
        if any(keyword in question_lower for keyword in specific_keywords):
            return "specific_info"

        # ìš”ì•½ í‚¤ì›Œë“œ
        summary_keywords = ["ìš”ì•½", "ì •ë¦¬", "í•µì‹¬", "ì£¼ìš” ë‚´ìš©", "ê°œìš”", "ê°œìš”"]
        if any(keyword in question_lower for keyword in summary_keywords):
            return "summary"

        # ë¹„êµ ë¶„ì„ í‚¤ì›Œë“œ
        comparison_keywords = ["ë¹„êµ", "ì°¨ì´", "ëŒ€ë¹„", "ì–´ëŠ ê²ƒì´", "vs", "versus"]
        if any(keyword in question_lower for keyword in comparison_keywords):
            return "comparison"

        # ê´€ê³„ ë¶„ì„ í‚¤ì›Œë“œ
        relationship_keywords = ["ê´€ê³„", "ìƒê´€ê´€ê³„", "ê²½í–¥", "ì˜í–¥", "ë©”ì»¤ë‹ˆì¦˜", "ì›ì¸"]
        if any(keyword in question_lower for keyword in relationship_keywords):
            return "relationship"

        # ê¸°ë³¸ê°’
        return "general"

    def _detect_question_category(self, question: str) -> List[str]:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ê°ì§€

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ (technical/business/hr/safety/reference)
            ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ê°€ ê´€ë ¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        """
        # Few-shot í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì´ ì–´ë–¤ ì¹´í…Œê³ ë¦¬ì˜ ë¬¸ì„œë¥¼ í•„ìš”ë¡œ í•˜ëŠ”ì§€ ë¶„ì„í•˜ì„¸ìš”.

**ì¹´í…Œê³ ë¦¬ ì •ì˜:**
- technical: ê³¼í•™, ê¸°ìˆ , ì—°êµ¬, OLED, ë””ìŠ¤í”Œë ˆì´, ê³µí•™, í•™ìˆ  ë‚´ìš©
- business: ì‚¬ì—…, ë‰´ìŠ¤, ì œí’ˆ ë°œí‘œ, ë§ˆì¼€íŒ…, ì‹œì¥ ë¶„ì„
- hr: ì¸ì‚¬, êµìœ¡, ì¶œê²° ê´€ë¦¬, ì§ì› ê´€ë¦¬
- safety: ì•ˆì „, ê·œì •, ìœ„í—˜ ê´€ë¦¬, ë³´ê±´
- reference: ì¼ë°˜ ì°¸ê³  ìë£Œ

**ë¶„ë¥˜ ì˜ˆì‹œ:**
1. ì§ˆë¬¸: "TADF ì¬ë£Œì˜ ì–‘ì íš¨ìœ¨ì€?"
   ì¹´í…Œê³ ë¦¬: technical

2. ì§ˆë¬¸: "LGë””ìŠ¤í”Œë ˆì´ì˜ ì‹ ì œí’ˆ ì¶œì‹œì¼ì€?"
   ì¹´í…Œê³ ë¦¬: business

3. ì§ˆë¬¸: "ì¶œê²° ì‹œìŠ¤í…œ ë¡œê·¸ì¸ ë°©ë²•ì€?"
   ì¹´í…Œê³ ë¦¬: hr

4. ì§ˆë¬¸: "ì‘ì—…ì¥ ì•ˆì „ ìˆ˜ì¹™ì€?"
   ì¹´í…Œê³ ë¦¬: safety

5. ì§ˆë¬¸: "ë¶„ì êµ¬ì¡°ì™€ ì„±ëŠ¥ì˜ ê´€ê³„ëŠ”?"
   ì¹´í…Œê³ ë¦¬: technical

6. ì§ˆë¬¸: "HRD-Net ì‹œìŠ¤í…œ ì‚¬ìš©ë²•ì€?"
   ì¹´í…Œê³ ë¦¬: hr

**ë¶„ì„ ëŒ€ìƒ:**
ì§ˆë¬¸: {question}

**ì§€ì‹œì‚¬í•­:**
1. ì§ˆë¬¸ì˜ ì£¼ì œì™€ ì˜ë„ë¥¼ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”
2. ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ê°€ ê´€ë ¨ë  ìˆ˜ ìˆìœ¼ë©´ ëª¨ë‘ ë‚˜ì—´í•˜ì„¸ìš” (ìµœëŒ€ 2ê°œ)
3. ì‘ë‹µì€ ì¹´í…Œê³ ë¦¬ ì´ë¦„ë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš” (ì†Œë¬¸ì, ì¶”ê°€ ì„¤ëª… ì—†ì´)
4. ì˜ˆ: "technical" ë˜ëŠ” "technical,business"

ì¹´í…Œê³ ë¦¬:"""

        try:
            # LLM í˜¸ì¶œ (LLMì˜ invoke ë©”ì„œë“œ ì‚¬ìš©)
            response = self.llm.invoke(prompt)

            # ì‘ë‹µì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            categories_str = response.strip().lower()
            categories = [c.strip() for c in categories_str.split(",")]

            # ìœ íš¨í•œ ì¹´í…Œê³ ë¦¬ë§Œ í•„í„°ë§
            valid_categories = ["technical", "business", "hr", "safety", "reference"]
            filtered_categories = [c for c in categories if c in valid_categories]

            if filtered_categories:
                print(f"  âœ“ ì§ˆë¬¸ ì¹´í…Œê³ ë¦¬ ê°ì§€: {', '.join(filtered_categories)}")
                return filtered_categories
            else:
                # ìœ íš¨í•˜ì§€ ì•Šì€ ì‘ë‹µì´ë©´ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ë°˜í™˜ (í•„í„°ë§ ì—†ìŒ)
                print(f"  âš  ì•Œ ìˆ˜ ì—†ëŠ” ì¹´í…Œê³ ë¦¬ ì‘ë‹µ '{categories_str}', í•„í„°ë§ ë¹„í™œì„±í™”")
                return []

        except Exception as e:
            print(f"  âš  ì¹´í…Œê³ ë¦¬ ê°ì§€ ì‹¤íŒ¨ ({e}), í•„í„°ë§ ë¹„í™œì„±í™”")
            return []

    def _filter_by_category(self, results: List[tuple], target_categories: List[str]) -> List[tuple]:
        """ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ í•„í„°ë§

        Args:
            results: (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            target_categories: ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸

        Returns:
            í•„í„°ë§ëœ (Document, score) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        # ì¹´í…Œê³ ë¦¬ê°€ ë¹„ì–´ìˆìœ¼ë©´ í•„í„°ë§ í•˜ì§€ ì•ŠìŒ
        if not target_categories:
            return results

        filtered_results = []
        for doc, score in results:
            doc_category = doc.metadata.get("category", "reference")

            # ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ê°€ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ì™€ ì¼ì¹˜í•˜ë©´ í¬í•¨
            if doc_category in target_categories:
                filtered_results.append((doc, score))

        # í•„í„°ë§ ê²°ê³¼ê°€ ë„ˆë¬´ ì ìœ¼ë©´ (3ê°œ ë¯¸ë§Œ) ì›ë³¸ ë°˜í™˜ (ë„ˆë¬´ ì—„ê²©í•œ í•„í„°ë§ ë°©ì§€)
        if len(filtered_results) < 3:
            print(f"  âš  ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ê²°ê³¼ ë¶€ì¡± ({len(filtered_results)}ê°œ), í•„í„°ë§ ë¹„í™œì„±í™”")
            return results

        print(f"  âœ“ ì¹´í…Œê³ ë¦¬ í•„í„°ë§: {len(results)}ê°œ â†’ {len(filtered_results)}ê°œ (ì¹´í…Œê³ ë¦¬: {', '.join(target_categories)})")
        return filtered_results

    def _get_context(self, question: str, chat_history: List[Dict] = None) -> str:
        context_start = time.perf_counter()
        # Chat history ìºì‹œ ì—…ë°ì´íŠ¸
        if chat_history:
            self._chat_history_cache = chat_history

        # ì¹´í…Œê³ ë¦¬ ê°ì§€ (Phase 1: ì£¼ì œ ì¼ê´€ì„± ê²€ì¦)
        categories = self._detect_question_category(question)

        # ì¿¼ë¦¬ íƒ€ì… ê°ì§€
        query_type = self._detect_query_type(question)
        
        # êµ¬ì²´ì  ì •ë³´ ì¶”ì¶œ ëª¨ë“œ: Small-to-Large ê²€ìƒ‰ í™œìš©
        if query_type == "specific_info":
            try:
                # 1ë‹¨ê³„: Small-to-Large ê²€ìƒ‰ìœ¼ë¡œ ì •í™•í•œ ì²­í¬ ì°¾ê¸°
                stl_results = self.small_to_large_search.search_with_context_expansion(
                    question, top_k=20, max_parents=5, partial_context_size=300
                )
                
                if stl_results:
                    # Small-to-Large ê²°ê³¼ë¥¼ (doc, score) í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
                    weighted_results = []
                    for doc in stl_results:
                        # ì²­í¬ íƒ€ì…ë³„ ê°€ì¤‘ì¹˜ ì ìš©
                        chunk_type_weight = doc.metadata.get("chunk_type_weight", 1.0)
                        # ê¸°ë³¸ ì ìˆ˜ (Small-to-LargeëŠ” ì •í™•í•œ ë§¤ì¹­ì„ ìš°ì„ í•˜ë¯€ë¡œ ë†’ì€ ì ìˆ˜)
                        base_score = 0.8 * chunk_type_weight
                        weighted_results.append((doc, base_score))
                    
                    # ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ì ìš© (Phase 1)
                    weighted_results = self._filter_by_category(weighted_results, categories)

                    # Re-ranking ì ìš© (ìˆëŠ” ê²½ìš°)
                    if self.use_reranker and len(weighted_results) > 0:
                        docs_for_rerank = [{
                            "page_content": d.page_content,
                            "metadata": d.metadata,
                            "vector_score": s,
                            "document": d
                        } for d, s in weighted_results]
                        reranked = self.reranker.rerank(question, docs_for_rerank, top_k=min(15, len(docs_for_rerank)))
                        pairs = [(d["document"], d.get("rerank_score", 0.8)) for d in reranked]
                    else:
                        pairs = weighted_results
                    
                    # ì¤‘ë³µ ì œê±°
                    dedup = self._unique_by_file(pairs, self.top_k * 2)
                    self._last_retrieved_docs = dedup[:self.top_k]
                    docs = [d for d, _ in self._last_retrieved_docs]
                    elapsed = time.perf_counter() - context_start
                    print(f"[Timing] context retrieval (Small-to-Large, type={query_type}): {elapsed:.2f}s")
                    print(f"ğŸ” êµ¬ì²´ì  ì •ë³´ ì¶”ì¶œ ëª¨ë“œ: Small-to-Large ê²€ìƒ‰ (ì¿¼ë¦¬ íƒ€ì…: {query_type})")
                    return self._format_docs(docs)
            except Exception as e:
                print(f"Small-to-Large ê²€ìƒ‰ ì‹¤íŒ¨, ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±: {e}")
                # í´ë°±: ê¸°ë³¸ ê²€ìƒ‰ ê³„ì† ì§„í–‰
        
        # ìš”ì•½ ëª¨ë“œ: ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
        if query_type == "summary":
            # ìš”ì•½ì€ ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ í•„ìš”
            original_top_k = self.top_k
            self.top_k = min(10, original_top_k * 2)
            try:
                context = self._get_context_standard(question, categories)
                elapsed = time.perf_counter() - context_start
                print(f"[Timing] context retrieval (summary, type={query_type}): {elapsed:.2f}s")
                self.top_k = original_top_k
                return context
            except:
                self.top_k = original_top_k
                return ""

        # ê¸°ë³¸ ê²€ìƒ‰ (ê¸°ì¡´ ë¡œì§)
        context = self._get_context_standard(question, categories)
        elapsed = time.perf_counter() - context_start
        print(f"[Timing] context retrieval (standard, type={query_type}): {elapsed:.2f}s")
        return context

    def _get_context_standard(self, question: str, categories: List[str] = None) -> str:
        """í‘œì¤€ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
        if categories is None:
            categories = []
        overall_start = time.perf_counter()
        
        # ğŸ†• ë™ì  top_k ê²°ì • (ì§ˆë¬¸ íŠ¹ì„± ë¶„ì„)
        dynamic_top_k = self.determine_optimal_top_k(question)
        print(f"ğŸ” ì§ˆë¬¸ íŠ¹ì„± ë¶„ì„: top_k = {dynamic_top_k} (ê¸°ë³¸: {self.top_k})")
        
        # Multi-Query Rewriting ì ìš©
        if self.enable_multi_query:
            mq_start = time.perf_counter()
            queries = self.generate_rewritten_queries(question, num_queries=self.multi_query_num)
            print(f"[Timing] multi_query_generate: {time.perf_counter() - mq_start:.2f}s (queries={len(queries)})")
            all_retrieved_chunks = []
            chunk_id_set = set()
            
            # ëª¨ë“  ì¿¼ë¦¬ì— ëŒ€í•´ ê²€ìƒ‰ ìˆ˜í–‰
            for idx, query in enumerate(queries, start=1):
                query_start = time.perf_counter()
                try:
                    results = []
                    if self.use_reranker:
                        base = self._search_candidates(query)
                        if base:
                            docs_for_rerank = [{
                                "page_content": d.page_content,
                                "metadata": d.metadata,
                                "vector_score": s,
                                "document": d
                            } for d, s in base]
                            reranked = self.reranker.rerank(query, docs_for_rerank, top_k=max(self.top_k * 3, 15))
                            results = [(d["document"], d.get("rerank_score", 0)) for d in reranked]
                        else:
                            results = []
                    else:
                        results = self.vectorstore.similarity_search_with_score(query, k=max(self.top_k * 3, 15))

                    # ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ì ìš©
                    results = self._filter_by_category(results, categories)

                    print(f"[Timing] retrieval[{idx}/{len(queries)}]: {time.perf_counter() - query_start:.2f}s (docs={len(results)})")
                    
                    # ì¤‘ë³µ ì œê±° (ë¬¸ì„œ ë‚´ìš© ê¸°ì¤€)
                    for doc, score in results:
                        doc_id = f"{doc.metadata.get('source', '')}_{doc.page_content[:50]}"
                        if doc_id not in chunk_id_set:
                            all_retrieved_chunks.append((doc, score))
                            chunk_id_set.add(doc_id)
                            
                except Exception as e:
                    print(f"ì¿¼ë¦¬ '{query}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    continue
            
            if all_retrieved_chunks:
                # ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ì ìš© (ìµœì¢… í†µí•©)
                all_retrieved_chunks = self._filter_by_category(all_retrieved_chunks, categories)

                # ì›ë³¸ ì¿¼ë¦¬ë¡œ ì¬ìˆœìœ„ ë§¤ê¹€
                if self.use_reranker:
                    rerank_start = time.perf_counter()
                    docs_for_final_rerank = [{
                        "page_content": d.page_content,
                        "metadata": d.metadata,
                        "vector_score": s,
                        "document": d
                    } for d, s in all_retrieved_chunks]
                    final_reranked = self.reranker.rerank(question, docs_for_final_rerank, top_k=max(self.top_k * 2, 20))
                    pairs = [(d["document"], d.get("rerank_score", 0)) for d in final_reranked]
                    print(f"[Timing] final_rerank (multi-query): {time.perf_counter() - rerank_start:.2f}s (candidates={len(all_retrieved_chunks)})")
                else:
                    pairs = all_retrieved_chunks

                # ğŸ†• ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ í•„í„°ë§ íŒŒì´í”„ë¼ì¸
                filter_start = time.perf_counter()

                # 1ë‹¨ê³„: í†µê³„ ê¸°ë°˜ ì´ìƒì¹˜ ì œê±° (ê°œì„ ì•ˆ 3)
                pairs = self._statistical_outlier_removal(pairs, method='mad')

                # 2ë‹¨ê³„: Re-ranker Gap ê¸°ë°˜ ì»·ì˜¤í”„ (ê°œì„ ì•ˆ 5)
                pairs = self._reranker_gap_based_cutoff(pairs, min_docs=self.top_k)

                print(f"[Timing] smart_filtering: {time.perf_counter() - filter_start:.2f}s")

                dedup = self._unique_by_file(pairs, dynamic_top_k)  # ë™ì  top_k ì‚¬ìš©
                self._last_retrieved_docs = dedup
                docs = [d for d, _ in dedup]
                print(f"[Timing] context_standard total: {time.perf_counter() - overall_start:.2f}s (mode=multi-query, top_k={dynamic_top_k})")
                return self._format_docs(docs)
        
        # í´ë°±: ë‹¨ì¼ ì¿¼ë¦¬ ê²€ìƒ‰ (ë™ì˜ì–´ í™•ì¥ í¬í•¨)
        syn_start = time.perf_counter()
        expanded_question = self.expand_query_with_synonyms(question)
        print(f"[Timing] synonym_expand: {time.perf_counter() - syn_start:.2f}s")
        
        if self.use_reranker:
            retrieval_start = time.perf_counter()
            base = self._search_candidates(expanded_question)
            if not base:
                self._last_retrieved_docs = []
                print(f"[Timing] context_standard total: {time.perf_counter() - overall_start:.2f}s (mode=fallback, docs=0)")
                return ""
            
            # base ëŠ” (doc, score) í˜•íƒœ
            docs_for_rerank = [{
                "page_content": d.page_content,
                "metadata": d.metadata,
                "vector_score": s,
                "document": d
            } for d, s in base]
            print(f"[Timing] candidate_retrieval (fallback): {time.perf_counter() - retrieval_start:.2f}s (candidates={len(base)})")
            rerank_start = time.perf_counter()
            reranked = self.reranker.rerank(expanded_question, docs_for_rerank, top_k=max(self.top_k * 8, 40))
            pairs = [(d["document"], d.get("rerank_score", 0)) for d in reranked]
            print(f"[Timing] final_rerank (fallback): {time.perf_counter() - rerank_start:.2f}s")

            # ğŸ†• ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ í•„í„°ë§ íŒŒì´í”„ë¼ì¸
            filter_start = time.perf_counter()

            # 1ë‹¨ê³„: í†µê³„ ê¸°ë°˜ ì´ìƒì¹˜ ì œê±° (ê°œì„ ì•ˆ 3)
            pairs = self._statistical_outlier_removal(pairs, method='mad')

            # 2ë‹¨ê³„: Re-ranker Gap ê¸°ë°˜ ì»·ì˜¤í”„ (ê°œì„ ì•ˆ 5)
            pairs = self._reranker_gap_based_cutoff(pairs, min_docs=self.top_k)

            print(f"[Timing] smart_filtering: {time.perf_counter() - filter_start:.2f}s")

            dedup = self._unique_by_file(pairs, dynamic_top_k)  # ë™ì  top_k ì‚¬ìš©

            # ìºì‹œ ì €ì¥: ì‹¤ì œ ì‚¬ìš©ëœ ë¬¸ì„œì™€ ì ìˆ˜
            self._last_retrieved_docs = dedup  # [(doc, score), ...]

            docs = [d for d, _ in dedup]
            print(f"[Timing] deduplication: {time.perf_counter() - rerank_start:.2f}s (selected={len(dedup)})")
        else:
            retrieval_start = time.perf_counter()
            pairs = self.vectorstore.similarity_search_with_score(expanded_question, k=max(self.top_k * 8, 40))
            # ë„ë©”ì¸ í•„í„°ë§ ì ìš©

            # ğŸ†• ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ í•„í„°ë§ íŒŒì´í”„ë¼ì¸
            filter_start = time.perf_counter()

            # 1ë‹¨ê³„: í†µê³„ ê¸°ë°˜ ì´ìƒì¹˜ ì œê±° (ê°œì„ ì•ˆ 3)
            pairs = self._statistical_outlier_removal(pairs, method='mad')

            # 2ë‹¨ê³„: Re-ranker Gap ê¸°ë°˜ ì»·ì˜¤í”„ (ê°œì„ ì•ˆ 5)
            pairs = self._reranker_gap_based_cutoff(pairs, min_docs=self.top_k)

            print(f"[Timing] smart_filtering: {time.perf_counter() - filter_start:.2f}s")

            dedup = self._unique_by_file(pairs, dynamic_top_k)  # ë™ì  top_k ì‚¬ìš©

            # ìºì‹œ ì €ì¥
            self._last_retrieved_docs = dedup

            docs = [d for d, _ in dedup]
            print(f"[Timing] candidate_retrieval (vector fallback): {time.perf_counter() - retrieval_start:.2f}s (selected={len(dedup)})")
        print(f"[Timing] context_standard total: {time.perf_counter() - overall_start:.2f}s (mode=fallback, top_k={dynamic_top_k})")
        return self._format_docs(docs)

    def expand_query_with_synonyms(self, original_query: str) -> str:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ ì¿¼ë¦¬ì— ëŒ€í•œ ë™ì˜ì–´/ì—°ê´€ì–´ë¥¼ ìƒì„±í•˜ê³  í™•ì¥ëœ ì¿¼ë¦¬ë¥¼ ë°˜í™˜"""
        if not self.enable_synonym_expansion:
            return original_query
            
        try:
            prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ ê²€ìƒ‰ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤. ì‚¬ìš©ì ì¿¼ë¦¬ì˜ ê²€ìƒ‰ íš¨ê³¼ë¥¼ ë†’ì´ê¸° ìœ„í•´ ë™ì˜ì–´ì™€ ì—°ê´€ì–´ë¥¼ ìƒì„±í•˜ì„¸ìš”.

**ì›ë³¸ ì¿¼ë¦¬**: "{original_query}"

**ìƒì„± ê·œì¹™**:
1. ë™ì˜ì–´: ì¿¼ë¦¬ì˜ í•µì‹¬ ê°œë…ê³¼ ë™ì¼í•œ ì˜ë¯¸ì˜ ë‹¤ë¥¸ í‘œí˜„
2. ìƒìœ„ì–´: ë” ì¼ë°˜ì ì¸ ê°œë…
3. í•˜ìœ„ì–´: ë” êµ¬ì²´ì ì¸ ê°œë…
4. ê´€ë ¨ì–´: ë°€ì ‘í•˜ê²Œ ì—°ê´€ëœ ê°œë…

**Few-shot ì˜ˆì‹œ**:
[ì˜ˆì‹œ 1]
ì›ë³¸: "OLED íš¨ìœ¨"
ë™ì˜ì–´: ["ìœ ê¸°ë°œê´‘ë‹¤ì´ì˜¤ë“œ íš¨ìœ¨", "OLED ì„±ëŠ¥", "ë°œê´‘ íš¨ìœ¨"]

[ì˜ˆì‹œ 2]
ì›ë³¸: "TADF ì¬ë£Œ"
ë™ì˜ì–´: ["ì—´í™œì„±í™” ì§€ì—° í˜•ê´‘ ì¬ë£Œ", "thermally activated delayed fluorescence", "TADF ì†Œì¬"]

[ì˜ˆì‹œ 3]
ì›ë³¸: "ë°œê´‘ íš¨ìœ¨ ì¸¡ì •"
ë™ì˜ì–´: ["ê´‘ì¶œë ¥ ì¸¡ì •", "luminescence efficiency", "ë°œê´‘ ì„±ëŠ¥ í‰ê°€"]

**ì¶œë ¥ í˜•ì‹**: JSON ë¦¬ìŠ¤íŠ¸
{{"synonyms": ["ìš©ì–´1", "ìš©ì–´2", "ìš©ì–´3"], "related_terms": ["ê´€ë ¨ì–´1", "ê´€ë ¨ì–´2"]}}

**ìƒì„±**:"""
            
            response = self.llm.invoke(prompt)
            
            # ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            if hasattr(response, 'content'):
                response_text = response.content
            elif hasattr(response, 'text'):
                response_text = response.text
            else:
                response_text = str(response)
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ë©€í‹°ë¼ì¸ JSON ì§€ì›)
                json_match = re.search(r'\{.*?\}', response_text, re.DOTALL)
                if json_match:
                    expansion_data = json.loads(json_match.group())
                    synonyms = expansion_data.get("synonyms", [])
                    related_terms = expansion_data.get("related_terms", [])
                    
                    # ëª¨ë“  ìš©ì–´ë¥¼ ê²°í•©
                    all_terms = synonyms + related_terms
                    if all_terms:
                        expanded_query = f"{original_query} ({', '.join(all_terms)})"
                    else:
                        expanded_query = original_query
                    
                    print(f"ğŸ” ë™ì˜ì–´ í™•ì¥: {original_query} â†’ {expanded_query}")
                    return expanded_query
                else:
                    # JSON í˜•ì‹ì´ ì•„ë‹Œ ê²½ìš° í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
                    lines = response_text.strip().split('\n')
                    all_terms = []
                    for line in lines:
                        line = line.strip().strip('"[],')
                        if line and len(line) > 1 and not line.startswith('ë™ì˜ì–´') and not line.startswith('ê´€ë ¨ì–´'):
                            all_terms.append(line)
                    
                    if all_terms:
                        expanded_query = f"{original_query} ({', '.join(all_terms[:5])})"  # ìµœëŒ€ 5ê°œ
                    else:
                        expanded_query = original_query
                    
                    print(f"ğŸ” ë™ì˜ì–´ í™•ì¥: {original_query} â†’ {expanded_query}")
                    return expanded_query
                    
            except (json.JSONDecodeError, ValueError) as e:
                print(f"ë™ì˜ì–´ íŒŒì‹± ì‹¤íŒ¨: {e}")
                
        except Exception as e:
            print(f"ë™ì˜ì–´ í™•ì¥ ì‹¤íŒ¨: {e}")
        
        return original_query

    def determine_optimal_top_k(self, question: str) -> int:
        """ì§ˆë¬¸ íŠ¹ì„±ì— ë”°ë¼ ìµœì ì˜ top_k ê°’ì„ ë™ì ìœ¼ë¡œ ê²°ì •"""
        try:
            prompt = f"""ë‹¹ì‹ ì€ RAG ê²€ìƒ‰ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸ íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ë¬¸ì„œ ê²€ìƒ‰ ê°œìˆ˜ë¥¼ ê²°ì •í•˜ì„¸ìš”.

**ì§ˆë¬¸**: "{question}"

**ë¶„ì„ ì ˆì°¨**:
1ë‹¨ê³„ [ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜]:
   - ë‹¨ì¼ ì‚¬ì‹¤ ì°¾ê¸°: "ë¬´ì—‡", "ì–¼ë§ˆ", "ëˆ„êµ¬", "ì–¸ì œ", "ì–´ë””" (ëª…í™•í•œ í•˜ë‚˜ì˜ ë‹µë³€)
   - ëª©ë¡ ë‚˜ì—´: "ëª¨ë‘", "ì „ì²´", "ë‚˜ì—´", "ëª©ë¡", "ì œëª©ë“¤" (ì—¬ëŸ¬ í•­ëª©)
   - ë¹„êµ/ë¶„ì„: "ì°¨ì´", "ë¹„êµ", "vs", "ëŒ€ë¹„", "ê´€ê³„" (ë‹¤ê°ë„ ë¶„ì„)
   - ì¢…í•© ì •ë³´: "ìš”ì•½", "í•µì‹¬", "ê°œìš”", "ì •ë¦¬" (ì „ì²´ ì»¨í…ìŠ¤íŠ¸)
   - ë³µí•© ì§ˆë¬¸: ì—¬ëŸ¬ ìœ í˜•ì´ í˜¼í•©ëœ ê²½ìš°

2ë‹¨ê³„ [ë³µì¡ë„ í‰ê°€]:
   - ë‚®ìŒ: ë‹¨ìˆœí•œ ì‚¬ì‹¤ í™•ì¸ (3-5ê°œ)
   - ì¤‘ê°„: ë¹„êµ/ë¶„ì„, ê¸°ë³¸ ì¢…í•© (10-15ê°œ)
   - ë†’ìŒ: ëª©ë¡ ë‚˜ì—´, ë³µí•© ì§ˆë¬¸ (20-30ê°œ)

**Few-shot ì˜ˆì‹œ**:
[ì˜ˆì‹œ 1]
ì§ˆë¬¸: "OLED íš¨ìœ¨ì€ ì–¼ë§ˆì¸ê°€?"
ìœ í˜•: ë‹¨ì¼ ì‚¬ì‹¤ ì°¾ê¸°
ë³µì¡ë„: ë‚®ìŒ
ì¶”ì²œ ê°œìˆ˜: 5

[ì˜ˆì‹œ 2]
ì§ˆë¬¸: "ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ ëª¨ë“  ì¬ë£Œë¥¼ ë‚˜ì—´í•´ì£¼ì„¸ìš”."
ìœ í˜•: ëª©ë¡ ë‚˜ì—´
ë³µì¡ë„: ë†’ìŒ
ì¶”ì²œ ê°œìˆ˜: 25

[ì˜ˆì‹œ 3]
ì§ˆë¬¸: "OLEDì™€ LEDì˜ ì°¨ì´ì ì„ ë¹„êµí•´ì£¼ì„¸ìš”."
ìœ í˜•: ë¹„êµ/ë¶„ì„
ë³µì¡ë„: ì¤‘ê°„
ì¶”ì²œ ê°œìˆ˜: 12

**ì¶œë ¥ í˜•ì‹**: ìˆ«ìë§Œ ì¶œë ¥ (ë²”ìœ„: 3-30)

**ë¶„ì„ ê²°ê³¼**:"""

            response = self.llm.invoke(prompt)
            response_text = response.content if hasattr(response, 'content') else str(response)
            
            # ìˆ«ì ì¶”ì¶œ
            numbers = re.findall(r'\d+', response_text)
            if numbers:
                top_k = int(numbers[0])
                top_k = max(3, min(30, top_k))  # 3~30 ë²”ìœ„ ì œí•œ
                print(f"ğŸ¯ ë™ì  top_k ê²°ì •: {top_k} (ì§ˆë¬¸ ìœ í˜• ë¶„ì„)")
                return top_k
        except Exception as e:
            print(f"ë™ì  top_k ê²°ì • ì‹¤íŒ¨: {e}")
        
        # í´ë°±: ê¸°ë³¸ê°’
        return self.top_k
    
    def generate_rewritten_queries(self, original_query: str, num_queries: int = 3) -> List[str]:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ ì¿¼ë¦¬ë¥¼ ì—¬ëŸ¬ ê´€ì ì—ì„œ ì¬ì‘ì„±í•œ ëŒ€ì•ˆ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±"""
        if not self.enable_multi_query:
            return [original_query]
            
        try:
            prompt = f"""ë‹¹ì‹ ì€ ê²€ìƒ‰ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì›ë³¸ ì¿¼ë¦¬ë¥¼ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì¬ì‘ì„±í•˜ì—¬ ê²€ìƒ‰ ë¦¬ì½œì„ í–¥ìƒì‹œí‚¤ì„¸ìš”.

**ì›ë³¸ ì¿¼ë¦¬**: "{original_query}"

**ì¬ì‘ì„± ì „ëµ** (ê°ê° 1ê°œì”© ìƒì„±):
1. **ê¸°ìˆ ì  ê´€ì **: êµ¬ì²´ì ì¸ ê¸°ìˆ  ìš©ì–´ì™€ ë°©ë²•ë¡  ì¤‘ì‹¬
2. **ê°œë…ì  ê´€ì **: ì¶”ìƒì  ê°œë…ê³¼ ì´ë¡  ì¤‘ì‹¬  
3. **ì‘ìš© ê´€ì **: ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€ì™€ ì ìš© ì¤‘ì‹¬
4. **ë¹„êµ ê´€ì **: ë¹„êµ ë¶„ì„ ì§ˆë¬¸ í˜•íƒœ (ì ìš© ê°€ëŠ¥í•œ ê²½ìš°)
5. **ë¬¸ì œ í•´ê²° ê´€ì **: ë¬¸ì œ ì •ì˜ ë° í•´ê²°ì±… ì¤‘ì‹¬ (ì ìš© ê°€ëŠ¥í•œ ê²½ìš°)

**Few-shot ì˜ˆì‹œ**:
[ì›ë³¸] "OLED íš¨ìœ¨ í–¥ìƒ ë°©ë²•"
[ì¬ì‘ì„±]
1. ê¸°ìˆ ì : "OLED ë°œê´‘ íš¨ìœ¨(luminous efficacy) ê°œì„  ê¸°ìˆ "
2. ê°œë…ì : "ìœ ê¸°ë°œê´‘ë‹¤ì´ì˜¤ë“œì˜ ê´‘ì¶œë ¥ í–¥ìƒ ì›ë¦¬"
3. ì‘ìš©: "OLED ë””ìŠ¤í”Œë ˆì´ íš¨ìœ¨ ìµœì í™” ì‚¬ë¡€"
4. ë¹„êµ: "OLED íš¨ìœ¨ ë¹„êµ: ë‹¤ë¥¸ ë””ìŠ¤í”Œë ˆì´ ê¸°ìˆ  ëŒ€ë¹„"
5. ë¬¸ì œí•´ê²°: "OLED íš¨ìœ¨ ì €í•˜ ì›ì¸ ë° í•´ê²°ì±…"

**ì¶œë ¥ í˜•ì‹**: JSON ë¦¬ìŠ¤íŠ¸
["ì¿¼ë¦¬1", "ì¿¼ë¦¬2", "ì¿¼ë¦¬3"]

**ì¬ì‘ì„±**:"""
            
            response = self.llm.invoke(prompt)
            
            # ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            if hasattr(response, 'content'):
                response_text = response.content
            elif hasattr(response, 'text'):
                response_text = response.text
            else:
                response_text = str(response)
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
                json_match = re.search(r'\[.*?\]', response_text)
                if json_match:
                    rewritten_queries = json.loads(json_match.group())
                else:
                    # JSON í˜•ì‹ì´ ì•„ë‹Œ ê²½ìš° í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
                    lines = response_text.strip().split('\n')
                    rewritten_queries = []
                    for line in lines:
                        line = line.strip().strip('"[]')
                        if line and len(line) > 1:
                            rewritten_queries.append(line)
                    rewritten_queries = rewritten_queries[:num_queries]  # ìµœëŒ€ num_queriesê°œ
                
                # ì›ë³¸ ì¿¼ë¦¬ê°€ í¬í•¨ë˜ì§€ ì•Šì•˜ë‹¤ë©´, ë¦¬ìŠ¤íŠ¸ì˜ ë§¨ ì•ì— ì¶”ê°€
                if original_query not in rewritten_queries:
                    rewritten_queries.insert(0, original_query)
                    
                print(f"ğŸ”„ ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„±: {original_query} â†’ {len(rewritten_queries)}ê°œ ì¿¼ë¦¬")
                return rewritten_queries
                    
            except (json.JSONDecodeError, ValueError) as e:
                print(f"ë‹¤ì¤‘ ì¿¼ë¦¬ íŒŒì‹± ì‹¤íŒ¨: {e}")
                
        except Exception as e:
            print(f"ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return [original_query]

    def _format_chat_history(self, messages: List[Dict[str, str]], max_messages: int = 5) -> str:
        if not messages:
            return "ì´ì „ ëŒ€í™” ì—†ìŒ"
        recent_messages = messages[-max_messages * 2:] if len(messages) > max_messages * 2 else messages
        formatted = []
        for msg in recent_messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            if role == "user":
                formatted.append(f"ì‚¬ìš©ì: {content}")
            elif role == "assistant":
                formatted.append(f"ì–´ì‹œìŠ¤í„´íŠ¸: {content}")
        return "\n".join(formatted) if formatted else "ì´ì „ ëŒ€í™” ì—†ìŒ"

    def query(self, question: str, chat_history: List[Dict[str, str]] = None) -> Dict[str, Any]:
        try:
            formatted_history = self._format_chat_history(chat_history or [])
            
            # ì¿¼ë¦¬ íƒ€ì… ê°ì§€ ë° í”„ë¡¬í”„íŠ¸ ì„ íƒ
            query_type = self._detect_query_type(question)
            if query_type in self.prompt_templates:
                selected_template = self.prompt_templates[query_type]
                self.prompt = PromptTemplate(
                    template=selected_template,
                    input_variables=["chat_history", "context", "question"]
                )
                # ì²´ì¸ ì¬êµ¬ì„± (í”„ë¡¬í”„íŠ¸ ë³€ê²½ ë°˜ì˜)
                self.chain = (
                    {
                        "context": lambda x: self._get_context(x["question"]),
                        "chat_history": lambda x: x.get("chat_history", "ì´ì „ ëŒ€í™” ì—†ìŒ"),
                        "question": lambda x: x["question"]
                    }
                    | self.prompt
                    | self.llm
                    | StrOutputParser()
                )
            
            # ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (_last_retrieved_docs ì—…ë°ì´íŠ¸ë¨)
            context = self._get_context(question, chat_history)
            
            # ë‹µë³€ ìƒì„±
            answer = self.chain.invoke({
                "question": question,
                "chat_history": formatted_history
            })
            
            # Phase 2: ë‹µë³€ ê²€ì¦ ë° ì¬ìƒì„± (ìƒìš© ì„œë¹„ìŠ¤ ìˆ˜ì¤€)
            docs_for_confidence = [d for d, _ in self._last_retrieved_docs[:self.top_k]]
            verification_result = self._verify_answer_quality(question, answer, docs_for_confidence)
            
            if not verification_result["is_valid"]:
                print(f"âš ï¸ ë‹µë³€ ê²€ì¦ ì‹¤íŒ¨: {verification_result['reason']}")
                print(f"ğŸ”„ ë¬¸ì„œ ê¸°ë°˜ ì¬ìƒì„± ì‹œë„...")
                
                # ë¬¸ì„œ ê¸°ë°˜ ì¬ìƒì„±
                regenerated_answer = self._regenerate_answer(question, answer, docs_for_confidence, formatted_history)
                if regenerated_answer:
                    answer = regenerated_answer
                    print(f"âœ… ë‹µë³€ ì¬ìƒì„± ì™„ë£Œ")
                else:
                    print(f"âš ï¸ ì¬ìƒì„± ì‹¤íŒ¨, ì›ë³¸ ë‹µë³€ ì‚¬ìš©")

            # Phase A-2: NotebookLM ìŠ¤íƒ€ì¼ ì¸ë¼ì¸ Citation ì¶”ê°€
            # ìºì‹œëœ ë¬¸ì„œì—ì„œ Document ê°ì²´ ì¶”ì¶œ
            source_docs = [doc for doc, _ in self._last_retrieved_docs[:self.top_k]]

            if source_docs:
                # Citation ìƒì„± ë° ë‹µë³€ì— í†µí•©
                answer = self._generate_source_citations(answer, source_docs)

            # ìºì‹œëœ ë¬¸ì„œì—ì„œ ì¶œì²˜ ì •ë³´ ìƒì„±
            sources = []
            docs_for_confidence = []

            for doc, score in self._last_retrieved_docs[:self.top_k]:
                docs_for_confidence.append(doc)
                source_info = {
                    "file_name": doc.metadata.get("file_name", "Unknown"),
                    "page_number": doc.metadata.get("page_number", "Unknown"),
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "similarity_score": float(round(score * 100, 1))  # 0-100 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
                }
                sources.append(source_info)
            
            # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            confidence = self._calculate_confidence_score(question, answer, docs_for_confidence)
            
            return {
                "answer": answer,
                "sources": sources,
                "confidence": confidence,
                "success": True
            }
        except Exception as e:
            print(f"âŒ query() ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return {
                "answer": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "sources": [],
                "confidence": 0.0,
                "success": False
            }
    
    def _verify_answer_quality(self, question: str, answer: str, docs: List[Document]) -> Dict[str, Any]:
        """ë‹µë³€ í’ˆì§ˆ ê²€ì¦ (Phase 2: ìƒìš© ì„œë¹„ìŠ¤ ìˆ˜ì¤€)
        
        Returns:
            {
                "is_valid": bool,
                "reason": str,
                "scores": {
                    "no_forbidden_phrases": float,
                    "has_citation": float,
                    "content_match": float,
                    "specificity": float
                }
            }
        """
        if not docs or not answer:
            return {
                "is_valid": False,
                "reason": "ë¬¸ì„œ ë˜ëŠ” ë‹µë³€ì´ ì—†ìŒ",
                "scores": {}
            }
        
        answer_lower = answer.lower()
        doc_contents = " ".join([d.page_content.lower() for d in docs])
        doc_metadata = [(d.metadata.get("file_name", ""), d.metadata.get("page_number", "")) for d in docs]
        
        scores = {}
        
        # 1. ê¸ˆì§€ êµ¬ë¬¸ ê²€ì‚¬
        forbidden_phrases = [
            "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤",
            "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "ì—†ìŠµë‹ˆë‹¤",
            "ì •ë³´ ë¶€ì¡±",
            "í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "cannot find",
            "not found",
            "no information"
        ]
        has_forbidden = any(phrase in answer_lower for phrase in forbidden_phrases)
        scores["no_forbidden_phrases"] = 0.0 if has_forbidden else 1.0
        
        # 2. ë¬¸ì„œ ì¸ìš© ê²€ì‚¬ (í˜ì´ì§€ ë²ˆí˜¸, íŒŒì¼ëª… ë“± ë©”íƒ€ë°ì´í„° ì–¸ê¸‰)
        has_citation = False
        citation_keywords = ["í˜ì´ì§€", "page", "ë¬¸ì„œ", "íŒŒì¼", "ì„¹ì…˜", "section"]
        has_citation_keywords = any(keyword in answer_lower for keyword in citation_keywords)
        
        # íŒŒì¼ëª…ì´ë‚˜ í˜ì´ì§€ ë²ˆí˜¸ ì§ì ‘ ì–¸ê¸‰ í™•ì¸
        for file_name, page_num in doc_metadata:
            if file_name and file_name.lower() in answer_lower:
                has_citation = True
                break
            if page_num and str(page_num) in answer:
                has_citation = True
                break
        
        scores["has_citation"] = 1.0 if (has_citation or has_citation_keywords) else 0.3
        
        # 3. ë¬¸ì„œ ë‚´ìš©ê³¼ì˜ ì¼ì¹˜ ê²€ì‚¬ (í‚¤ì›Œë“œ ë§¤ì¹­)
        question_keywords = set(re.findall(r'\w+', question.lower()))
        doc_keywords = set(re.findall(r'\w+', doc_contents[:1000]))  # ì²˜ìŒ 1000ìë§Œ
        
        # ë‹µë³€ì— ë¬¸ì„œ í‚¤ì›Œë“œê°€ ì–¼ë§ˆë‚˜ í¬í•¨ë˜ëŠ”ì§€
        matching_keywords = question_keywords.intersection(doc_keywords)
        answer_has_keywords = sum(1 for kw in matching_keywords if kw in answer_lower)
        content_match_score = min(1.0, answer_has_keywords / max(len(matching_keywords), 1))
        scores["content_match"] = content_match_score
        
        # 4. êµ¬ì²´ì„± ê²€ì‚¬ (ì¼ë°˜í™”ëœ ë‹µë³€ ê°ì§€)
        # ì¼ë°˜í™”ëœ êµ¬ë¬¸ íŒ¨í„´
        generic_phrases = [
            "ì¼ë°˜ì ìœ¼ë¡œ",
            "ë³´í†µ",
            "ëŒ€ë¶€ë¶„ì˜ ê²½ìš°",
            "ì¼ë°˜ì ìœ¼ë¡œ ì•Œë ¤ì§„",
            "ì¼ë°˜ì ì¸ ì›ë¦¬",
            "ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ”"
        ]
        has_generic = any(phrase in answer_lower for phrase in generic_phrases)
        
        # ë¬¸ì„œ íŠ¹ì • ë‚´ìš© (ìˆ˜ì¹˜, ì´ë¦„, ê³ ìœ ëª…ì‚¬ ë“±) í¬í•¨ ì—¬ë¶€
        has_specifics = bool(re.search(r'\d+[.%]?', answer)) or len([w for w in answer.split() if len(w) > 5]) > 3
        specificity_score = 0.5 if has_generic else 1.0
        if has_specifics:
            specificity_score = min(1.0, specificity_score + 0.3)
        scores["specificity"] = specificity_score
        
        # ì¢…í•© ê²€ì¦
        total_score = (
            scores["no_forbidden_phrases"] * 0.4 +
            scores["has_citation"] * 0.3 +
            scores["content_match"] * 0.2 +
            scores["specificity"] * 0.1
        )
        
        is_valid = total_score >= 0.6 and scores["no_forbidden_phrases"] > 0
        
        reasons = []
        if not is_valid:
            if scores["no_forbidden_phrases"] == 0:
                reasons.append("ê¸ˆì§€ êµ¬ë¬¸ ì‚¬ìš©")
            if scores["has_citation"] < 0.5:
                reasons.append("ë¬¸ì„œ ì¸ìš© ë¶€ì¡±")
            if scores["content_match"] < 0.3:
                reasons.append("ë¬¸ì„œ ë‚´ìš©ê³¼ ë¶ˆì¼ì¹˜")
            if scores["specificity"] < 0.5:
                reasons.append("ì¼ë°˜í™”ëœ ë‹µë³€")
        
        return {
            "is_valid": is_valid,
            "reason": ", ".join(reasons) if reasons else "ì •ìƒ",
            "total_score": total_score,
            "scores": scores
        }
    
    def _regenerate_answer(self, question: str, original_answer: str, docs: List[Document], 
                          chat_history: str) -> Optional[str]:
        """ê²€ì¦ ì‹¤íŒ¨ ì‹œ ë¬¸ì„œ ê¸°ë°˜ ì¬ìƒì„± (Phase 2)"""
        if not docs:
            return None
        
        try:
            # ì¬ìƒì„± ì „ìš© í”„ë¡¬í”„íŠ¸
            context = self._format_docs(docs)
            
            regeneration_prompt = f"""ì´ì „ì— ìƒì„±ëœ ë‹µë³€ì´ ë¬¸ì„œ ê¸°ë°˜ì´ ì•„ë‹ˆì—ˆìŠµë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì‹œ ë‹µë³€í•˜ì„¸ìš”.

âš ï¸ ì¤‘ìš” ê·œì¹™:
1. **ë¬¸ì„œ ìš°ì„ **: ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œì—ì„œë§Œ ì •ë³´ë¥¼ ì°¾ìœ¼ì„¸ìš”
2. **ê¸ˆì§€ êµ¬ë¬¸**: "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ì—†ìŠµë‹ˆë‹¤"ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
3. **ë¬¸ì„œ ì¸ìš©**: ë°˜ë“œì‹œ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì¸ìš©í•˜ê³  í˜ì´ì§€/íŒŒì¼ ì •ë³´ë¥¼ ëª…ì‹œí•˜ì„¸ìš”
4. **êµ¬ì²´ì„±**: ë¬¸ì„œì˜ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ì´ë¦„, ë‚´ìš©ì„ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”

ì´ì „ ëŒ€í™”:
{chat_history}

ì œê³µëœ ë¬¸ì„œ:
{context}

ì§ˆë¬¸:
{question}

ì´ì „ ë‹µë³€ (ì°¸ê³ ìš©, ê°œì„  í•„ìš”):
{original_answer}

ìœ„ ì´ì „ ë‹µë³€ì„ ê°œì„ í•˜ì—¬, ì œê³µëœ ë¬¸ì„œì— ê·¼ê±°í•œ êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.

ë‹µë³€:"""
            
            # LLM ì¬ìƒì„±
            regenerated = self.llm.invoke(regeneration_prompt)
            
            # ì‘ë‹µ íŒŒì‹±
            if hasattr(regenerated, 'content'):
                return regenerated.content.strip()
            elif hasattr(regenerated, 'text'):
                return regenerated.text.strip()
            else:
                return str(regenerated).strip()
                
        except Exception as e:
            print(f"âš ï¸ ì¬ìƒì„± ì˜¤ë¥˜: {e}")
            return None
    
    def _calculate_confidence_score(self, question: str, answer: str, docs: List[Document]) -> float:
        """ë‹µë³€ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° (0-100)"""
        if not docs:
            return 0.0
        
        # 1. ë¬¸ì„œ ê°œìˆ˜ ê¸°ë°˜ ì ìˆ˜ (ë” ë§ì€ ì¶œì²˜ = ë†’ì€ ì‹ ë¢°ë„)
        doc_score = min(len(docs) / 5.0, 1.0)  # 5ê°œ ì´ìƒì´ë©´ ë§Œì 
        
        # 2. ë‹µë³€ ê¸¸ì´ ì ìˆ˜ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ë©´ ê°ì )
        answer_length = len(answer)
        if answer_length < 50:
            length_score = 0.3
        elif answer_length < 100:
            length_score = 0.6
        elif answer_length < 500:
            length_score = 1.0
        elif answer_length < 1000:
            length_score = 0.9
        else:
            length_score = 0.8
        
        # 3. "ì •ë³´ ì—†ìŒ" í‚¤ì›Œë“œ ê°ì§€
        negative_keywords = ["ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤", "ì£„ì†¡í•©ë‹ˆë‹¤"]
        has_negative = any(keyword in answer for keyword in negative_keywords)
        negative_penalty = 0.5 if has_negative else 1.0
        
        # ìµœì¢… ì‹ ë¢°ë„ ì ìˆ˜ (0-100)
        confidence = (doc_score * 0.4 + length_score * 0.4 + negative_penalty * 0.2) * 100
        return round(confidence, 1)
    
    def query_stream(self, question: str, chat_history: List[Dict[str, str]] = None) -> Iterator[str]:
        overall_start = time.perf_counter()
        try:
            formatted_history = self._format_chat_history(chat_history or [])

            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ë¡œê·¸ í¬í•¨)
            context = self._get_context(question)

            # ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¡°í•© í›„ ë¡œê·¸ ì¶œë ¥
            prompt_text = self.prompt.format(
                chat_history=formatted_history,
                context=context,
                question=question
            )
            print("[Prompt] ---------- START ----------")
            print(prompt_text)
            print("[Prompt] ----------- END -----------")

            chain_start = time.perf_counter()
            first_chunk = True
            for chunk in self.llm.stream(prompt_text):
                # chunk íƒ€ì…ë³„ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                if hasattr(chunk, "content") and isinstance(chunk.content, str):
                    text = chunk.content
                elif hasattr(chunk, "text") and isinstance(chunk.text, str):
                    text = chunk.text
                else:
                    text = str(chunk)

                if text:
                    if first_chunk:
                        print(f"[Timing] LLM first token delay: {time.perf_counter() - chain_start:.2f}s")
                        first_chunk = False
                    yield text

            print(f"[Timing] LLM streaming total: {time.perf_counter() - chain_start:.2f}s")
            print(f"[Timing] query_stream total: {time.perf_counter() - overall_start:.2f}s")
        except Exception as e:
            print(f"[Timing] query_stream total: {time.perf_counter() - overall_start:.2f}s (error)")
            yield f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def get_source_documents(self, question: str = None) -> List[Dict[str, Any]]:
        """ìºì‹œëœ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¶œì²˜ë¡œ ë°˜í™˜ (ë‹µë³€ ìƒì„±ì— ì‹¤ì œ ì‚¬ìš©ëœ ë¬¸ì„œ)"""
        try:
            if not self._last_retrieved_docs:
                return []
            
            # ìºì‹œëœ ë¬¸ì„œì— ì ìˆ˜ ì •ê·œí™” ì ìš©
            is_reranker = self.use_reranker
            probs = self._normalize_scores(self._last_retrieved_docs, is_reranker=is_reranker)
            
            sources = []
            for (doc, raw_score), normalized_score in zip(self._last_retrieved_docs, probs):
                # 15% ì„ê³„ê°’ ì œê±° - ì‹¤ì œ ì‚¬ìš©ëœ ë¬¸ì„œëŠ” ëª¨ë‘ í‘œì‹œ
                sources.append({
                    "file_name": doc.metadata.get("file_name", "Unknown"),
                    "page_number": doc.metadata.get("page_number", "Unknown"),
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "similarity_score": float(round(normalized_score, 1)),
                    "raw_score": float(round(raw_score, 4))  # ë””ë²„ê¹…ìš©
                })
            
            return sources
        except Exception as e:
            print(f"ì¶œì²˜ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def clear_memory(self):
        pass
    
    def update_llm(self, llm_api_type: str, llm_base_url: str, llm_model: str, 
                   llm_api_key: str = "", temperature: float = 0.7):
        self.llm_api_type = llm_api_type
        self.llm_base_url = llm_base_url
        self.llm_model = llm_model
        self.llm_api_key = llm_api_key
        self.temperature = temperature
        self.llm = self._create_llm()
        self.chain = (
            {
                "context": lambda x: self._get_context(x["question"]),
                "chat_history": lambda x: x.get("chat_history", "ì´ì „ ëŒ€í™” ì—†ìŒ"),
                "question": lambda x: x["question"]
            }
            | self.prompt
            | self.llm
            | StrOutputParser()
        )
    
    def update_retriever(self, vectorstore, top_k: int = 3):
        self.vectorstore = vectorstore
        self.top_k = top_k
        self.retriever = vectorstore.as_retriever(
            search_kwargs={"k": max(top_k * 5, 20)}
        )
        self.chain = (
            {
                "context": lambda x: self._get_context(x["question"]),
                "chat_history": lambda x: x.get("chat_history", "ì´ì „ ëŒ€í™” ì—†ìŒ"),
                "question": lambda x: x["question"]
            }
            | self.prompt
            | self.llm
            | StrOutputParser()
        )

    def _to_percentage(self, scores: List[float], is_reranker: bool) -> List[float]:
        """ì ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¥¼ 0~100%ë¡œ ì •ê·œí™”"""
        if not scores:
            return []
        if is_reranker:
            mn = min(scores)
            mx = max(scores)
            if abs(mx - mn) < 1e-9:
                return [50.0 for _ in scores]
            return [max(0.0, min(100.0, (s - mn) / (mx - mn) * 100.0)) for s in scores]
        # ë²¡í„° ê²€ìƒ‰: ê±°ë¦¬ê°€ 0~2 (ì‘ì„ìˆ˜ë¡ ìœ ì‚¬) ê°€ì • â†’ ìœ ì‚¬ë„ë¡œ ë³€í™˜
        return [max(0.0, min(100.0, (2.0 - s) / 2.0 * 100.0)) for s in scores]

    def _normalize_scores(self, pairs: List[tuple], is_reranker: bool) -> List[float]:
        """(doc, raw_score) -> 0~100% í™•ë¥ í˜• ì ìˆ˜ë¡œ ë³´ì • (ê°œì„  ë²„ì „)
        - reranker: Z-score ì •ê·œí™” í›„ Min-Maxë¡œ [0, 1] ë³€í™˜
        - vector:   í•˜ì´í¼ë³¼ë¦­ ë³€í™˜ + Min-Max ì •ê·œí™” í›„ softmax
        """
        import math
        import statistics
        
        if not pairs:
            return []
        
        raw = [float(s) for _, s in pairs]
        
        if is_reranker:
            # Reranker ì ìˆ˜: ì¼ë°˜ì ìœ¼ë¡œ ì–‘ìˆ˜ì´ë©° í° ê°’ì´ ì¢‹ìŒ
            # ìŒìˆ˜ ê°’ í•„í„°ë§ ë° ì •ê·œí™”
            filtered_scores = [s for s in raw if s > 0]
            
            if not filtered_scores:
                # ëª¨ë“  ì ìˆ˜ê°€ 0 ì´í•˜ì¸ ê²½ìš° ê· ë“± ë¶„ë°°
                return [50.0] * len(pairs)
            
            # Z-score ì •ê·œí™”
            mean_score = statistics.mean(filtered_scores)
            try:
                stdev_score = statistics.stdev(filtered_scores) if len(filtered_scores) > 1 else 1.0
            except:
                stdev_score = 1.0
            
            z_scores = []
            for s in raw:
                if s > 0 and stdev_score > 0:
                    z = (s - mean_score) / stdev_score
                    z_scores.append(z)
                else:
                    z_scores.append(-2.0)  # ìŒìˆ˜ ì ìˆ˜ëŠ” ë‚®ì€ Z-score
            
            # Z-scoreë¥¼ [0, 1] ë²”ìœ„ë¡œ Min-Max ì •ê·œí™”
            min_z = min(z_scores)
            max_z = max(z_scores)
            z_range = max_z - min_z if max_z > min_z else 1.0
            
            normalized = []
            for z in z_scores:
                if z_range > 0:
                    norm_val = (z - min_z) / z_range
                else:
                    norm_val = 0.5
                normalized.append(max(0.0, min(1.0, norm_val)))
            
            # Softmax ì ìš© (ë” ë¶€ë“œëŸ¬ìš´ í™•ë¥  ë¶„í¬)
            mx = max(normalized)
            exps = [math.exp(v - mx) for v in normalized]
            Z = sum(exps) or 1.0
            probs = [min(100.0, max(0.0, 100.0 * v / Z)) for v in exps]
        else:
            # Vector ê²€ìƒ‰: ê±°ë¦¬ ê¸°ë°˜ ì ìˆ˜ (ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ)
            # ìŒìˆ˜ ê°’ ì²˜ë¦¬ ë° í•˜ì´í¼ë³¼ë¦­ ë³€í™˜
            sims = []
            for s in raw:
                if s >= 0:
                    # ê±°ë¦¬ â†’ ìœ ì‚¬ë„ ë³€í™˜: similarity = 1 / (1 + distance)
                    sim = 1.0 / (1.0 + s)
                else:
                    # ìŒìˆ˜ ê±°ë¦¬ëŠ” ë¹„ì •ìƒ â†’ ë‚®ì€ ìœ ì‚¬ë„
                    sim = 0.01
                sims.append(sim)
            
            # Min-Max ì •ê·œí™”
            min_sim = min(sims)
            max_sim = max(sims)
            sim_range = max_sim - min_sim if max_sim > min_sim else 1.0
            
            normalized = []
            for sim in sims:
                if sim_range > 0:
                    norm_val = (sim - min_sim) / sim_range
                else:
                    norm_val = 0.5
                normalized.append(max(0.0, min(1.0, norm_val)))
            
            # Softmax ì ìš©
            mx = max(normalized)
            exps = [math.exp(v - mx) for v in normalized]
            Z = sum(exps) or 1.0
            probs = [min(100.0, max(0.0, 100.0 * v / Z)) for v in exps]

        return probs

    # ========================================
    # Phase A-2: Source Citation Enhancement
    # NotebookLM-style inline citations
    # ========================================

    def _split_sentences(self, text: str) -> List[str]:
        """ë‹µë³€ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬

        í•œê¸€/ì˜ë¬¸ ë¬¸ì¥ êµ¬ë¶„ ê³ ë ¤:
        - ë§ˆì¹¨í‘œ(.), ë¬¼ìŒí‘œ(?), ëŠë‚Œí‘œ(!)
        - ë‹¨, "Dr.", "Mr.", "etc." ë“±ì€ ì œì™¸

        Args:
            text: ë¶„ë¦¬í•  í…ìŠ¤íŠ¸

        Returns:
            ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
        """
        if not text:
            return []

        # 1. íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ë³´í˜¸ (Dr., Mr. ë“±)
        text = re.sub(r'(Dr|Mr|Ms|Mrs|etc)\.', r'\1<DOT>', text)

        # 2. ë¬¸ì¥ ë¶„ë¦¬ (., ?, !) - êµ¬ë¶„ìë„ í•¨ê»˜ ìº¡ì²˜
        sentences = re.split(r'([.!?])\s+', text)

        # 3. ì¬ì¡°í•© (êµ¬ë¶„ìì™€ ë¬¸ì¥ì„ ë‹¤ì‹œ í•©ì¹¨)
        result = []
        for i in range(0, len(sentences)-1, 2):
            if i+1 < len(sentences):
                sentence = sentences[i] + sentences[i+1]
            else:
                sentence = sentences[i]
            result.append(sentence)

        # ë§ˆì§€ë§‰ ë¬¸ì¥ ì¶”ê°€ (êµ¬ë¶„ì ì—†ì´ ëë‚˜ëŠ” ê²½ìš°)
        if len(sentences) % 2 == 1:
            result.append(sentences[-1])

        # 4. <DOT> ë³µì›
        result = [s.replace('<DOT>', '.') for s in result]

        # 5. ë¹ˆ ë¬¸ì¥ ì œê±° ë° trim
        return [s.strip() for s in result if s.strip()]

    def _embed_text(self, text: str) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜

        ê¸°ì¡´ vectorstoreì˜ ì„ë² ë”© ëª¨ë¸ ì¬ì‚¬ìš©

        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° (numpy array)
        """
        if not text:
            return np.zeros(1024)  # ê¸°ë³¸ ì°¨ì› (mxbai-embed-large)

        try:
            # VectorStoreManagerì˜ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
            embedding_model = self.vectorstore.embeddings

            # í…ìŠ¤íŠ¸ ì„ë² ë”©
            embedding = embedding_model.embed_query(text)
            return np.array(embedding)
        except Exception as e:
            print(f"    âš ï¸ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            return np.zeros(1024)  # ê¸°ë³¸ ì°¨ì›

    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°

        Args:
            vec1: ì²« ë²ˆì§¸ ë²¡í„°
            vec2: ë‘ ë²ˆì§¸ ë²¡í„°

        Returns:
            ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (0.0 ~ 1.0)
        """
        # ì˜ë²¡í„° ì²´í¬
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)

        if norm1 == 0 or norm2 == 0:
            return 0.0

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = np.dot(vec1, vec2) / (norm1 * norm2)

        # 0~1 ë²”ìœ„ë¡œ clipping
        return float(max(0.0, min(1.0, similarity)))

    def _format_citation(self, source: Document) -> str:
        """ì¶œì²˜ë¥¼ NotebookLM ìŠ¤íƒ€ì¼ë¡œ í¬ë§·

        í˜•ì‹: [íŒŒì¼ëª…, p.í˜ì´ì§€, ì‹ ë¢°ë„: ì ìˆ˜]

        Args:
            source: ì¶œì²˜ ë¬¸ì„œ

        Returns:
            í¬ë§·ëœ ì¶œì²˜ ë¬¸ìì—´
        """
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        file_name = source.metadata.get('file_name', 'Unknown')
        page = source.metadata.get('page_number', '?')

        # Document tupleì—ì„œ score ì¶”ì¶œ ì‹œë„
        score = source.metadata.get('score', 0.0)

        # ì§§ì€ íŒŒì¼ëª… ì¶”ì¶œ (í™•ì¥ì ì œê±°)
        short_name = file_name.rsplit('.', 1)[0]

        # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸° (30ì ì œí•œ)
        if len(short_name) > 30:
            short_name = short_name[:27] + "..."

        # ì¶œì²˜ í¬ë§·
        citation = f"[{short_name}, p.{page}]"

        return citation

    def _find_best_source_for_sentence(self, sentence: str, sources: List[Document]) -> Optional[Document]:
        """ë¬¸ì¥ê³¼ ê°€ì¥ ê´€ë ¨ëœ ì¶œì²˜ ì°¾ê¸°

        ë°©ë²•:
        1. ë¬¸ì¥ê³¼ ê° ì¶œì²˜ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        2. ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ ì¶œì²˜ ì„ íƒ
        3. ìœ ì‚¬ë„ê°€ ì„ê³„ê°’(0.4) ì´í•˜ë©´ None ë°˜í™˜

        Args:
            sentence: ë¶„ì„í•  ë¬¸ì¥
            sources: í›„ë³´ ì¶œì²˜ ë¬¸ì„œë“¤

        Returns:
            ê°€ì¥ ê´€ë ¨ëœ ì¶œì²˜ (ë˜ëŠ” None)
        """
        if not sources or not sentence:
            return None

        # 1. ë¬¸ì¥ ì„ë² ë”©
        sentence_embedding = self._embed_text(sentence)

        # 2. ê° ì¶œì²˜ì™€ ìœ ì‚¬ë„ ê³„ì‚°
        best_source = None
        best_similarity = 0.0

        for source in sources:
            # ì¶œì²˜ í…ìŠ¤íŠ¸ ì„ë² ë”© (ì²˜ìŒ 500ìë§Œ - ì„±ëŠ¥ ìµœì í™”)
            source_text = source.page_content[:500] if len(source.page_content) > 500 else source.page_content
            source_embedding = self._embed_text(source_text)

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            similarity = self._cosine_similarity(sentence_embedding, source_embedding)

            # ì„ê³„ê°’ ì²´í¬ (0.4)
            if similarity > best_similarity and similarity > 0.4:
                best_similarity = similarity
                best_source = source

        return best_source

    def _generate_source_citations(self, answer: str, sources: List[Document]) -> str:
        """NotebookLM ìŠ¤íƒ€ì¼ ì¶œì²˜ ì¸ë¼ì¸ í‘œì‹œ

        Args:
            answer: ìƒì„±ëœ ë‹µë³€
            sources: ì‚¬ìš©ëœ ì¶œì²˜ ë¬¸ì„œë“¤

        Returns:
            ì¶œì²˜ê°€ ì¸ë¼ì¸ìœ¼ë¡œ í‘œì‹œëœ ë‹µë³€
        """
        if not sources or not answer:
            return answer

        print(f"  ğŸ“ Citation ìƒì„± ì¤‘... (ë¬¸ì„œ {len(sources)}ê°œ)")

        # 1. ë‹µë³€ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        sentences = self._split_sentences(answer)
        print(f"    âœ“ ë¬¸ì¥ ë¶„ë¦¬: {len(sentences)}ê°œ")

        # 2. ê° ë¬¸ì¥ì— ì¶œì²˜ ë§¤ì¹­
        cited_sentences = []
        citation_count = 0

        for i, sentence in enumerate(sentences):
            # ë¬¸ì¥ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ skip (ì ‘ì†ì‚¬, ì „í™˜ì–´ ë“±)
            if len(sentence) < 15:
                cited_sentences.append(sentence)
                continue

            # ë¬¸ì¥ê³¼ ê°€ì¥ ê´€ë ¨ëœ ì¶œì²˜ ì°¾ê¸°
            best_source = self._find_best_source_for_sentence(sentence, sources)

            if best_source:
                # ì¸ë¼ì¸ ì¶œì²˜ ìƒì„±
                citation = self._format_citation(best_source)
                cited_sentence = f"{sentence.strip()} {citation}"
                citation_count += 1
            else:
                cited_sentence = sentence.strip()

            cited_sentences.append(cited_sentence)

        print(f"    âœ“ Citation ì¶”ê°€: {citation_count}/{len(sentences)}ê°œ ë¬¸ì¥")

        return " ".join(cited_sentences)

