# RAG 성능 테스트 계획 (품질 중심)

**작성일**: 2025-11-12  
**버전**: v3.7.0  
**목적**: 다양한 질문 유형에 대한 응답 품질 평가 및 개선

---

## 🎯 테스트 목표

### 핵심 목표
**논문 DB 기반 RAG 시스템의 실제 사용 시나리오 검증**

1. **특정 논문에 대한 질문**: 특정 논문의 내용 이해 및 요약
2. **주제/키워드 기반 논문 검색**: 특정 주제나 키워드로 관련 논문 찾기
3. **여러 논문에서 인사이트 추출**: 여러 논문의 공통점/차이점 분석

### 평가 기준 (시간보다 품질 우선)
- ✅ **답변 정확도**: 답변이 문서 내용과 일치하는가?
- ✅ **답변 완전성**: 질문에 대한 답변이 충분히 완전한가?
- ✅ **출처 다양성**: 여러 논문에서 정보를 종합했는가?
- ✅ **Citation 품질**: 인용이 정확하고 적절한가?
- ✅ **인사이트 추출**: 단순 요약을 넘어 인사이트를 제공하는가?

---

## 📋 테스트 케이스 설계

### Phase 1: 특정 논문에 대한 질문 (Single Paper Queries)

**목적**: 특정 논문의 내용을 정확히 이해하고 요약하는 능력 검증

#### 1.1 논문 요약 질문
```
테스트 케이스:
1. "[논문명]의 주요 내용을 요약해줘"
2. "[논문명]에서 제시한 핵심 방법론은?"
3. "[논문명]의 실험 결과는?"

평가 기준:
- 논문의 핵심 내용을 정확히 반영하는가?
- 주요 섹션(서론, 방법론, 결과, 결론)을 포함하는가?
- Citation이 해당 논문을 정확히 가리키는가?
```

#### 1.2 논문 내 특정 정보 추출
```
테스트 케이스:
1. "[논문명]에서 제시한 효율 수치는?"
2. "[논문명]에서 사용한 재료는?"
3. "[논문명]의 실험 조건은?"

평가 기준:
- 정확한 수치/정보를 추출하는가?
- Citation이 해당 페이지를 정확히 가리키는가?
- 단순 복사가 아닌 이해 기반 설명인가?
```

#### 1.3 논문의 기술적 개념 설명
```
테스트 케이스:
1. "[논문명]에서 제시한 Hyperfluorescence 기술이란?"
2. "[논문명]의 TADF 메커니즘은?"
3. "[논문명]에서 설명한 발광 원리는?"

평가 기준:
- 기술적 개념을 정확히 설명하는가?
- 논문의 설명을 왜곡하지 않는가?
- 전문 용어를 적절히 사용하는가?
```

**예상 테스트 수**: 9개 케이스

---

### Phase 2: 주제/키워드 기반 논문 검색 (Topic/Keyword Search)

**목적**: 특정 주제나 키워드로 관련 논문을 찾고 요약하는 능력 검증

#### 2.1 단일 키워드 검색
```
테스트 케이스:
1. "OLED 효율 향상에 대한 논문을 찾아줘"
2. "TADF 재료 관련 연구를 찾아줘"
3. "Hyperfluorescence 기술 논문을 찾아줘"

평가 기준:
- 관련 논문을 정확히 찾는가?
- 논문의 관련성을 설명하는가?
- 여러 논문을 제시하는가? (다양성)
```

#### 2.2 복합 키워드 검색
```
테스트 케이스:
1. "OLED와 TADF를 결합한 연구를 찾아줘"
2. "고효율 유기 발광 재료 논문을 찾아줘"
3. "양자점 기반 디스플레이 기술 논문을 찾아줘"

평가 기준:
- 복합 조건을 정확히 이해하는가?
- 관련 논문을 찾는가?
- 각 논문의 관련성을 설명하는가?
```

#### 2.3 Exhaustive Query (모든 논문 찾기)
```
테스트 케이스:
1. "모든 OLED 논문을 찾아줘"
2. "TADF 관련 전체 논문 리스트를 보여줘"
3. "Hyperfluorescence 기술 논문 전체를 찾아줘"

평가 기준:
- 파일 리스트 형식으로 반환하는가?
- 관련 논문을 모두 포함하는가?
- 순위가 직관적인가? (관련도 순)
```

**예상 테스트 수**: 9개 케이스

---

### Phase 3: 여러 논문에서 인사이트 추출 (Multi-Paper Insights)

**목적**: 여러 논문의 정보를 종합하여 인사이트를 제공하는 능력 검증

#### 3.1 비교 분석 질문
```
테스트 케이스:
1. "OLED와 QLED 기술의 차이점을 비교해줘"
2. "TADF와 형광 재료의 장단점을 비교해줘"
3. "Hyperfluorescence와 기존 기술의 차이는?"

평가 기준:
- 여러 논문에서 정보를 종합하는가?
- 각 기술의 특징을 정확히 비교하는가?
- Citation이 여러 논문을 가리키는가?
- 단순 나열이 아닌 통합적 분석인가?
```

#### 3.2 트렌드 분석 질문
```
테스트 케이스:
1. "OLED 효율 향상 기술의 발전 추세는?"
2. "최근 TADF 재료 연구 동향은?"
3. "Hyperfluorescence 기술의 연구 방향은?"

평가 기준:
- 시간 순서를 고려하는가?
- 발전 추세를 정확히 파악하는가?
- 여러 논문의 공통점을 찾는가?
- 인사이트를 제공하는가?
```

#### 3.3 공통점/차이점 분석
```
테스트 케이스:
1. "여러 논문에서 공통으로 언급하는 OLED 효율 향상 방법은?"
2. "TADF 재료 연구에서 가장 많이 사용되는 방법론은?"
3. "Hyperfluorescence 논문들의 공통 실험 조건은?"

평가 기준:
- 여러 논문의 공통점을 찾는가?
- 차이점을 명확히 구분하는가?
- Citation이 여러 논문을 가리키는가?
- 통계적 인사이트를 제공하는가?
```

#### 3.4 종합 분석 질문
```
테스트 케이스:
1. "OLED 효율을 높이기 위한 최적의 방법은?"
2. "TADF와 Hyperfluorescence 중 어떤 기술이 더 유망한가?"
3. "최근 연구 동향을 바탕으로 향후 발전 방향은?"

평가 기준:
- 여러 논문의 정보를 종합하는가?
- 근거를 제시하는가?
- 인사이트를 제공하는가?
- Citation이 충분한가?
```

**예상 테스트 수**: 12개 케이스

---

### Phase 4: 엣지 케이스 및 한계 테스트 (Edge Cases)

**목적**: 시스템의 한계 파악 및 개선점 발견

#### 4.1 모호한 질문
```
테스트 케이스:
1. "OLED는?"
2. "효율은?"
3. "최신 연구는?"

평가 기준:
- 모호함을 인지하고 명확히 하는가?
- 적절한 답변을 제공하는가?
- 사용자에게 추가 정보를 요청하는가?
```

#### 4.2 존재하지 않는 정보 질문
```
테스트 케이스:
1. "양자 컴퓨터 관련 논문은?"
2. "블록체인 기술 논문은?"
3. "인공지능 논문은?"

평가 기준:
- 존재하지 않음을 정확히 알리는가?
- 관련 없는 정보를 제공하지 않는가?
- 적절한 안내 메시지를 제공하는가?
```

#### 4.3 복잡한 다단계 질문
```
테스트 케이스:
1. "OLED 효율이 가장 높은 논문을 찾고, 그 논문에서 사용한 재료를 분석해줘"
2. "TADF 재료 논문 중 최신 논문을 찾고, 기존 연구와 비교해줘"

평가 기준:
- 다단계 질문을 정확히 이해하는가?
- 각 단계를 순차적으로 처리하는가?
- 최종 답변이 모든 단계를 포함하는가?
```

**예상 테스트 수**: 9개 케이스

---

## 📊 평가 메트릭

### 1. 답변 정확도 (Answer Accuracy)
**정의**: 답변이 문서 내용과 일치하는 정도

**측정 방법**:
- 수동 평가: 각 답변을 문서와 대조하여 정확도 점수 부여 (0-100)
- 자동 평가: 답변과 문서 간 의미 유사도 계산 (BERTScore 등)

**목표**: 90% 이상

### 2. 답변 완전성 (Answer Completeness)
**정의**: 질문에 대한 답변이 충분히 완전한 정도

**측정 방법**:
- 질문의 의도와 답변의 일치도 평가
- 필수 정보 포함 여부 체크리스트

**목표**: 85% 이상

### 3. 출처 다양성 (Source Diversity)
**정의**: 답변에 사용된 고유 논문 수

**측정 방법**:
- Citation에서 고유 파일명 추출
- 평균 고유 논문 수 계산

**목표**: 
- 단일 논문 질문: 1개 (정확도 우선)
- 다중 논문 질문: 평균 2.5개 이상

### 4. Citation 품질 (Citation Quality)
**정의**: Citation의 정확성과 적절성

**측정 방법**:
- Citation Coverage: 답변 문장 중 Citation이 있는 비율
- Citation Accuracy: Citation이 해당 내용을 정확히 가리키는 비율

**목표**:
- Citation Coverage: 95% 이상
- Citation Accuracy: 90% 이상

### 5. 인사이트 추출 (Insight Extraction)
**정의**: 단순 요약을 넘어 인사이트를 제공하는 정도

**측정 방법**:
- 수동 평가: 답변이 단순 요약인지 인사이트를 제공하는지 판단
- 비교 분석: 여러 논문의 공통점/차이점을 찾는 정도

**목표**: 다중 논문 질문에서 80% 이상

---

## 🧪 테스트 실행 계획

### Step 1: 테스트 케이스 준비 (1일)
1. 실제 논문 DB에서 테스트용 논문 선정 (10-20개)
2. 각 Phase별 테스트 케이스 작성
3. Ground Truth 답변 준비 (수동 작성 또는 검증)

### Step 2: 자동화 테스트 스크립트 작성 (1일)
```python
# test_rag_quality.py 구조
class RAGQualityTester:
    def test_single_paper_queries(self):
        # Phase 1 테스트 실행
        pass
    
    def test_topic_keyword_search(self):
        # Phase 2 테스트 실행
        pass
    
    def test_multi_paper_insights(self):
        # Phase 3 테스트 실행
        pass
    
    def evaluate_answer_quality(self, answer, sources, ground_truth):
        # 품질 평가 메트릭 계산
        pass
```

### Step 3: 테스트 실행 (2-3일)
1. 각 Phase별 순차 실행
2. 결과 로깅 (JSON 형식)
3. 중간 결과 분석 및 이슈 기록

### Step 4: 결과 분석 및 개선 (2일)
1. 메트릭별 성능 분석
2. 실패 케이스 원인 분석
3. 개선 방안 도출

---

## 📈 예상 결과 및 개선 목표

### 현재 예상 성능 (Phase 3 Day 2 기준)
- 답변 정확도: 85-90%
- 답변 완전성: 75-80%
- 출처 다양성: 평균 2.4개 (목표 2.5개)
- Citation Coverage: 95% (목표 달성)
- 인사이트 추출: 70-75%

### 개선 목표 (이번 테스트 후)
- 답변 정확도: 90% 이상
- 답변 완전성: 85% 이상
- 출처 다양성: 평균 2.5개 이상
- Citation Coverage: 95% 유지
- 인사이트 추출: 80% 이상

---

## 🔧 테스트 환경

### 필수 요구사항
- 논문 DB: 최소 20개 논문 (다양한 주제)
- LLM: 안정적인 모델 (gpt-4o-mini 또는 llama-4-scout)
- 임베딩: mxbai-embed-large
- Re-ranker: multilingual-mini

### 테스트 데이터 구조
```
test_data/
├── papers/              # 테스트용 논문 파일들
│   ├── OLED_paper1.pdf
│   ├── OLED_paper2.pdf
│   ├── TADF_paper1.pdf
│   └── ...
├── test_cases/          # 테스트 케이스 정의
│   ├── phase1_single_paper.json
│   ├── phase2_topic_search.json
│   ├── phase3_multi_paper.json
│   └── phase4_edge_cases.json
└── ground_truth/        # 정답 답변 (수동 작성)
    ├── phase1_answers.json
    ├── phase2_answers.json
    └── ...
```

---

## 📝 테스트 결과 보고서 형식

### 보고서 구조
```json
{
  "test_metadata": {
    "date": "2025-11-12",
    "version": "v3.7.0",
    "total_tests": 39,
    "test_duration": "2시간 30분"
  },
  "overall_metrics": {
    "answer_accuracy": 87.5,
    "answer_completeness": 82.3,
    "source_diversity": 2.4,
    "citation_coverage": 95.2,
    "insight_extraction": 76.8
  },
  "phase_results": {
    "phase1": { ... },
    "phase2": { ... },
    "phase3": { ... },
    "phase4": { ... }
  },
  "failed_cases": [ ... ],
  "recommendations": [ ... ]
}
```

---

## 🎯 다음 단계

1. ✅ 테스트 계획 수립 완료
2. ⏳ 테스트 케이스 작성
3. ⏳ 테스트 스크립트 개발
4. ⏳ 테스트 실행 및 결과 분석

---

**작성자**: AI Assistant  
**검토 필요**: 실제 논문 DB 확인 후 테스트 케이스 조정 필요

