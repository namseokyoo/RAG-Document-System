import streamlit as st
import os
from datetime import datetime
import uuid

# ì˜¤í”„ë¼ì¸ ëª¨ë“œ ì„¤ì • (ì™¸ë¶€ ë„¤íŠ¸ì›Œí¬ ì˜ì¡´ì„± ì œê±°)
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_DATASETS_OFFLINE"] = "1"
os.environ["HF_HUB_OFFLINE"] = "1"

# íì‡„ë§ í™˜ê²½ì—ì„œ tiktoken ì˜¤ë¥˜ ë°©ì§€
os.environ["TIKTOKEN_CACHE_DIR"] = "./tiktoken_cache"
os.environ["OPENAI_API_BASE"] = "http://localhost:11434"

# ChromaDB í…”ë ˆë©”íŠ¸ë¦¬ ë¹„í™œì„±í™” (íì‡„ë§ í™˜ê²½)
os.environ["ANONYMIZED_TELEMETRY"] = "False"
os.environ["CHROMA_TELEMETRY"] = "False"

# PyTorch Hub ë¹„í™œì„±í™” (íì‡„ë§ í™˜ê²½)
os.environ["TORCH_HOME"] = "./torch_cache"
os.environ["HF_HOME"] = "./huggingface_cache"

# ì¶”ê°€ ì˜¤í”„ë¼ì¸ ëª¨ë“œ ì„¤ì •
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # ê²½ê³  ë©”ì‹œì§€ ë°©ì§€
os.environ["PYTHONWARNINGS"] = "ignore::UserWarning"  # ì‚¬ìš©ì ê²½ê³  ë¬´ì‹œ

from config import ConfigManager
from utils.document_processor import DocumentProcessor
from utils.vector_store import VectorStoreManager
from utils.rag_chain import RAGChain
from utils.chat_history import ChatHistoryManager

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="RAG ì‹œìŠ¤í…œ - Ollama",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì „ì—­ ìŠ¤íƒ€ì¼ ì„¤ì •
st.markdown("""
<style>
    /* ì „ì²´ ê¸€ì í¬ê¸° 15% ì¶•ì†Œ */
    html, body, [class*="css"] {
        font-size: 85% !important;
    }
    
    /* ì œëª© í¬ê¸° ì¡°ì • */
    h1 { font-size: 1.7rem !important; }
    h2 { font-size: 1.4rem !important; }
    h3 { font-size: 1.2rem !important; }
    
    /* ì„¸ì…˜ ë¦¬ìŠ¤íŠ¸ ê°„ê²© ì¤„ì´ê¸° */
    .stDivider {
        margin: 0.3rem 0 !important;
    }
    
    /* ë²„íŠ¼ í…ìŠ¤íŠ¸ í¬ê¸° */
    .stButton button {
        font-size: 0.85rem !important;
    }
    
    /* ìº¡ì…˜ í¬ê¸° */
    .caption {
        font-size: 0.75rem !important;
    }
    
    /* ì…ë ¥ í•„ë“œ */
    .stTextInput input, .stNumberInput input, .stSelectbox select {
        font-size: 0.85rem !important;
    }
    
    /* ì‚¬ì´ë“œë°” ì—¬ë°± ì¡°ì • */
    section[data-testid="stSidebar"] > div {
        padding-top: 1rem !important;
    }
    
    /* ì±„íŒ… ì…ë ¥ì°½ í•˜ë‹¨ ì—¬ë°± (í‘¸í„° ê³µê°„ í™•ë³´) */
    .stChatInputContainer {
        margin-bottom: 3rem !important;
    }
    
    /* ë©”ì¸ ì»¨í…ì¸  í•˜ë‹¨ ì—¬ë°± */
    .main .block-container {
        padding-bottom: 4rem !important;
    }
</style>
""", unsafe_allow_html=True)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())

if "messages" not in st.session_state:
    st.session_state.messages = []

if "config_manager" not in st.session_state:
    st.session_state.config_manager = ConfigManager()

if "vector_store" not in st.session_state:
    config = st.session_state.config_manager.get_all()
    st.session_state.vector_store = VectorStoreManager(
        embedding_api_type=config.get("embedding_api_type", "ollama"),
        embedding_base_url=config["embedding_base_url"],
        embedding_model=config["embedding_model"],
        embedding_api_key=config.get("embedding_api_key", ""),
        distance_function=config.get("chroma_distance_function", "l2")
    )

if "rag_chain" not in st.session_state:
    config = st.session_state.config_manager.get_all()
    multi_query_num = int(config.get("multi_query_num", 3))
    enable_multi_query = config.get("enable_multi_query", True) and multi_query_num > 0
    st.session_state.rag_chain = RAGChain(
        vectorstore=st.session_state.vector_store.get_vectorstore(),
        llm_api_type=config.get("llm_api_type", "ollama"),
        llm_base_url=config["llm_base_url"],
        llm_model=config["llm_model"],
        llm_api_key=config.get("llm_api_key", ""),
        temperature=config.get("temperature", 0.7),
        top_k=config["top_k"],
        # Re-ranker ì„¤ì • (ê¸°ë³¸ í™œì„±í™”)
        use_reranker=config.get("use_reranker", True),
        reranker_model=config.get("reranker_model", "multilingual-mini"),
        reranker_initial_k=config.get("reranker_initial_k", 20),
        # Query Expansion ì„¤ì •
        enable_synonym_expansion=config.get("enable_synonym_expansion", True),
        enable_multi_query=enable_multi_query,
        multi_query_num=multi_query_num
    )

if "chat_history_manager" not in st.session_state:
    st.session_state.chat_history_manager = ChatHistoryManager()

if "doc_processor" not in st.session_state:
    config = st.session_state.config_manager.get_all()
    # RAGChainì˜ LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ DocumentProcessorì— ì „ë‹¬ (ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ìš©)
    llm_client = st.session_state.rag_chain.llm if "rag_chain" in st.session_state else None
    st.session_state.doc_processor = DocumentProcessor(
        chunk_size=config["chunk_size"],
        chunk_overlap=config["chunk_overlap"],
        llm_client=llm_client
    )

# ë©”ì¸ íƒ€ì´í‹€
st.title("ğŸ“š ë¬¸ì„œ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ›ï¸ ì œì–´íŒ")
    
    # ==================== 1. ì„¸ì…˜ ê´€ë¦¬ ====================
    with st.expander("ğŸ’¬ ì„¸ì…˜ ê´€ë¦¬", expanded=True):
        # í˜„ì¬ ì„¸ì…˜ ì •ë³´ í‘œì‹œ
        current_session_info = st.session_state.chat_history_manager.get_all_sessions_with_info()
        current_session = next((s for s in current_session_info if s["session_id"] == st.session_state.session_id), None)
        
        if current_session:
            st.caption(f"ğŸ“Œ {current_session['title']}")
            st.caption(f"ğŸ’¬ ëŒ€í™” ìˆ˜: {len([m for m in st.session_state.messages if m['role'] == 'user'])}")
        else:
            st.caption(f"ğŸ“Œ ìƒˆ ì„¸ì…˜")
        
        # ìƒˆ ì„¸ì…˜ ì‹œì‘ ë²„íŠ¼
        if st.button("ğŸ†• ìƒˆ ì„¸ì…˜ ì‹œì‘", use_container_width=True, type="primary", key="new_session_btn"):
            # ìƒˆ ì„¸ì…˜ ID ìƒì„±
            st.session_state.session_id = str(uuid.uuid4())
            st.session_state.messages = []
            st.session_state.rag_chain.clear_memory()
            st.success("âœ… ìƒˆ ì„¸ì…˜ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.rerun()
        
        st.markdown("**ğŸ“‚ ê³¼ê±° ì„¸ì…˜**")
        
        all_sessions = st.session_state.chat_history_manager.get_all_sessions_with_info()
        
        if all_sessions:
            # í˜„ì¬ ì„¸ì…˜ ì œì™¸
            past_sessions = [s for s in all_sessions if s["session_id"] != st.session_state.session_id]
            
            if past_sessions:
                for idx, session in enumerate(past_sessions[:10]):  # ìµœê·¼ 10ê°œë§Œ í‘œì‹œ
                    col1, col2 = st.columns([5, 1])
                    
                    with col1:
                        # ì„¸ì…˜ ì œëª© ë° ì •ë³´ í‘œì‹œ
                        st.markdown(f"ğŸ“ **{session['title']}**")
                        st.caption(f"ğŸ’¬ {session['message_count']}ê°œ ëŒ€í™” | {session['timestamp'][:10]}")
                    
                    with col2:
                        # ë©”ë‰´ ë²„íŠ¼ (â‹®)
                        with st.popover("â‹®", use_container_width=True):
                            st.write("**ë©”ë‰´**")
                            
                            # ë¶ˆëŸ¬ì˜¤ê¸° ë²„íŠ¼
                            if st.button(
                                "ğŸ“‚ ë¶ˆëŸ¬ì˜¤ê¸°", 
                                key=f"load_{session['session_id']}",
                                use_container_width=True
                            ):
                                # ì„¸ì…˜ ë¡œë“œ
                                st.session_state.session_id = session['session_id']
                                st.session_state.messages = []
                                st.session_state.rag_chain.clear_memory()
                                st.success(f"âœ… '{session['title']}' ì„¸ì…˜ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤!")
                                st.rerun()
                            
                            # ì‚­ì œ ë²„íŠ¼
                            if st.button(
                                "ğŸ—‘ï¸ ì‚­ì œ", 
                                key=f"delete_{session['session_id']}",
                                use_container_width=True,
                                type="secondary"
                            ):
                                if st.session_state.chat_history_manager.clear_history(session['session_id']):
                                    st.success("âœ… ì‚­ì œ ì™„ë£Œ")
                                    st.rerun()
                    
                    # ë§ˆì§€ë§‰ í•­ëª©ì´ ì•„ë‹ ë•Œë§Œ êµ¬ë¶„ì„  í‘œì‹œ
                    if idx < len(past_sessions[:10]) - 1:
                        st.markdown("<hr style='margin: 0.5rem 0; border: 0; border-top: 1px solid #e0e0e0;'>", unsafe_allow_html=True)
            else:
                st.info("í˜„ì¬ ì„¸ì…˜ë§Œ ìˆìŠµë‹ˆë‹¤.")
        else:
            st.info("ì €ì¥ëœ ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ==================== 2. ì—…ë¡œë“œ ====================
    with st.expander("ğŸ“¤ ì—…ë¡œë“œ", expanded=False):
        # ê°„ì´ ë¬¸ì„œ ì‘ì„±
        st.markdown("**âœï¸ ê°„ì´ ë¬¸ì„œ ì‘ì„±**")
        doc_title = st.text_input(
            "ì œëª©",
            placeholder="ë¬¸ì„œ ì œëª©ì„ ì…ë ¥í•˜ì„¸ìš”...",
            key="quick_doc_title"
        )
        doc_content = st.text_area(
            "ë‚´ìš©",
            placeholder="ë¬¸ì„œ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”...",
            height=150,
            key="quick_doc_content"
        )
        
        if st.button("ğŸ“ ë¬¸ì„œ ì¶”ê°€", use_container_width=True, key="add_quick_doc_btn"):
            if doc_title and doc_content:
                try:
                    from langchain.schema import Document
                    from datetime import datetime
                    
                    # Document ê°ì²´ ìƒì„±
                    doc = Document(
                        page_content=doc_content,
                        metadata={
                            "file_name": f"{doc_title}.txt",
                            "file_type": "text",
                            "upload_time": datetime.now().isoformat(),
                            "page_number": 1,
                            "chunk_index": 0,
                            "total_chunks": 1
                        }
                    )
                    
                    # ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€
                    if st.session_state.vector_store.add_documents([doc]):
                        st.success(f"âœ… '{doc_title}' ë¬¸ì„œê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        # ì…ë ¥ í•„ë“œ ì´ˆê¸°í™”ë¥¼ ìœ„í•´ rerun
                        st.rerun()
                    else:
                        st.error("âŒ ë¬¸ì„œ ì¶”ê°€ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    st.error(f"âŒ ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {str(e)}")
            else:
                st.warning("âš ï¸ ì œëª©ê³¼ ë‚´ìš©ì„ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        st.markdown("---")
    
    # íŒŒì¼ ì—…ë¡œë“œ
        st.markdown("**ğŸ“‚ íŒŒì¼ ì—…ë¡œë“œ**")
    uploaded_files = st.file_uploader(
        "ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš” (PDF, PPT, Excel)",
        type=["pdf", "pptx", "xlsx", "xls"],
            accept_multiple_files=True,
            key="file_uploader"
    )
    
    if uploaded_files:
        if st.button("ğŸ“¥ ì—…ë¡œë“œ ë° ì²˜ë¦¬", type="primary", key="upload_btn"):
            with st.spinner("ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘..."):
                for uploaded_file in uploaded_files:
                    try:
                        # íŒŒì¼ ì €ì¥
                        file_path = os.path.join("data/uploaded_files", uploaded_file.name)
                        with open(file_path, "wb") as f:
                            f.write(uploaded_file.getbuffer())
                        
                        # íŒŒì¼ íƒ€ì… í™•ì¸
                        file_type = st.session_state.doc_processor.get_file_type(uploaded_file.name)
                        
                        # ë¬¸ì„œ ì²˜ë¦¬
                        chunks = st.session_state.doc_processor.process_document(
                            file_path, uploaded_file.name, file_type
                        )
                        
                        # ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€
                        st.session_state.vector_store.add_documents(chunks)
                        
                        st.success(f"âœ… {uploaded_file.name} ì²˜ë¦¬ ì™„ë£Œ ({len(chunks)} ì²­í¬)")
                    except Exception as e:
                        st.error(f"âŒ {uploaded_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            
            st.rerun()
    
        st.markdown("**ğŸ“‹ ì—…ë¡œë“œëœ íŒŒì¼**")
    documents = st.session_state.vector_store.get_documents_list()
    
    if documents:
        for doc in documents:
            col1, col2 = st.columns([3, 1])
            with col1:
                st.write(f"**{doc['file_name']}**")
                st.caption(f"íƒ€ì…: {doc['file_type']} | ì²­í¬: {doc['chunk_count']}")
                st.caption(f"ì—…ë¡œë“œ: {doc['upload_time'][:19]}")
            with col2:
                if st.button("ğŸ—‘ï¸", key=f"delete_{doc['file_name']}"):
                    if st.session_state.vector_store.delete_document(doc['file_name']):
                        st.success(f"âœ… {doc['file_name']} ì‚­ì œ ì™„ë£Œ")
                        st.rerun()
                    else:
                        st.error(f"âŒ {doc['file_name']} ì‚­ì œ ì‹¤íŒ¨")
                st.markdown("<hr style='margin: 0.5rem 0; border: 0; border-top: 1px solid #e0e0e0;'>", unsafe_allow_html=True)
    else:
        st.info("ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ==================== 3. ì„¤ì • ====================
    with st.expander("âš™ï¸ ì„¤ì •", expanded=False):
        # í˜„ì¬ ì„¤ì • ë¡œë“œ
        config = st.session_state.config_manager.get_all()
        
        st.markdown("**ğŸ¤– LLM ì„¤ì •**")
        llm_api_type = st.selectbox(
            "LLM API íƒ€ì…",
            options=["request", "ollama", "openai", "openai-compatible"],
            index=["request", "ollama", "openai", "openai-compatible"].index(config.get("llm_api_type", "request")),
            help="Request: HTTP ìš”ì²­ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ) / Ollama: LangChain ë˜í¼ / OpenAI: ê³µì‹ API / OpenAI Compatible: í˜¸í™˜ API",
            key="llm_api_type_select"
        )
        llm_base_url = st.text_input(
            "LLM ì„œë²„ ì£¼ì†Œ", 
            value=config["llm_base_url"],
            help="Ollama: http://localhost:11434 / OpenAI Compatible: ì‚¬ë‚´ API ì£¼ì†Œ",
            key="llm_base_url_input"
        )
        llm_model = st.text_input(
            "LLM ëª¨ë¸ëª…", 
            value=config["llm_model"],
            help="ì˜ˆ: llama3, gpt-4, gpt-3.5-turbo",
            key="llm_model_input"
        )
        llm_api_key = st.text_input(
            "LLM API í‚¤ (ì„ íƒì‚¬í•­)", 
            value=config.get("llm_api_key", ""),
            type="password",
            help="OpenAI API í‚¤ (OllamaëŠ” ë¶ˆí•„ìš”)",
            key="llm_api_key_input"
        )
        temperature = st.slider(
            "Temperature (ì˜¨ë„)",
            min_value=0.0,
            max_value=2.0,
            value=config.get("temperature", 0.7),
            step=0.1,
            help="ë‚®ìŒ(0.1-0.3): ì¼ê´€ì /ì •í™• | ì¤‘ê°„(0.5-0.7): ê· í˜• | ë†’ìŒ(0.8-1.0): ì°½ì˜ì /ë‹¤ì–‘",
            key="temperature_input"
        )
        
        st.markdown("**ğŸ” ì„ë² ë”© ì„¤ì •**")
        embedding_api_type = st.selectbox(
            "ì„ë² ë”© API íƒ€ì…",
            options=["request", "ollama", "openai", "openai-compatible"],
            index=["request", "ollama", "openai", "openai-compatible"].index(config.get("embedding_api_type", "ollama")),
            help="Request: HTTP ìš”ì²­ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ) / Ollama: LangChain ë˜í¼ / OpenAI: ê³µì‹ OpenAI API / OpenAI Compatible: í˜¸í™˜ API",
            key="embedding_api_type_select"
        )
        embedding_base_url = st.text_input(
            "ì„ë² ë”© ì„œë²„ ì£¼ì†Œ", 
            value=config["embedding_base_url"],
            help="Ollama: http://localhost:11434 / OpenAI Compatible: ì‚¬ë‚´ API ì£¼ì†Œ",
            key="embedding_base_url_input"
        )
        embedding_model = st.text_input(
            "ì„ë² ë”© ëª¨ë¸ëª…", 
            value=config["embedding_model"],
            help="ì˜ˆ: nomic-embed-text, text-embedding-ada-002",
            key="embedding_model_input"
        )
        embedding_api_key = st.text_input(
            "ì„ë² ë”© API í‚¤ (ì„ íƒì‚¬í•­)", 
            value=config.get("embedding_api_key", ""),
            type="password",
            help="OpenAI API í‚¤ (OllamaëŠ” ë¶ˆí•„ìš”)",
            key="embedding_api_key_input"
        )
        
        st.markdown("**ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ ì„¤ì •**")
        chunk_size = st.number_input("ì²­í¬ í¬ê¸°", min_value=100, max_value=2000, value=config["chunk_size"], key="chunk_size_input")
        if chunk_size != 1500:
            st.warning(f"âš ï¸ ê¶Œì¥ê°’ì€ 1500ì…ë‹ˆë‹¤. ë³€ê²½ ì‹œ DBë¥¼ ì¬êµ¬ì¶•í•´ì•¼ í•©ë‹ˆë‹¤!")
        chunk_overlap = st.number_input("ì²­í¬ ì˜¤ë²„ë©", min_value=0, max_value=500, value=config["chunk_overlap"], key="chunk_overlap_input")
        if chunk_overlap != 200:
            st.warning(f"âš ï¸ ê¶Œì¥ê°’ì€ 200ì…ë‹ˆë‹¤.")
        top_k = st.number_input("ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (Top K)", min_value=1, max_value=10, value=config["top_k"], key="top_k_input")
        
        # ì„¤ì • ì €ì¥ ë²„íŠ¼
        if st.button("ğŸ’¾ ì„¤ì • ì €ì¥", type="primary", use_container_width=True, key="save_config_btn"):
            new_config = {
                "llm_api_type": llm_api_type,
                "llm_base_url": llm_base_url,
                "llm_model": llm_model,
                "llm_api_key": llm_api_key,
                "temperature": temperature,
                "embedding_api_type": embedding_api_type,
                "embedding_base_url": embedding_base_url,
                "embedding_model": embedding_model,
                "embedding_api_key": embedding_api_key,
                "chunk_size": chunk_size,
                "chunk_overlap": chunk_overlap,
                "top_k": top_k
            }
            
            if st.session_state.config_manager.save_config(new_config):
                # ì„¤ì • ë³€ê²½ ì‹œ ì»´í¬ë„ŒíŠ¸ ì—…ë°ì´íŠ¸
                st.session_state.vector_store.update_embeddings(
                    embedding_api_type, embedding_base_url, embedding_model, embedding_api_key
                )
                st.session_state.rag_chain.update_llm(
                    llm_api_type, llm_base_url, llm_model, llm_api_key, temperature
                )
                st.session_state.rag_chain.update_retriever(
                    st.session_state.vector_store.get_vectorstore(),
                    top_k
                )
                # LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ DocumentProcessorì— ì „ë‹¬ (ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ìš©)
                st.session_state.doc_processor = DocumentProcessor(
                    chunk_size,
                    chunk_overlap,
                    llm_client=st.session_state.rag_chain.llm
                )
                st.success("âœ… ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.rerun()
            else:
                st.error("âŒ ì„¤ì • ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

# ë©”ì¸ ì˜ì—­ - ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
st.header("ğŸ’¬ ëŒ€í™”")

# ì„¸ì…˜ ì´ë ¥ ë¡œë“œ (ìµœì´ˆ 1íšŒ)
if not st.session_state.messages:
    history = st.session_state.chat_history_manager.load_history(st.session_state.session_id)
    for msg in history:
        st.session_state.messages.append({
            "role": "user",
            "content": msg["question"]
        })
        st.session_state.messages.append({
            "role": "assistant",
            "content": msg["answer"],
            "sources": msg.get("sources", [])
        })

# ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    if message["role"] == "user":
        # ì‚¬ìš©ì ë©”ì‹œì§€ - ì˜¤ë¥¸ìª½ ì •ë ¬
        st.markdown(f"""
        <div style="display: flex; justify-content: flex-end; align-items: flex-start; margin-bottom: 1rem; gap: 0.5rem;">
            <div style="max-width: 70%; background-color: #e3f2fd; border-radius: 15px; padding: 0.75rem 1rem; text-align: left;">
                {message["content"]}
            </div>
            <div style="width: 36px; height: 36px; border-radius: 50%; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); display: flex; align-items: center; justify-content: center; flex-shrink: 0; color: white; font-weight: bold; font-size: 1rem;">
                ğŸ‘¤
            </div>
        </div>
        """, unsafe_allow_html=True)
    else:
        # AI ë©”ì‹œì§€ - ì™¼ìª½ ì •ë ¬, ì „ì²´ ë„ˆë¹„
        with st.chat_message("assistant"):
            st.write(message["content"])
            
            # ì¶œì²˜ ì •ë³´ í‘œì‹œ
            if "sources" in message and message["sources"]:
                with st.expander("ğŸ“ ì¶œì²˜ ì •ë³´ (ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨)"):
                    for idx, source in enumerate(message["sources"], 1):
                        st.write(f"**{idx}. {source['file_name']}** (í˜ì´ì§€: {source['page_number']})")
                        
                        # ìœ ì‚¬ë„ ì ìˆ˜ê°€ ìˆëŠ” ê²½ìš° í‘œì‹œ
                        if 'similarity_score' in source:
                            score = source['similarity_score']
                            # Re-ranker ì ìˆ˜ (0~10, ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ) vs Vector Search distance (0~2, ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
                            if score > 3:  # Re-ranker ì ìˆ˜
                                similarity_percent = (score / 10) * 100  # 10ì  ë§Œì ì„ 100%ë¡œ ë³€í™˜
                            else:  # Vector Search distance
                                similarity_percent = max(0, 100 - (score * 20))
                            st.write(f"ğŸ¯ ìœ ì‚¬ë„: {similarity_percent:.1f}% (Re-rank ì ìˆ˜: {score:.4f})")
                        
                        st.caption(source['content'])
                        st.divider()

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ - ì˜¤ë¥¸ìª½ ì •ë ¬
    st.markdown(f"""
    <div style="display: flex; justify-content: flex-end; align-items: flex-start; margin-bottom: 1rem; gap: 0.5rem;">
        <div style="max-width: 70%; background-color: #e3f2fd; border-radius: 15px; padding: 0.75rem 1rem; text-align: left;">
            {prompt}
        </div>
        <div style="width: 36px; height: 36px; border-radius: 50%; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); display: flex; align-items: center; justify-content: center; flex-shrink: 0; color: white; font-weight: bold; font-size: 1rem;">
            ğŸ‘¤
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    # RAG ì²´ì¸ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±
    with st.chat_message("assistant"):
        # ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            # í˜„ì¬ ë©”ì‹œì§€ë¥¼ ì œì™¸í•œ ì´ì „ ëŒ€í™” ì´ë ¥ ì „ë‹¬
            chat_history = st.session_state.messages[:-1]  # ë°©ê¸ˆ ì¶”ê°€í•œ ì‚¬ìš©ì ë©”ì‹œì§€ ì œì™¸
            
            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹µë³€ í‘œì‹œ (ëŒ€í™” ì´ë ¥ í¬í•¨)
            for chunk in st.session_state.rag_chain.query_stream(prompt, chat_history):
                full_response += chunk
                message_placeholder.markdown(full_response + "â–Œ")
            
            # ìµœì¢… ë‹µë³€ í‘œì‹œ
            message_placeholder.markdown(full_response)
            
            # ì¶œì²˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨)
            sources = st.session_state.rag_chain.get_source_documents(prompt)
            
            # ì¶œì²˜ ì •ë³´ í‘œì‹œ
            if sources:
                with st.expander("ğŸ“ ì¶œì²˜ ì •ë³´ (ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨)"):
                    for idx, source in enumerate(sources, 1):
                        # ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ë°±ë¶„ìœ¨ë¡œ ë³€í™˜
                        score = source['similarity_score']
                        # Re-ranker ì ìˆ˜ (0~10, ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ) vs Vector Search distance (0~2, ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
                        if score > 3:  # Re-ranker ì ìˆ˜
                            similarity_percent = (score / 10) * 100
                        else:  # Vector Search distance
                            similarity_percent = max(0, 100 - (score * 20))
                        
                        st.write(f"**{idx}. {source['file_name']}** (í˜ì´ì§€: {source['page_number']})")
                        st.write(f"ğŸ¯ ìœ ì‚¬ë„: {similarity_percent:.1f}% (Re-rank ì ìˆ˜: {score:.4f})")
                        st.caption(source['content'])
                        st.divider()
            
            # ë©”ì‹œì§€ ì €ì¥
            st.session_state.messages.append({
                "role": "assistant",
                "content": full_response,
                "sources": sources
            })
            
            # ëŒ€í™” ì´ë ¥ íŒŒì¼ì— ì €ì¥
            st.session_state.chat_history_manager.save_message(
                st.session_state.session_id,
                prompt,
                full_response,
                sources
            )
            
        except Exception as e:
            error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            st.error(error_msg)
            st.session_state.messages.append({
                "role": "assistant",
                "content": error_msg
            })

# í‘¸í„° (í™”ë©´ í•˜ë‹¨ ê³ ì •)
st.markdown("""
<div style="
    position: fixed;
    bottom: 0;
    left: 0;
    width: 100%;
    background-color: var(--background-color);
    padding: 0.5rem 1rem;
    text-align: center;
    font-size: 0.75rem;
    color: var(--text-color);
    border-top: 1px solid #e0e0e0;
    z-index: 999;
">
    ğŸš€ ì „ë¬¸ ë¬¸ì„œ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ | LangChain + ChromaDB + Streamlit
</div>
""", unsafe_allow_html=True)

