# Phase 2.5 êµ¬í˜„ ê³„íšì„œ

> **ëª©í‘œ**: Phase 3 ì§„í–‰ ì „ ê¸°ì¡´ PDF/PPTX ì²˜ë¦¬ í’ˆì§ˆ í–¥ìƒ ë° ì‹œìŠ¤í…œ ìš´ì˜ ì•ˆì •í™”

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 1.5-2ì£¼
**ìš°ì„ ìˆœìœ„**: High
**ì‘ì„±ì¼**: 2025-01-09

---

## ğŸ“‹ Overview

Phase 2.5ëŠ” Excel ì§€ì›(Phase 3) ì „ì— í˜„ì¬ ì‹œìŠ¤í…œì˜ í’ˆì§ˆê³¼ ìš´ì˜ì„±ì„ ê°œì„ í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.

### í•µì‹¬ ëª©í‘œ
1. âœ… **Question Classifier ë¡œê¹…**: ì˜¤ë¶„ë¥˜ ì¼€ì´ìŠ¤ ë¶„ì„ ë° ê°œì„ 
2. âœ… **PDF Vision ì²˜ë¦¬**: ê·¸ë˜í”„/í‘œ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ
3. âœ… **Exhaustive Retrieval ìë™í™”**: í‚¤ì›Œë“œ ì˜ì¡´ì„± ì œê±°
4. âœ… **ChromaDB ë™ì‹œ ì ‘ì†**: ë„¤íŠ¸ì›Œí¬ í´ë” í™˜ê²½ ì•ˆì •í™”
5. âœ… **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: ë³‘ëª© ì§€ì  ì‹ë³„ ë° ìµœì í™”
6. ğŸ”„ **í”¼ë“œë°± ì‹œìŠ¤í…œ**: ì‚¬ìš©ì ë§Œì¡±ë„ ì¶”ì  (ì„ íƒ)

---

## ğŸ› ï¸ Task 1: Question Classifier ë¡œê¹… ì‹œìŠ¤í…œ

### ëª©í‘œ
ë¶„ë¥˜ê¸°ì˜ ë™ì‘ì„ ì¶”ì í•˜ì—¬ ì˜¤ë¶„ë¥˜ ì¼€ì´ìŠ¤ë¥¼ ì‹ë³„í•˜ê³  ê°œì„ í•©ë‹ˆë‹¤.

### êµ¬í˜„ ìƒì„¸

#### 1.1 ë¡œê¹… ì¸í”„ë¼ êµ¬ì¶•
**íŒŒì¼**: `utils/question_classifier.py`

```python
import logging
import json
from datetime import datetime
from pathlib import Path

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
LOG_DIR = Path("logs")
LOG_DIR.mkdir(exist_ok=True)

# ì „ìš© ë¡œê±° ìƒì„±
classifier_logger = logging.getLogger("question_classifier")
classifier_logger.setLevel(logging.INFO)
classifier_logger.propagate = False  # ìƒìœ„ ë¡œê±°ë¡œ ì „íŒŒ ë°©ì§€

# íŒŒì¼ í•¸ë“¤ëŸ¬ (JSONL í˜•ì‹)
fh = logging.FileHandler(LOG_DIR / "classifier_history.jsonl", encoding="utf-8")
fh.setLevel(logging.INFO)
formatter = logging.Formatter('%(message)s')
fh.setFormatter(formatter)
classifier_logger.addHandler(fh)

# ì½˜ì†” í•¸ë“¤ëŸ¬ (verbose ëª¨ë“œìš©)
ch = logging.StreamHandler()
ch.setLevel(logging.INFO)
classifier_logger.addHandler(ch)
```

#### 1.2 ë¶„ë¥˜ ë¡œê·¸ ê¸°ë¡
**ìˆ˜ì •**: `classify_question()` í•¨ìˆ˜

```python
def classify_question(
    query: str,
    use_llm: bool = True,
    verbose: bool = False
) -> dict:
    """
    ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ê³  ë¡œê·¸ ê¸°ë¡

    Returns:
        {
            "question_type": str,
            "confidence": float,
            "reasoning": str,
            "method": str  # "llm" | "rule"
        }
    """
    start_time = time.time()

    # ê¸°ì¡´ ë¶„ë¥˜ ë¡œì§
    result = _classify_internal(query, use_llm)

    # ë¡œê·¸ ì—”íŠ¸ë¦¬ ìƒì„±
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "query": query,
        "question_type": result["question_type"],
        "confidence": result.get("confidence", 1.0),
        "method": "llm" if use_llm else "rule",
        "reasoning": result.get("reasoning", ""),
        "processing_time_ms": int((time.time() - start_time) * 1000)
    }

    # JSONL ê¸°ë¡
    classifier_logger.info(json.dumps(log_entry, ensure_ascii=False))

    # ì½˜ì†” ì¶œë ¥ (verbose ëª¨ë“œ)
    if verbose:
        print(f"[Classifier] {query[:50]}... â†’ {result['question_type']} "
              f"(conf: {result.get('confidence', 1.0):.2f})")

    return result
```

#### 1.3 ë¶„ì„ ë„êµ¬ ê°œë°œ
**ì‹ ê·œ íŒŒì¼**: `scripts/analyze_classifier_logs.py`

```python
"""
Question Classifier ë¡œê·¸ ë¶„ì„ ë„êµ¬
Usage: python scripts/analyze_classifier_logs.py
"""
import json
from pathlib import Path
from collections import Counter, defaultdict
from datetime import datetime, timedelta

def analyze_classifier_logs(log_file: str = "logs/classifier_history.jsonl"):
    """ë¶„ë¥˜ê¸° ë¡œê·¸ ë¶„ì„"""

    if not Path(log_file).exists():
        print(f"âš ï¸  ë¡œê·¸ íŒŒì¼ ì—†ìŒ: {log_file}")
        return

    # ë¡œê·¸ ë¡œë“œ
    logs = []
    with open(log_file, "r", encoding="utf-8") as f:
        for line in f:
            try:
                logs.append(json.loads(line))
            except:
                continue

    if not logs:
        print("ğŸ“­ ë¡œê·¸ ë°ì´í„° ì—†ìŒ")
        return

    print(f"\n{'='*60}")
    print(f"Question Classifier ë¶„ì„ ë¦¬í¬íŠ¸")
    print(f"{'='*60}")
    print(f"ì´ ì¿¼ë¦¬ ìˆ˜: {len(logs)}")
    print(f"ë¶„ì„ ê¸°ê°„: {logs[0]['timestamp'][:10]} ~ {logs[-1]['timestamp'][:10]}")

    # 1. íƒ€ì…ë³„ ë¶„í¬
    print(f"\n[1] ì§ˆë¬¸ ìœ í˜• ë¶„í¬")
    type_dist = Counter(log['question_type'] for log in logs)
    for qtype, count in type_dist.most_common():
        pct = count / len(logs) * 100
        print(f"  {qtype:20s}: {count:4d} ({pct:5.1f}%)")

    # 2. ì‹ ë¢°ë„ ë¶„ì„
    print(f"\n[2] ì‹ ë¢°ë„ ë¶„ì„")
    confidences = [log.get('confidence', 1.0) for log in logs]
    avg_conf = sum(confidences) / len(confidences)
    print(f"  í‰ê·  ì‹ ë¢°ë„: {avg_conf:.3f}")

    low_conf_logs = [log for log in logs if log.get('confidence', 1.0) < 0.7]
    if low_conf_logs:
        pct = len(low_conf_logs) / len(logs) * 100
        print(f"  ë‚®ì€ ì‹ ë¢°ë„ (<0.7): {len(low_conf_logs)} ({pct:.1f}%)")

        print(f"\n  [ë‚®ì€ ì‹ ë¢°ë„ ìƒ˜í”Œ]")
        for log in low_conf_logs[:5]:
            print(f"    - {log['query'][:50]}...")
            print(f"      â†’ {log['question_type']} (conf: {log['confidence']:.2f})")
    else:
        print(f"  ë‚®ì€ ì‹ ë¢°ë„ ì¼€ì´ìŠ¤ ì—†ìŒ âœ…")

    # 3. ë©”ì†Œë“œ ë¶„í¬
    print(f"\n[3] ë¶„ë¥˜ ë°©ë²•")
    method_dist = Counter(log['method'] for log in logs)
    for method, count in method_dist.most_common():
        pct = count / len(logs) * 100
        print(f"  {method:10s}: {count:4d} ({pct:5.1f}%)")

    # 4. ì²˜ë¦¬ ì‹œê°„
    print(f"\n[4] ì²˜ë¦¬ ì„±ëŠ¥")
    times = [log.get('processing_time_ms', 0) for log in logs]
    avg_time = sum(times) / len(times)
    max_time = max(times)
    print(f"  í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time:.1f}ms")
    print(f"  ìµœëŒ€ ì²˜ë¦¬ ì‹œê°„: {max_time:.1f}ms")

    # 5. ì¼ë³„ íŠ¸ë Œë“œ
    print(f"\n[5] ì¼ë³„ ì¿¼ë¦¬ ìˆ˜")
    daily_counts = defaultdict(int)
    for log in logs:
        date = log['timestamp'][:10]
        daily_counts[date] += 1

    for date in sorted(daily_counts.keys())[-7:]:  # ìµœê·¼ 7ì¼
        count = daily_counts[date]
        print(f"  {date}: {count:3d} queries")

    print(f"\n{'='*60}\n")

if __name__ == "__main__":
    analyze_classifier_logs()
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 1ì¼

---

## ğŸ–¼ï¸ Task 2: PDF ê·¸ë˜í”„/í‘œ Vision ì²˜ë¦¬

### ëª©í‘œ
PDFì˜ ê·¸ë˜í”„, í‘œ, ë„ì‹ ì´ë¯¸ì§€ë¥¼ GPT-4o Visionìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ ê°€ëŠ¥í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.

### êµ¬í˜„ ìƒì„¸

#### 2.1 PDF ì´ë¯¸ì§€ ì¶”ì¶œ
**íŒŒì¼**: `utils/pdf_image_extractor.py` (ì‹ ê·œ)

```python
"""
PDF ì´ë¯¸ì§€ ì¶”ì¶œ ë° Vision ë¶„ì„
"""
import fitz  # PyMuPDF
from pathlib import Path
from PIL import Image
import io
from typing import List, Dict

def extract_images_from_pdf(pdf_path: str) -> List[Dict]:
    """
    PDFì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ

    Returns:
        [
            {
                "page": int,
                "image_index": int,
                "image_bytes": bytes,
                "width": int,
                "height": int
            },
            ...
        ]
    """
    doc = fitz.open(pdf_path)
    images = []

    for page_num in range(len(doc)):
        page = doc[page_num]
        image_list = page.get_images()

        for img_idx, img in enumerate(image_list):
            xref = img[0]
            base_image = doc.extract_image(xref)

            image_bytes = base_image["image"]
            image_ext = base_image["ext"]

            # PILë¡œ ì´ë¯¸ì§€ ë¡œë“œ (í¬ê¸° í™•ì¸)
            pil_image = Image.open(io.BytesIO(image_bytes))
            width, height = pil_image.size

            # ë„ˆë¬´ ì‘ì€ ì´ë¯¸ì§€ ì œì™¸ (ì•„ì´ì½˜, ë¡œê³  ë“±)
            if width < 100 or height < 100:
                continue

            # 512x512ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (ë¹„ìš© ì ˆê°)
            pil_image.thumbnail((512, 512), Image.Resampling.LANCZOS)

            # ë‹¤ì‹œ ë°”ì´íŠ¸ë¡œ ë³€í™˜
            buffer = io.BytesIO()
            pil_image.save(buffer, format="PNG")
            resized_bytes = buffer.getvalue()

            images.append({
                "page": page_num + 1,
                "image_index": img_idx,
                "image_bytes": resized_bytes,
                "width": pil_image.size[0],
                "height": pil_image.size[1],
                "original_ext": image_ext
            })

    doc.close()
    return images
```

#### 2.2 Vision ë¶„ì„ (GPT-4o)
**íŒŒì¼**: `utils/pdf_image_extractor.py` (ê³„ì†)

```python
def analyze_image_with_vision(
    image_bytes: bytes,
    llm_vision,  # OpenAI Vision LLM
    page_num: int
) -> str:
    """
    ì´ë¯¸ì§€ë¥¼ GPT-4o Visionìœ¼ë¡œ ë¶„ì„

    Returns:
        ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ì„¤ëª…
    """
    import base64

    # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
    image_base64 = base64.b64encode(image_bytes).decode('utf-8')

    prompt = f"""ë‹¤ìŒ ì´ë¯¸ì§€ëŠ” ë…¼ë¬¸ì˜ {page_num}í˜ì´ì§€ì— ìˆëŠ” ê·¸ë˜í”„, í‘œ, ë˜ëŠ” ë„ì‹ì…ë‹ˆë‹¤.
ì´ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì„¸ìš”.

**ì¶œë ¥ í˜•ì‹**:
1. ì´ë¯¸ì§€ íƒ€ì…: [ê·¸ë˜í”„/í‘œ/ë„ì‹/ì‚¬ì§„]
2. ì£¼ìš” ë‚´ìš©:
   - [í•µì‹¬ ë‚´ìš©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½]
3. ì„¸ë¶€ ì •ë³´:
   - ì¶• ì œëª©/ë²”ë¡€ (ê·¸ë˜í”„ì¸ ê²½ìš°)
   - ì—´/í–‰ ì œëª© (í‘œì¸ ê²½ìš°)
   - ì£¼ìš” ìˆ˜ì¹˜ ë°ì´í„°
   - íŠ¸ë Œë“œ ë˜ëŠ” íŒ¨í„´

**ì¤‘ìš”**: ê³¼í•™ ë…¼ë¬¸ ê²€ìƒ‰ì„ ìœ„í•œ ê²ƒì´ë¯€ë¡œ, ê¸°ìˆ ì  ìš©ì–´ë¥¼ ì •í™•íˆ í‘œê¸°í•˜ì„¸ìš”.
"""

    # GPT-4o Vision í˜¸ì¶œ
    from langchain_core.messages import HumanMessage

    message = HumanMessage(
        content=[
            {"type": "text", "text": prompt},
            {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/png;base64,{image_base64}"
                }
            }
        ]
    )

    response = llm_vision.invoke([message])
    return response.content
```

#### 2.3 Document Processor í†µí•©
**íŒŒì¼**: `utils/document_processor.py` (ìˆ˜ì •)

```python
def process_pdf_with_vision(
    self,
    file_path: str,
    enable_vision: bool = False
) -> List[Document]:
    """
    PDF ì²˜ë¦¬ (í…ìŠ¤íŠ¸ + Vision ì´ë¯¸ì§€ ë¶„ì„)
    """
    # ê¸°ì¡´ í…ìŠ¤íŠ¸ ì²­í‚¹
    text_docs = self._process_pdf_text(file_path)

    if not enable_vision:
        return text_docs

    # Vision ì´ë¯¸ì§€ ë¶„ì„
    from utils.pdf_image_extractor import extract_images_from_pdf, analyze_image_with_vision

    images = extract_images_from_pdf(file_path)

    if not images:
        return text_docs

    # GPT-4o Vision LLM ì´ˆê¸°í™”
    llm_vision = self._get_vision_llm()

    # ì´ë¯¸ì§€ë³„ ë¶„ì„
    image_docs = []
    for img in images:
        try:
            description = analyze_image_with_vision(
                img["image_bytes"],
                llm_vision,
                img["page"]
            )

            # Document ìƒì„±
            doc = Document(
                page_content=f"[í˜ì´ì§€ {img['page']} ì´ë¯¸ì§€]\n{description}",
                metadata={
                    "source": file_path,
                    "page": img["page"],
                    "type": "image",
                    "image_index": img["image_index"]
                }
            )
            image_docs.append(doc)

        except Exception as e:
            print(f"âš ï¸  ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨ (í˜ì´ì§€ {img['page']}): {e}")

    # í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ë¬¸ì„œ ë³‘í•©
    # í˜ì´ì§€ë³„ë¡œ ì •ë ¬í•˜ì—¬ ì‚½ì…
    all_docs = text_docs + image_docs
    all_docs.sort(key=lambda x: (x.metadata.get("page", 0), x.metadata.get("type", "text")))

    return all_docs
```

#### 2.4 ë¹„ìš© ìµœì í™”: ì˜¤í”„ë¼ì¸ ëŒ€ì•ˆ (Camelot)
**íŒŒì¼**: `utils/pdf_table_extractor.py` (ì‹ ê·œ, ì„ íƒ ì‚¬í•­)

```python
"""
ì˜¤í”„ë¼ì¸ í‘œ ì¶”ì¶œ (Camelot ì‚¬ìš©)
ë¹„ìš© ì ˆê°ì„ ìœ„í•œ ëŒ€ì•ˆ
"""
import camelot

def extract_tables_from_pdf(pdf_path: str) -> List[Dict]:
    """
    Camelotìœ¼ë¡œ PDF í‘œ ì¶”ì¶œ

    Returns:
        [
            {
                "page": int,
                "table_index": int,
                "markdown": str
            },
            ...
        ]
    """
    # Camelotìœ¼ë¡œ í‘œ ì¶”ì¶œ
    tables = camelot.read_pdf(pdf_path, pages='all', flavor='lattice')

    results = []
    for i, table in enumerate(tables):
        # DataFrameì„ Markdownìœ¼ë¡œ ë³€í™˜
        markdown = table.df.to_markdown(index=False)

        results.append({
            "page": table.page,
            "table_index": i,
            "markdown": f"[í‘œ {i+1}]\n{markdown}"
        })

    return results
```

**ë¹„ìš© ë¹„êµ**:
- **GPT-4o Vision**: 100í˜ì´ì§€ Ã— 5 ì´ë¯¸ì§€ Ã— $0.00638 (512x512) = **$3.19**
- **Camelot**: ë¬´ë£Œ (ì˜¤í”„ë¼ì¸)

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 3-4ì¼

---

## ğŸ” Task 3: Exhaustive Retrieval ìë™ ê°ì§€

### ëª©í‘œ
"ëª¨ë“ /ì „ì²´" í‚¤ì›Œë“œ ì—†ì´ë„ LLMì´ ì¿¼ë¦¬ ë³µì¡ë„ë¥¼ íŒë‹¨í•˜ì—¬ ìë™ìœ¼ë¡œ exhaustive retrievalì„ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤.

### êµ¬í˜„ ìƒì„¸

#### 3.1 ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ì„ê¸°
**íŒŒì¼**: `utils/query_analyzer.py` (ì‹ ê·œ)

```python
"""
ì¿¼ë¦¬ ë²”ìœ„ ìë™ ê°ì§€
"""
from typing import Dict

def analyze_query_scope(
    query: str,
    llm,
    temperature: float = 0.0
) -> Dict[str, any]:
    """
    ì¿¼ë¦¬ì˜ ê²€ìƒ‰ ë²”ìœ„ë¥¼ LLMìœ¼ë¡œ íŒë‹¨

    Returns:
        {
            "scope": "narrow" | "medium" | "broad",
            "recommended_top_k": int,
            "reason": str
        }
    """

    prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì˜ ê²€ìƒ‰ ë²”ìœ„ë¥¼ íŒë‹¨í•˜ì„¸ìš”.

ì§ˆë¬¸: {query}

íŒë‹¨ ê¸°ì¤€:
- **narrow**: íŠ¹ì • ì €ì/ë…¼ë¬¸/ê°œë…ì— ëŒ€í•œ êµ¬ì²´ì  ì§ˆë¬¸
  ì˜ˆ: "Balkenholì´ 2020ë…„ì— ë°œí‘œí•œ ë…¼ë¬¸ì€?", "OLED íš¨ìœ¨ ì •ì˜ëŠ”?"
  â†’ ì†Œìˆ˜ì˜ ë¬¸ì„œ(3-5ê°œ)ë¡œ ë‹µë³€ ê°€ëŠ¥

- **medium**: ì—¬ëŸ¬ ê°œë… ë¹„êµ, ê´€ê³„ ë¶„ì„, íŠ¹ì • ì£¼ì œ ìš”ì•½
  ì˜ˆ: "Aì™€ Bì˜ ì°¨ì´ì ì€?", "ìµœê·¼ OLED ì—°êµ¬ ë™í–¥ì€?"
  â†’ ì¤‘ê°„ ê·œëª¨ ë¬¸ì„œ(10-20ê°œ) í•„ìš”

- **broad**: íŠ¹ì • ì£¼ì œì— ëŒ€í•œ í¬ê´„ì  ì¡°ì‚¬, ì „ì²´ ë¦¬ë·°
  ì˜ˆ: "Balkenholì˜ ëª¨ë“  ì—°êµ¬ ì„¤ëª…", "OLED ê´€ë ¨ ëª¨ë“  ë…¼ë¬¸ ìš”ì•½"
  â†’ ëŒ€ê·œëª¨ ë¬¸ì„œ(50-100ê°œ) í•„ìš”

ì‘ë‹µ í˜•ì‹ (JSONë§Œ):
{{
    "scope": "narrow|medium|broad",
    "reason": "íŒë‹¨ ê·¼ê±°ë¥¼ 1ë¬¸ì¥ìœ¼ë¡œ"
}}
"""

    # LLM í˜¸ì¶œ
    response = llm.invoke(prompt, temperature=temperature)

    # JSON íŒŒì‹±
    import json
    import re

    # JSON ë¸”ë¡ ì¶”ì¶œ
    json_match = re.search(r'\{[^}]+\}', response.content, re.DOTALL)
    if not json_match:
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
        return {"scope": "narrow", "recommended_top_k": 3, "reason": "íŒŒì‹± ì‹¤íŒ¨"}

    result = json.loads(json_match.group())

    # scopeì— ë”°ë¼ top_k ì¶”ì²œ
    scope_to_top_k = {
        "narrow": 3,
        "medium": 10,
        "broad": 100
    }

    result["recommended_top_k"] = scope_to_top_k.get(result["scope"], 3)

    return result
```

#### 3.2 RAGChain í†µí•©
**íŒŒì¼**: `utils/rag_chain.py` (ìˆ˜ì •)

```python
def invoke(self, query: str, **kwargs) -> Dict:
    """
    ì§ˆë¬¸ì— ë‹µë³€ (ìë™ exhaustive retrieval ì§€ì›)
    """
    # 1. ì¿¼ë¦¬ ë²”ìœ„ ë¶„ì„ (ì˜µì…˜)
    if self.enable_auto_exhaustive:
        from utils.query_analyzer import analyze_query_scope

        scope_result = analyze_query_scope(query, self.llm)

        # ë¡œê·¸ ê¸°ë¡
        print(f"[Query Scope] {scope_result['scope']} "
              f"(top_k: {scope_result['recommended_top_k']}) "
              f"- {scope_result['reason']}")

        # top_k ë™ì  ì¡°ì •
        dynamic_top_k = scope_result["recommended_top_k"]
    else:
        dynamic_top_k = self.top_k

    # 2. ê¸°ì¡´ ê²€ìƒ‰ ë¡œì§
    docs = self._search_candidates(query, top_k=dynamic_top_k)

    # ... (ì´í•˜ ë™ì¼)
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 2-3ì¼

---

## ğŸ” Task 4: ChromaDB ë™ì‹œ ì ‘ì† ëŒ€ì‘

### ëª©í‘œ
ë„¤íŠ¸ì›Œí¬ í´ë” í™˜ê²½ì—ì„œ ì—¬ëŸ¬ ì‚¬ìš©ìê°€ ë™ì‹œì— DBë¥¼ ì‚¬ìš©í•  ë•Œ íŒŒì¼ ì ê¸ˆ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.

### êµ¬í˜„ ìƒì„¸

#### 4.1 ì½ê¸° ì „ìš© ëª¨ë“œ
**íŒŒì¼**: `utils/vector_store.py` (ìˆ˜ì •)

```python
class VectorStoreManager:
    def __init__(
        self,
        ...,
        mode: str = "readwrite"  # "readonly" | "readwrite"
    ):
        """
        Args:
            mode:
                - "readonly": ì½ê¸° ì „ìš© (ê³µìœ  DBìš©)
                - "readwrite": ì½ê¸°/ì“°ê¸° ê°€ëŠ¥ (ê°œì¸ DBìš©)
        """
        self.mode = mode

        if mode == "readonly":
            # ì½ê¸° ì „ìš© í´ë¼ì´ì–¸íŠ¸
            self.client = chromadb.PersistentClient(
                path=persist_directory,
                settings=Settings(
                    allow_reset=False,
                    anonymized_telemetry=False
                )
            )
        else:
            # ì½ê¸°/ì“°ê¸° í´ë¼ì´ì–¸íŠ¸
            self.client = chromadb.PersistentClient(
                path=persist_directory
            )

    def add_documents(self, docs: List[Document]):
        """ë¬¸ì„œ ì¶”ê°€ (ì½ê¸° ì „ìš© ëª¨ë“œì—ì„œëŠ” ì˜¤ë¥˜)"""
        if self.mode == "readonly":
            raise PermissionError(
                "ê³µìœ  DBëŠ” ì½ê¸° ì „ìš©ì…ë‹ˆë‹¤. ê°œì¸ DBì— ì¶”ê°€í•˜ì„¸ìš”."
            )

        # ... ê¸°ì¡´ ë¡œì§
```

#### 4.2 íŒŒì¼ ì ê¸ˆ ì²˜ë¦¬
**íŒŒì¼**: `utils/db_lock.py` (ì‹ ê·œ)

```python
"""
DB íŒŒì¼ ì ê¸ˆ ê´€ë¦¬
ë„¤íŠ¸ì›Œí¬ í™˜ê²½ì—ì„œ ì“°ê¸° ì¶©ëŒ ë°©ì§€
"""
import time
from pathlib import Path
from datetime import datetime

class DBLock:
    """
    ê°„ë‹¨í•œ íŒŒì¼ ê¸°ë°˜ ì ê¸ˆ
    """
    def __init__(self, db_path: str):
        self.lock_file = Path(db_path) / ".lock"
        self.lock_info_file = Path(db_path) / ".lock_info"

    def acquire(self, timeout: int = 10) -> bool:
        """
        ì ê¸ˆ íšë“

        Args:
            timeout: ëŒ€ê¸° ì‹œê°„ (ì´ˆ)

        Returns:
            True if success, False if timeout
        """
        start = time.time()

        while self.lock_file.exists():
            # íƒ€ì„ì•„ì›ƒ ì²´í¬
            if time.time() - start > timeout:
                # ì ê¸ˆ ì •ë³´ ì½ê¸°
                if self.lock_info_file.exists():
                    with open(self.lock_info_file, 'r') as f:
                        lock_info = f.read()
                    raise TimeoutError(
                        f"DB ì ê¸ˆ íšë“ ì‹¤íŒ¨ (íƒ€ì„ì•„ì›ƒ)\n"
                        f"ë‹¤ë¥¸ ì‚¬ìš©ìê°€ DBë¥¼ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤: {lock_info}"
                    )
                else:
                    raise TimeoutError("DB ì ê¸ˆ íšë“ ì‹¤íŒ¨ (íƒ€ì„ì•„ì›ƒ)")

            time.sleep(0.5)

        # ì ê¸ˆ íŒŒì¼ ìƒì„±
        self.lock_file.touch()

        # ì ê¸ˆ ì •ë³´ ê¸°ë¡
        with open(self.lock_info_file, 'w') as f:
            f.write(f"User: {os.getenv('USERNAME', 'unknown')}\n")
            f.write(f"Time: {datetime.now().isoformat()}\n")
            f.write(f"PID: {os.getpid()}\n")

        return True

    def release(self):
        """ì ê¸ˆ í•´ì œ"""
        if self.lock_file.exists():
            self.lock_file.unlink()
        if self.lock_info_file.exists():
            self.lock_info_file.unlink()

    def __enter__(self):
        self.acquire()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.release()
```

#### 4.3 ì‚¬ìš© ì˜ˆì‹œ
**íŒŒì¼**: `utils/vector_store.py` (ìˆ˜ì •)

```python
def add_documents(self, docs: List[Document]):
    """ë¬¸ì„œ ì¶”ê°€ (íŒŒì¼ ì ê¸ˆ ì‚¬ìš©)"""
    if self.mode == "readonly":
        raise PermissionError("ì½ê¸° ì „ìš© ëª¨ë“œì—ì„œëŠ” ì¶”ê°€ ë¶ˆê°€")

    from utils.db_lock import DBLock

    # ì ê¸ˆ íšë“ í›„ ì¶”ê°€
    with DBLock(self.persist_directory):
        # ... ê¸°ì¡´ add_documents ë¡œì§
        pass
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 1-2ì¼

---

## ğŸ“Š Task 5: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

### ëª©í‘œ
ê° ë‹¨ê³„ë³„ ì²˜ë¦¬ ì‹œê°„ì„ ì¸¡ì •í•˜ì—¬ ë³‘ëª© ì§€ì ì„ ì‹ë³„í•©ë‹ˆë‹¤.

### êµ¬í˜„ ìƒì„¸

#### 5.1 ì„±ëŠ¥ ë¡œê±°
**íŒŒì¼**: `utils/performance_logger.py` (ì‹ ê·œ)

```python
"""
ì„±ëŠ¥ ì¸¡ì • ë° ë¡œê¹…
"""
import time
import json
import logging
from pathlib import Path
from datetime import datetime
from contextlib import contextmanager

# ë¡œê·¸ ë””ë ‰í† ë¦¬
LOG_DIR = Path("logs")
LOG_DIR.mkdir(exist_ok=True)

# ì„±ëŠ¥ ë¡œê±°
perf_logger = logging.getLogger("performance")
perf_logger.setLevel(logging.INFO)
perf_logger.propagate = False

fh = logging.FileHandler(LOG_DIR / "performance_history.jsonl", encoding="utf-8")
formatter = logging.Formatter('%(message)s')
fh.setFormatter(formatter)
perf_logger.addHandler(fh)

class PerformanceTracker:
    """
    ì„±ëŠ¥ ì¸¡ì • ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
    """
    def __init__(self, query: str):
        self.query = query
        self.start_time = None
        self.breakdown = {}

    @contextmanager
    def measure(self, step_name: str):
        """ë‹¨ê³„ë³„ ì‹œê°„ ì¸¡ì •"""
        step_start = time.time()
        try:
            yield
        finally:
            elapsed_ms = int((time.time() - step_start) * 1000)
            self.breakdown[f"{step_name}_ms"] = elapsed_ms

    def start(self):
        """ì „ì²´ ì¸¡ì • ì‹œì‘"""
        self.start_time = time.time()

    def finish(self, **extra_info):
        """ì¸¡ì • ì™„ë£Œ ë° ë¡œê·¸ ê¸°ë¡"""
        total_time_ms = int((time.time() - self.start_time) * 1000)

        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "query": self.query,
            "total_time_ms": total_time_ms,
            "breakdown": self.breakdown,
            **extra_info
        }

        # ë¡œê·¸ ê¸°ë¡
        perf_logger.info(json.dumps(log_entry, ensure_ascii=False))

        # ë³‘ëª© ê²½ê³  (3ì´ˆ ì´ìƒ)
        if total_time_ms > 3000:
            print(f"âš ï¸  ëŠë¦° ì¿¼ë¦¬ ê°ì§€ ({total_time_ms}ms): {self.query[:50]}...")

            # ê°€ì¥ ëŠë¦° ë‹¨ê³„ ì‹ë³„
            slowest_step = max(self.breakdown.items(), key=lambda x: x[1])
            print(f"    ë³‘ëª©: {slowest_step[0]} ({slowest_step[1]}ms)")

        return log_entry
```

#### 5.2 RAGChain í†µí•©
**íŒŒì¼**: `utils/rag_chain.py` (ìˆ˜ì •)

```python
def invoke(self, query: str, **kwargs) -> Dict:
    """ì§ˆë¬¸ì— ë‹µë³€ (ì„±ëŠ¥ ì¸¡ì •)"""

    from utils.performance_logger import PerformanceTracker

    tracker = PerformanceTracker(query)
    tracker.start()

    # 1. ê²€ìƒ‰
    with tracker.measure("retrieval"):
        docs = self._search_candidates(query)

    # 2. Re-ranking
    with tracker.measure("reranking"):
        if self.use_reranker:
            docs = self._rerank_documents(query, docs)

    # 3. ì»¨í…ìŠ¤íŠ¸ í™•ì¥
    with tracker.measure("context_expansion"):
        expanded_docs = self._expand_context(docs)

    # 4. LLM ìƒì„±
    with tracker.measure("llm_generation"):
        answer = self._generate_answer(query, expanded_docs)

    # ì¸¡ì • ì™„ë£Œ
    perf_info = tracker.finish(
        llm_model=self.llm_model,
        num_docs_retrieved=len(docs),
        final_docs=len(expanded_docs)
    )

    return {
        "answer": answer,
        "sources": expanded_docs,
        "performance": perf_info
    }
```

#### 5.3 ë¶„ì„ ë„êµ¬
**íŒŒì¼**: `scripts/analyze_performance_logs.py` (ì‹ ê·œ)

```python
"""
ì„±ëŠ¥ ë¡œê·¸ ë¶„ì„
"""
import json
from pathlib import Path
from collections import defaultdict
import statistics

def analyze_performance(log_file: str = "logs/performance_history.jsonl"):
    """ì„±ëŠ¥ ë¡œê·¸ ë¶„ì„"""

    logs = []
    with open(log_file, "r", encoding="utf-8") as f:
        for line in f:
            logs.append(json.loads(line))

    print(f"\n{'='*60}")
    print(f"ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸")
    print(f"{'='*60}")
    print(f"ì´ ì¿¼ë¦¬ ìˆ˜: {len(logs)}")

    # 1. ì „ì²´ ì‘ë‹µ ì‹œê°„
    total_times = [log['total_time_ms'] for log in logs]
    print(f"\n[1] ì „ì²´ ì‘ë‹µ ì‹œê°„")
    print(f"  í‰ê· : {statistics.mean(total_times):.1f}ms")
    print(f"  ì¤‘ì•™ê°’: {statistics.median(total_times):.1f}ms")
    print(f"  ìµœëŒ€: {max(total_times):.1f}ms")
    print(f"  ìµœì†Œ: {min(total_times):.1f}ms")

    # 2. ë‹¨ê³„ë³„ í‰ê·  ì‹œê°„
    print(f"\n[2] ë‹¨ê³„ë³„ í‰ê·  ì‹œê°„")
    steps = defaultdict(list)
    for log in logs:
        for step, time_ms in log['breakdown'].items():
            steps[step].append(time_ms)

    for step, times in sorted(steps.items(), key=lambda x: -statistics.mean(x[1])):
        avg_time = statistics.mean(times)
        pct = avg_time / statistics.mean(total_times) * 100
        print(f"  {step:25s}: {avg_time:6.1f}ms ({pct:4.1f}%)")

    # 3. ëŠë¦° ì¿¼ë¦¬ (ìƒìœ„ 5ê°œ)
    print(f"\n[3] ëŠë¦° ì¿¼ë¦¬ Top 5")
    slow_queries = sorted(logs, key=lambda x: x['total_time_ms'], reverse=True)[:5]
    for i, log in enumerate(slow_queries, 1):
        print(f"  {i}. {log['query'][:50]}...")
        print(f"     ì‹œê°„: {log['total_time_ms']}ms")

        # ë³‘ëª© ë‹¨ê³„
        slowest_step = max(log['breakdown'].items(), key=lambda x: x[1])
        print(f"     ë³‘ëª©: {slowest_step[0]} ({slowest_step[1]}ms)")

if __name__ == "__main__":
    analyze_performance()
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 1ì¼

---

## ğŸ‘ Task 6: ì‚¬ìš©ì í”¼ë“œë°± ì‹œìŠ¤í…œ (ì„ íƒ)

### ëª©í‘œ
ë‹µë³€ í’ˆì§ˆì— ëŒ€í•œ ì‚¬ìš©ì í”¼ë“œë°±ì„ ìˆ˜ì§‘í•˜ì—¬ ì‹œìŠ¤í…œ ê°œì„ ì— í™œìš©í•©ë‹ˆë‹¤.

### êµ¬í˜„ ìƒì„¸

#### 6.1 UI ìˆ˜ì •
**íŒŒì¼**: `ui/chat_widget.py` (ìˆ˜ì •)

```python
class ChatWidget(QWidget):
    def _add_assistant_message(self, message: str, sources: List[Dict]):
        """
        ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€ (í”¼ë“œë°± ë²„íŠ¼ í¬í•¨)
        """
        # ê¸°ì¡´ ë©”ì‹œì§€ í‘œì‹œ
        msg_widget = QWidget()
        layout = QVBoxLayout(msg_widget)

        # ë©”ì‹œì§€ í…ìŠ¤íŠ¸
        text_label = QLabel(message)
        layout.addWidget(text_label)

        # í”¼ë“œë°± ë²„íŠ¼
        feedback_layout = QHBoxLayout()

        thumbs_up = QPushButton("ğŸ‘ ë„ì›€ë¨")
        thumbs_up.clicked.connect(
            lambda: self._record_feedback("positive", message, sources)
        )

        thumbs_down = QPushButton("ğŸ‘ ê°œì„  í•„ìš”")
        thumbs_down.clicked.connect(
            lambda: self._record_feedback("negative", message, sources)
        )

        feedback_layout.addWidget(thumbs_up)
        feedback_layout.addWidget(thumbs_down)
        feedback_layout.addStretch()

        layout.addLayout(feedback_layout)

        self.chat_area.addWidget(msg_widget)

    def _record_feedback(
        self,
        rating: str,
        answer: str,
        sources: List[Dict]
    ):
        """í”¼ë“œë°± ê¸°ë¡"""
        from utils.feedback_logger import log_feedback

        # ì‚¬ìš©ì ì½”ë©˜íŠ¸ ì…ë ¥ (ì„ íƒ)
        comment, ok = QInputDialog.getText(
            self,
            "í”¼ë“œë°±",
            "ì¶”ê°€ ì˜ê²¬ì´ ìˆìœ¼ë©´ ì…ë ¥í•˜ì„¸ìš” (ì„ íƒ):"
        )

        if ok:
            log_feedback(
                query=self.current_query,
                answer=answer,
                rating=rating,
                user_comment=comment if comment else "",
                sources_used=[s.get("source", "") for s in sources]
            )

            QMessageBox.information(self, "ê°ì‚¬í•©ë‹ˆë‹¤", "í”¼ë“œë°±ì´ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
```

#### 6.2 í”¼ë“œë°± ë¡œê±°
**íŒŒì¼**: `utils/feedback_logger.py` (ì‹ ê·œ)

```python
"""
ì‚¬ìš©ì í”¼ë“œë°± ë¡œê¹…
"""
import json
import logging
from pathlib import Path
from datetime import datetime

LOG_DIR = Path("logs")
LOG_DIR.mkdir(exist_ok=True)

feedback_logger = logging.getLogger("user_feedback")
feedback_logger.setLevel(logging.INFO)
feedback_logger.propagate = False

fh = logging.FileHandler(LOG_DIR / "user_feedback.jsonl", encoding="utf-8")
formatter = logging.Formatter('%(message)s')
fh.setFormatter(formatter)
feedback_logger.addHandler(fh)

def log_feedback(
    query: str,
    answer: str,
    rating: str,  # "positive" | "negative"
    user_comment: str = "",
    sources_used: List[str] = None
):
    """í”¼ë“œë°± ê¸°ë¡"""

    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "query": query,
        "answer": answer,
        "rating": rating,
        "user_comment": user_comment,
        "sources_used": sources_used or []
    }

    feedback_logger.info(json.dumps(log_entry, ensure_ascii=False))

    # ë¶€ì • í”¼ë“œë°± ì•Œë¦¼
    if rating == "negative":
        print(f"âš ï¸  ë¶€ì • í”¼ë“œë°±: {query[:50]}...")
```

#### 6.3 ì£¼ê°„ ë¦¬í¬íŠ¸
**íŒŒì¼**: `scripts/generate_weekly_feedback_report.py` (ì‹ ê·œ)

```python
"""
ì£¼ê°„ í”¼ë“œë°± ë¦¬í¬íŠ¸ ìƒì„±
"""
import json
from pathlib import Path
from datetime import datetime, timedelta
from collections import Counter

def generate_weekly_report(log_file: str = "logs/user_feedback.jsonl"):
    """ì£¼ê°„ í”¼ë“œë°± ë¦¬í¬íŠ¸"""

    # ìµœê·¼ 7ì¼ ë¡œê·¸ë§Œ
    cutoff = datetime.now() - timedelta(days=7)

    logs = []
    with open(log_file, "r", encoding="utf-8") as f:
        for line in f:
            log = json.loads(line)
            log_time = datetime.fromisoformat(log['timestamp'])
            if log_time >= cutoff:
                logs.append(log)

    print(f"\n{'='*60}")
    print(f"ì£¼ê°„ í”¼ë“œë°± ë¦¬í¬íŠ¸")
    print(f"{'='*60}")
    print(f"ê¸°ê°„: {cutoff.date()} ~ {datetime.now().date()}")
    print(f"ì´ í”¼ë“œë°± ìˆ˜: {len(logs)}")

    # 1. ë§Œì¡±ë„
    ratings = Counter(log['rating'] for log in logs)
    print(f"\n[1] ì‚¬ìš©ì ë§Œì¡±ë„")
    for rating, count in ratings.most_common():
        pct = count / len(logs) * 100
        emoji = "ğŸ˜Š" if rating == "positive" else "ğŸ˜"
        print(f"  {emoji} {rating:10s}: {count:3d} ({pct:5.1f}%)")

    # 2. ë¶€ì • í”¼ë“œë°± ì¼€ì´ìŠ¤
    negative_logs = [log for log in logs if log['rating'] == 'negative']
    if negative_logs:
        print(f"\n[2] ê°œì„  í•„ìš” ì¼€ì´ìŠ¤ ({len(negative_logs)}ê±´)")
        for log in negative_logs[:5]:
            print(f"\n  ì§ˆë¬¸: {log['query']}")
            print(f"  ë‹µë³€: {log['answer'][:100]}...")
            if log['user_comment']:
                print(f"  ì˜ê²¬: {log['user_comment']}")

    print(f"\n{'='*60}\n")

if __name__ == "__main__":
    generate_weekly_report()
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 2ì¼ (ì„ íƒ ì‚¬í•­)

---

## ğŸ“… Phase 2.5 ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

### í•„ìˆ˜ ì‘ì—…
- [ ] Task 1: Question Classifier ë¡œê¹… êµ¬í˜„ ë° 1ì£¼ì¼ ë°ì´í„° ìˆ˜ì§‘
- [ ] Task 2: PDF Vision ì²˜ë¦¬ êµ¬í˜„ ë° 10ê°œ ë…¼ë¬¸ í…ŒìŠ¤íŠ¸
- [ ] Task 3: Exhaustive Retrieval ìë™ ê°ì§€ êµ¬í˜„ ë° ì •í™•ë„ 80% ê²€ì¦
- [ ] Task 4: ChromaDB ë™ì‹œ ì ‘ì† ëŒ€ì‘ ë° 3ëª… ì´ìƒ í…ŒìŠ¤íŠ¸
- [ ] Task 5: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ êµ¬í˜„ ë° 1ì£¼ì¼ ë°ì´í„° ìˆ˜ì§‘

### ì„ íƒ ì‘ì—…
- [ ] Task 6: ì‚¬ìš©ì í”¼ë“œë°± ì‹œìŠ¤í…œ êµ¬í˜„

### ì„±ëŠ¥ ëª©í‘œ
- [ ] í‰ê·  ì‘ë‹µ ì‹œê°„ 5ì´ˆ ì´í•˜ (Llama-4-scout ê¸°ì¤€)
- [ ] Re-ranking ì²˜ë¦¬ ì‹œê°„ 1.5ì´ˆ ì´í•˜ (60ê°œ ë¬¸ì„œ)
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 2GB ì´í•˜

### í’ˆì§ˆ ëª©í‘œ
- [ ] Question Classifier í‰ê·  ì‹ ë¢°ë„ 0.8 ì´ìƒ
- [ ] PDF Vision ë¶„ì„ ì •í™•ë„ 90% ì´ìƒ (ìˆ˜ë™ ê²€ì¦)
- [ ] Exhaustive Retrieval ìë™ ê°ì§€ ì •í™•ë„ 80% ì´ìƒ

---

**ì‘ì„±ì**: Claude Code
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-01-09
