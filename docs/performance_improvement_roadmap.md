# RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ ê°œì„  ë¡œë“œë§µ (NotebookLM ë²¤ì¹˜ë§ˆí‚¹)

**ì‘ì„±ì¼**: 2025-11-06
**ë²„ì „**: v3.1 â†’ v3.2+ (ê³„íš)
**ëª©í‘œ**: ëŒ€ëŸ‰ ë¬¸ì„œ í™˜ê²½ì—ì„œë„ NotebookLM ìˆ˜ì¤€ ì´ìƒì˜ ì •í™•ë„ ìœ ì§€

---

## ğŸ“‹ ëª©ì°¨

1. [í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„](#í˜„ì¬-ì‹œìŠ¤í…œ-ë¶„ì„)
2. [NotebookLM ë²¤ì¹˜ë§ˆí‚¹](#notebooklm-ë²¤ì¹˜ë§ˆí‚¹)
3. [Phase A: ì¦‰ì‹œ ì ìš© (1-2ì£¼)](#phase-a-ì¦‰ì‹œ-ì ìš©)
4. [Phase B: ë‹¨ê¸° ê°œì„  (1-2ê°œì›”)](#phase-b-ë‹¨ê¸°-ê°œì„ )
5. [Phase C: ì¤‘ê¸° ê°œì„  (2-3ê°œì›”)](#phase-c-ì¤‘ê¸°-ê°œì„ )
6. [ì˜ˆìƒ ì„±ê³¼](#ì˜ˆìƒ-ì„±ê³¼)
7. [êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸](#êµ¬í˜„-ì²´í¬ë¦¬ìŠ¤íŠ¸)

---

## í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„

### ì„±ê³¼ ìš”ì•½ (v3.1)

| ì§€í‘œ | ê°’ | ë¹„ê³  |
|------|-----|------|
| **ì •í™•ë„** | 100% (11/11) | ì†Œê·œëª¨ ë¬¸ì„œ í™˜ê²½ |
| **í¬ë¡œìŠ¤ ë„ë©”ì¸ ì˜¤ì—¼** | 4.5% | Baseline 100% â†’ 95.5% ê°œì„  |
| **ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜** | 100% (5/5) | LLM ê¸°ë°˜ ìë™ ë¶„ë¥˜ |
| **ì¹˜ëª…ì  ì˜¤ë‹µ** | 0ê±´ | ì™„ì „ ì œê±° |
| **í‰ê·  ì¶œì²˜ ê°œìˆ˜** | 5.0ê°œ | Baseline 2.57ê°œ â†’ +94.6% |

### í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ

```
âœ… Phase 1-2: í‘œ êµ¬ì¡° ë³´ì¡´ + ìŠ¬ë¼ì´ë“œ ë¬¸ë§¥ (+30-45%)
âœ… Phase 3: ìŠ¬ë¼ì´ë“œ íƒ€ì… ë¶„ë¥˜ 9ì¢… (+15-20%)
âœ… Phase 4: Hybrid Search BM25+Vector (+30-40%)
âœ… ì¹´í…Œê³ ë¦¬ ì‹œìŠ¤í…œ: LLM ê¸°ë°˜ ìë™ ë¶„ë¥˜ (100% ì •í™•)
```

**ëˆ„ì  íš¨ê³¼**: ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ **+70-100% í–¥ìƒ**

### í™•ì¥ì„± ìš°ë ¤ì‚¬í•­

**í˜„ì¬ í™˜ê²½**:
- ë¬¸ì„œ ìˆ˜: 5ê°œ
- ì´ ì²­í¬: ~120ê°œ
- ë„ë©”ì¸: 3-4ê°œ (technical, business, hr, reference)

**ì˜ˆìƒ í™˜ê²½ (6ê°œì›” í›„)**:
- ë¬¸ì„œ ìˆ˜: 100+ ê°œ
- ì´ ì²­í¬: 2,000-5,000ê°œ
- ë„ë©”ì¸: 10+ ê°œ

**ìš°ë ¤ ì‚¬í•­**:
1. âš ï¸ ê²€ìƒ‰ ì •í™•ë„ ì €í•˜ (ë…¸ì´ì¦ˆ ì¦ê°€)
2. âš ï¸ ì‘ë‹µ ì‹œê°„ ì¦ê°€ (ëŒ€ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬)
3. âš ï¸ í¬ë¡œìŠ¤ ë„ë©”ì¸ ì˜¤ì—¼ ì¬ë°œ ê°€ëŠ¥ì„±
4. âš ï¸ ì¹´í…Œê³ ë¦¬ ê°ì§€ ì •í™•ë„ ì €í•˜
5. âš ï¸ Re-ranker ë¶€ë‹´ ì¦ê°€

---

## NotebookLM ë²¤ì¹˜ë§ˆí‚¹

### NotebookLM í•µì‹¬ íŠ¹ì§• (2025)

| í•­ëª© | ì„±ëŠ¥ | ë¹„ê³  |
|------|------|------|
| **ì •í™•ë„** | 86% | ì˜ë£Œ ë„ë©”ì¸ í…ŒìŠ¤íŠ¸ |
| **ì¶œì²˜ ì •í™•ë„** | 95% | Source citation |
| **Hallucination ë°©ì§€** | ê°•í•¨ | ë¬¸ì„œ ê¸°ë°˜ë§Œ ë‹µë³€ |
| **ëŒ€ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬** | ì•½í•¨ | 100+ ë¬¸ì„œì—ì„œ ì„±ëŠ¥ ì €í•˜ |
| **Multi-document** | ê°•í•¨ | ì—¬ëŸ¬ ë¬¸ì„œ í†µí•© ë¶„ì„ |
| **Confidence í‘œì‹œ** | ê°•í•¨ | ë¶ˆí™•ì‹¤ ì‹œ ëª…í™•íˆ í‘œì‹œ |

### í˜„ì¬ ì‹œìŠ¤í…œ vs NotebookLM

| í•­ëª© | NotebookLM | í˜„ì¬ ì‹œìŠ¤í…œ (v3.1) | ìš°ìœ„ |
|------|-----------|-------------------|------|
| **ì •í™•ë„** | 86% | 100% (11/11, ì†Œê·œëª¨) | âš ï¸ ëŒ€ê·œëª¨ ë¯¸ê²€ì¦ |
| **ì¶œì²˜ ëª…ì‹œ** | 95% | ì¶œë ¥ ì¤‘ (ê°œì„  í•„ìš”) | âŒ NotebookLM |
| **Hallucination ë°©ì§€** | âœ… | âœ… (ì¹´í…Œê³ ë¦¬ í•„í„°ë§) | ë™ë“± |
| **ëŒ€ëŸ‰ ë¬¸ì„œ** | âŒ 100+ ì•½í•¨ | âœ… (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰) | âœ… ìš°ë¦¬ |
| **ë„ë©”ì¸ ë¶„ë¦¬** | - | âœ… (95.5% ê°œì„ ) | âœ… ìš°ë¦¬ |
| **Confidence** | âœ… | âŒ (ë¯¸êµ¬í˜„) | âŒ NotebookLM |

**ëª©í‘œ**: NotebookLMì˜ ê°•ì ì„ í¡ìˆ˜í•˜ê³ , ëŒ€ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ ìš°ìœ„ ìœ ì§€

---

## Phase A: ì¦‰ì‹œ ì ìš© (1-2ì£¼)

### A-1: Standard ëª¨ë“œ ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ì¶”ê°€ â­â­â­

**ìš°ì„ ìˆœìœ„**: ìµœê³  (30ë¶„ ì†Œìš”)
**ì˜ˆìƒ íš¨ê³¼**: í¬ë¡œìŠ¤ ë„ë©”ì¸ ì˜¤ì—¼ 4.5% â†’ 0%

**í˜„ì¬ ë¬¸ì œ**:
```python
# Small-to-Large ëª¨ë“œ: ì¹´í…Œê³ ë¦¬ í•„í„°ë§ âœ“
# Standard ëª¨ë“œ: ì¹´í…Œê³ ë¦¬ í•„í„°ë§ âœ— â† ë¬¸ì œ

Query: "FRET ì—ë„ˆì§€ ì „ë‹¬ íš¨ìœ¨ì€?"
ì¶œì²˜: technical (4/5), hr (1/5) â† HRD-Net í˜¼ì… (ì˜¤ì—¼)
```

**êµ¬í˜„ ìœ„ì¹˜**: `utils/rag_chain.py::_get_context_standard()`

**êµ¬í˜„ ë‚´ìš©**:
```python
def _get_context_standard(self, question: str, categories: List[str] = None):
    """Standard ê²€ìƒ‰ ëª¨ë“œ (Hybrid Search)"""

    # ... ê¸°ì¡´ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë¡œì§ ...

    # ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ì¶”ê°€ (Small-to-Largeì™€ ë™ì¼)
    if categories:
        print(f"  ğŸ” ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ì ìš©: {', '.join(categories)}")
        candidates = self._filter_by_category(candidates, categories)
        print(f"  âœ“ í•„í„°ë§ í›„: {len(candidates)}ê°œ ë¬¸ì„œ")

    # ... ë‚˜ë¨¸ì§€ ë¡œì§ ...
```

**í…ŒìŠ¤íŠ¸ ë°©ë²•**:
1. OLED ê¸°ìˆ  ì§ˆë¬¸ â†’ HR ë¬¸ì„œ í˜¼ì… í™•ì¸
2. í•„í„°ë§ ì ìš© í›„ â†’ HR ë¬¸ì„œ ì™„ì „ ì œê±° í™•ì¸

---

### A-2: Source Citation ê°•í™” (NotebookLM ìˆ˜ì¤€) â­â­â­

**ìš°ì„ ìˆœìœ„**: ìµœê³  (3ì¼ ì†Œìš”)
**ì˜ˆìƒ íš¨ê³¼**: ì¶œì²˜ ì •í™•ë„ 95%, ì‚¬ìš©ì ì‹ ë¢°ë„ +30%

**í˜„ì¬ ë¬¸ì œ**:
```
## ì°¸ì¡° ì •ë³´
- [kFRET ê°’]: ë¬¸ì„œ #4, í˜ì´ì§€ 4 / ì„¹ì…˜ "ë³¸ë¬¸"

ë¬¸ì œì :
1. ì¶œì²˜ í‘œì‹œ ì¼ê´€ì„± ë¶€ì¡±
2. í˜ì´ì§€/ì„¹ì…˜ ì •ë³´ ë•Œë•Œë¡œ ë¶€ì •í™•
3. ì¶œì²˜ ì‹ ë¢°ë„ ì ìˆ˜ ë¯¸í‘œì‹œ
4. ë¬¸ì¥ ë‹¨ìœ„ ì¶œì²˜ ë§¤í•‘ ì—†ìŒ
```

**ê°œì„  ëª©í‘œ (NotebookLM ìŠ¤íƒ€ì¼)**:
```
ì œê³µëœ ë¬¸ì„œì— ë”°ë¥´ë©´, kFRET ê°’ì€ 87.8%ì…ë‹ˆë‹¤ [HF_OLED_Nature_Photonics_2024.pptx, slide 5, ì‹ ë¢°ë„: 826.2].

ë˜í•œ ACRSA ì¬ë£Œë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤ [HF_OLED_Nature_Photonics_2024.pptx, slide 3, ì‹ ë¢°ë„: 792.8].
```

**êµ¬í˜„ ìœ„ì¹˜**: `utils/rag_chain.py` (ìƒˆ ë©”ì„œë“œ ì¶”ê°€)

**êµ¬í˜„ ë‚´ìš©**:
```python
def _generate_source_citations(self, answer: str, sources: List[Document]) -> str:
    """NotebookLM ìŠ¤íƒ€ì¼ ì¶œì²˜ ì¸ë¼ì¸ í‘œì‹œ

    Args:
        answer: ìƒì„±ëœ ë‹µë³€
        sources: ì‚¬ìš©ëœ ì¶œì²˜ ë¬¸ì„œë“¤

    Returns:
        ì¶œì²˜ê°€ ì¸ë¼ì¸ìœ¼ë¡œ í‘œì‹œëœ ë‹µë³€
    """

    # 1. ë‹µë³€ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    sentences = self._split_sentences(answer)

    cited_sentences = []
    for sentence in sentences:
        # 2. ë¬¸ì¥ê³¼ ê°€ì¥ ê´€ë ¨ëœ ì¶œì²˜ ì°¾ê¸°
        best_source = self._find_best_source_for_sentence(sentence, sources)

        if best_source:
            # 3. ì¸ë¼ì¸ ì¶œì²˜ ìƒì„±
            citation = self._format_citation(best_source)
            cited_sentence = f"{sentence.strip()} {citation}"
        else:
            cited_sentence = sentence.strip()

        cited_sentences.append(cited_sentence)

    return " ".join(cited_sentences)

def _find_best_source_for_sentence(self, sentence: str, sources: List[Document]) -> Optional[Document]:
    """ë¬¸ì¥ê³¼ ê°€ì¥ ê´€ë ¨ëœ ì¶œì²˜ ì°¾ê¸° (semantic similarity)"""

    # 1. ë¬¸ì¥ ì„ë² ë”©
    sentence_embedding = self._embed_text(sentence)

    # 2. ê° ì¶œì²˜ì™€ ìœ ì‚¬ë„ ê³„ì‚°
    best_source = None
    best_similarity = 0.0

    for source in sources:
        source_embedding = self._embed_text(source.page_content)
        similarity = self._cosine_similarity(sentence_embedding, source_embedding)

        if similarity > best_similarity and similarity > 0.5:  # ì„ê³„ê°’
            best_similarity = similarity
            best_source = source

    return best_source

def _format_citation(self, source: Document) -> str:
    """ì¶œì²˜ë¥¼ NotebookLM ìŠ¤íƒ€ì¼ë¡œ í¬ë§·"""

    file_name = source.metadata.get('file_name', 'Unknown')
    page = source.metadata.get('page', '?')
    score = source.metadata.get('score', 0.0)

    # ì§§ì€ íŒŒì¼ëª… ì¶”ì¶œ (í™•ì¥ì ì œê±°)
    short_name = file_name.rsplit('.', 1)[0]
    if len(short_name) > 30:
        short_name = short_name[:27] + "..."

    return f"[{short_name}, p.{page}, ì‹ ë¢°ë„: {score:.1f}]"
```

**í…ŒìŠ¤íŠ¸ ë°©ë²•**:
1. ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ í•©ì„±í•˜ëŠ” ì§ˆë¬¸
2. ê° ë¬¸ì¥ì˜ ì¶œì²˜ê°€ ì •í™•íˆ í‘œì‹œë˜ëŠ”ì§€ í™•ì¸
3. ì¶œì²˜ ì‹ ë¢°ë„ ì ìˆ˜ í™•ì¸

---

### A-3: Answer Verification ê°œì„  â­â­â­

**ìš°ì„ ìˆœìœ„**: ë†’ìŒ (2ì¼ ì†Œìš”)
**ì˜ˆìƒ íš¨ê³¼**: ì¬ìƒì„± ë¹ˆë„ 50% ê°ì†Œ, ì‘ë‹µ ì‹œê°„ 10-15ì´ˆ ë‹¨ì¶•

**í˜„ì¬ ë¬¸ì œ**:
```
Query: "kFRET ê°’ì€?"
1ì°¨ ë‹µë³€: ê²€ì¦ ì‹¤íŒ¨ (ê¸ˆì§€ êµ¬ë¬¸ "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" ì‚¬ìš©)
2ì°¨ ì¬ìƒì„±: ì„±ê³µ
â†’ ì¶”ê°€ LLM í˜¸ì¶œ (10-15ì´ˆ ì§€ì—°, ë¹„ìš© ì¦ê°€)
```

**ê°œì„ ì•ˆ 1: Prompt Engineering ê°•í™”**

**êµ¬í˜„ ìœ„ì¹˜**: `utils/rag_chain.py::base_prompt_template`

**ê°œì„  ë‚´ìš©**:
```python
self.base_prompt_template = """ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.

âš ï¸ ì¤‘ìš” ê·œì¹™:
1. **ë¬¸ì„œ ìš°ì„  ì›ì¹™**: ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ì°¾ì•„ ë‹µë³€í•˜ì„¸ìš”.
2. **ì¼ë°˜ ì§€ì‹ ê¸ˆì§€**: ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ì¸¡í•˜ê±°ë‚˜ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.
3. **ê¸ˆì§€ í‘œí˜„**: ë‹¤ìŒ í‘œí˜„ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”:
   âŒ "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
   âŒ "ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤"
   âŒ "í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"

4. **ëŒ€ì‹  ì‚¬ìš©í•  í‘œí˜„**:
   âœ… "ì œê³µëœ ë¬¸ì„œì— ë”°ë¥´ë©´..."
   âœ… "ë¬¸ì„œ #1ì˜ 5í˜ì´ì§€ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
   âœ… "ì§ì ‘ì ì¸ ìˆ˜ì¹˜ëŠ” ëª…ì‹œë˜ì–´ ìˆì§€ ì•Šì§€ë§Œ, ê´€ë ¨ ì •ë³´ëŠ”..."

5. **NotebookLM ìŠ¤íƒ€ì¼ ë‹µë³€ í˜•ì‹**:
   "According to the provided document [íŒŒì¼ëª…, í˜ì´ì§€], [êµ¬ì²´ì  ì •ë³´]..."

ì´ì „ ëŒ€í™” ë‚´ìš©:
{chat_history}

ì°¸ê³  ë¬¸ì„œ:
{context}

í˜„ì¬ ì§ˆë¬¸: {question}

ë‹µë³€ ì ˆì°¨:
1ë‹¨ê³„ [ë¬¸ì„œ ë¶„ì„]:
   - ê° ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš© íŒŒì•…
   - ì§ˆë¬¸ ê´€ë ¨ í‚¤ì›Œë“œ ì‹ë³„
   - ë™ì˜ì–´, ì•½ì–´ ê³ ë ¤

2ë‹¨ê³„ [ì •ë³´ ì¶”ì¶œ]:
   - ì§ˆë¬¸ì— ì§ì ‘ ë‹µí•˜ëŠ” ì •ë³´ ì‹ë³„
   - ìˆ˜ì¹˜, ë‚ ì§œ, ì´ë¦„ ë“± êµ¬ì²´ì  ì‚¬ì‹¤ ì¶”ì¶œ
   - ì—¬ëŸ¬ ë¬¸ì„œ ì •ë³´ ëª¨ë‘ í¬í•¨

3ë‹¨ê³„ [ì •ë³´ í†µí•©]:
   - ì¶”ì¶œí•œ ì •ë³´ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ì„±
   - ê´€ë ¨ì„± ë†’ì€ ì •ë³´ ìš°ì„  ë°°ì¹˜

4ë‹¨ê³„ [ë‹µë³€ ìƒì„±]:
   - ë¬¸ì„œì—ì„œ í™•ì¸ëœ ì‚¬ì‹¤ë§Œ ì‚¬ìš©
   - ê° ì‚¬ì‹¤ë§ˆë‹¤ ì¶œì²˜ ëª…ì‹œ (ë¬¸ì„œ ë²ˆí˜¸, í˜ì´ì§€/ì„¹ì…˜)
   - ì¶œì²˜ì™€ í•¨ê»˜ ìì—°ìŠ¤ëŸ½ê²Œ ë¬¸ì¥ êµ¬ì„±

ë‹µë³€:"""
```

**ê°œì„ ì•ˆ 2: Self-Consistency Check ì¶”ê°€**

**êµ¬í˜„ ìœ„ì¹˜**: `utils/rag_chain.py` (ìƒˆ ë©”ì„œë“œ)

**êµ¬í˜„ ë‚´ìš©**:
```python
def _generate_with_self_consistency(self, question: str, context: str, n: int = 3) -> Dict[str, Any]:
    """ì—¬ëŸ¬ ë²ˆ ìƒì„± í›„ ì¼ê´€ì„± ê²€ì¦

    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        context: ê²€ìƒ‰ëœ ë¬¸ë§¥
        n: ìƒì„± íšŸìˆ˜ (ê¸°ë³¸ 3íšŒ)

    Returns:
        {
            'answer': ìµœì¢… ë‹µë³€,
            'consistency': ì¼ê´€ì„± ì ìˆ˜ (0-1),
            'variants': ìƒì„±ëœ ë‹µë³€ë“¤
        }
    """

    # 1. Në²ˆ ë…ë¦½ì ìœ¼ë¡œ ë‹µë³€ ìƒì„± (temperature ì•½ê°„ ì˜¬ë ¤ì„œ)
    print(f"  ğŸ”„ Self-consistency check: {n}íšŒ ìƒì„± ì¤‘...")

    original_temp = self.temperature
    self.temperature = 0.5  # ì•½ê°„ ë‹¤ì–‘ì„± ì¶”ê°€

    answers = []
    for i in range(n):
        answer = self._generate_answer_internal(question, context)
        answers.append(answer)
        print(f"    âœ“ {i+1}ë²ˆì§¸ ìƒì„± ì™„ë£Œ ({len(answer)} chars)")

    self.temperature = original_temp

    # 2. ë‹µë³€ ê°„ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
    consistency_score = self._calculate_answer_consistency(answers)
    print(f"  ğŸ“Š ì¼ê´€ì„± ì ìˆ˜: {consistency_score:.2%}")

    # 3. ì¼ê´€ì„±ì— ë”°ë¼ ì²˜ë¦¬
    if consistency_score > 0.8:
        # ë†’ì€ ì¼ê´€ì„±: ê°€ì¥ ìƒì„¸í•œ ë‹µë³€ ì„ íƒ
        best_answer = max(answers, key=lambda a: len(a))
        print(f"  âœ… ë†’ì€ ì¼ê´€ì„±: ìµœìƒ ë‹µë³€ ì„ íƒ")

    elif consistency_score > 0.5:
        # ì¤‘ê°„ ì¼ê´€ì„±: ê³µí†µ ì •ë³´ ì¶”ì¶œ
        best_answer = self._extract_common_info(answers)
        best_answer = f"âš ï¸ ì¤‘ê°„ ì‹ ë¢°ë„ (ì¼ê´€ì„±: {consistency_score:.1%})\n\n{best_answer}"
        print(f"  âš ï¸ ì¤‘ê°„ ì¼ê´€ì„±: ê³µí†µ ì •ë³´ ì¶”ì¶œ")

    else:
        # ë‚®ì€ ì¼ê´€ì„±: ê²½ê³ ì™€ í•¨ê»˜ ì²« ë²ˆì§¸ ë‹µë³€
        best_answer = f"âš ï¸ ë‚®ì€ ì‹ ë¢°ë„ (ì¼ê´€ì„±: {consistency_score:.1%})\nì œê³µëœ ë¬¸ì„œì—ì„œ ëª…í™•í•œ ë‹µë³€ì„ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤.\n\n{answers[0]}"
        print(f"  âš ï¸ ë‚®ì€ ì¼ê´€ì„±: ê²½ê³  í‘œì‹œ")

    return {
        'answer': best_answer,
        'consistency': consistency_score,
        'variants': answers
    }

def _calculate_answer_consistency(self, answers: List[str]) -> float:
    """ë‹µë³€ë“¤ ê°„ì˜ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°

    ë°©ë²•: ëª¨ë“  ë‹µë³€ ìŒì˜ ìœ ì‚¬ë„ í‰ê· 
    """
    from itertools import combinations

    if len(answers) < 2:
        return 1.0

    # ëª¨ë“  ìŒì˜ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = []
    for ans1, ans2 in combinations(answers, 2):
        # ê°„ë‹¨í•œ Jaccard ìœ ì‚¬ë„ (ë‹¨ì–´ ê¸°ë°˜)
        words1 = set(ans1.lower().split())
        words2 = set(ans2.lower().split())

        if len(words1) == 0 and len(words2) == 0:
            similarity = 1.0
        elif len(words1) == 0 or len(words2) == 0:
            similarity = 0.0
        else:
            intersection = words1 & words2
            union = words1 | words2
            similarity = len(intersection) / len(union)

        similarities.append(similarity)

    return sum(similarities) / len(similarities)

def _extract_common_info(self, answers: List[str]) -> str:
    """ì—¬ëŸ¬ ë‹µë³€ì—ì„œ ê³µí†µ ì •ë³´ ì¶”ì¶œ"""

    # ê° ë‹µë³€ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    all_sentences = []
    for answer in answers:
        sentences = [s.strip() for s in answer.split('.') if s.strip()]
        all_sentences.extend(sentences)

    # ê°€ì¥ ë¹ˆë²ˆí•œ ë¬¸ì¥ë“¤ ì„ íƒ (2ê°œ ì´ìƒ ë‹µë³€ì— ë“±ì¥)
    from collections import Counter
    sentence_counts = Counter(all_sentences)

    common_sentences = [
        sentence for sentence, count in sentence_counts.items()
        if count >= 2 or count >= len(answers) * 0.5
    ]

    if common_sentences:
        return '. '.join(common_sentences[:5]) + '.'
    else:
        # ê³µí†µ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ë‹µë³€ ë°˜í™˜
        return answers[0]
```

**ì‚¬ìš© ë°©ë²•**:
```python
# ê¸°ì¡´ generate_answer() ë©”ì„œë“œì—ì„œ ì˜µì…˜ìœ¼ë¡œ ì‚¬ìš©
if self.enable_self_consistency:
    result = self._generate_with_self_consistency(question, context, n=3)
    return result['answer']
else:
    return self._generate_answer_internal(question, context)
```

**í…ŒìŠ¤íŠ¸ ë°©ë²•**:
1. ëª¨í˜¸í•œ ì§ˆë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
2. ì¼ê´€ì„± ì ìˆ˜ í™•ì¸
3. ì¬ìƒì„± ë¹ˆë„ ì¸¡ì • (Before/After)

---

### Phase A ì˜ˆìƒ ì„±ê³¼

| ì§€í‘œ | Before (v3.1) | After (Phase A) | ê°œì„  |
|------|--------------|----------------|------|
| **í¬ë¡œìŠ¤ ë„ë©”ì¸ ì˜¤ì—¼** | 4.5% | 0% | -100% |
| **ì¶œì²˜ ì •í™•ë„** | ~60% | 95% | +58% |
| **ì¬ìƒì„± ë¹ˆë„** | ~20% | ~10% | -50% |
| **ì‚¬ìš©ì ì‹ ë¢°ë„** | - | - | +30% |
| **ì‘ë‹µ ì‹œê°„** | í‰ê·  92ì´ˆ | í‰ê·  77-82ì´ˆ | -10-15ì´ˆ |

**ì´ ì •í™•ë„ í–¥ìƒ**: +15-25%
**NotebookLM ëŒ€ë¹„**: ë™ë“± ìˆ˜ì¤€ ë‹¬ì„±

---

## Phase B: ë‹¨ê¸° ê°œì„  (1-2ê°œì›”)

### B-1: Query Rewriting ê³ ë„í™” â­â­â­

**ìš°ì„ ìˆœìœ„**: ë†’ìŒ (1ì£¼ ì†Œìš”)
**ì˜ˆìƒ íš¨ê³¼**: ê²€ìƒ‰ ì •í™•ë„ +25-35%

**í˜„ì¬ í•œê³„**:
```python
# í˜„ì¬ëŠ” ë™ì˜ì–´ í™•ì¥ë§Œ ìˆìŒ
enable_synonym_expansion: bool = True
multi_query_num: int = 3
```

**ê°œì„  ëª©í‘œ**:
1. Context-aware Rewriting (ëŒ€í™” ë§¥ë½ ë°˜ì˜)
2. Terminology Expansion (ì „ë¬¸ ìš©ì–´ í™•ì¥)
3. Question Decomposition (ë³µì¡í•œ ì§ˆë¬¸ ë¶„í•´)
4. HyDE (Hypothetical Document Embedding)

**êµ¬í˜„ ìœ„ì¹˜**: ìƒˆ íŒŒì¼ `utils/advanced_query_rewriter.py`

**êµ¬í˜„ ë‚´ìš©**:
```python
"""
ê³ ê¸‰ Query Rewriting
NotebookLM ìŠ¤íƒ€ì¼ì˜ ë‹¤ì¸µ ì¿¼ë¦¬ ë³€í™˜
"""
from typing import List, Dict, Any, Optional
from langchain_core.language_models import BaseLanguageModel


class AdvancedQueryRewriter:
    """ê³ ê¸‰ ì¿¼ë¦¬ ì¬ì‘ì„±ê¸°"""

    def __init__(self, llm: BaseLanguageModel, domain_lexicon: Dict[str, List[str]] = None):
        self.llm = llm
        self.domain_lexicon = domain_lexicon or {}

        # ê¸°ë³¸ ë„ë©”ì¸ ìš©ì–´ ì‚¬ì „
        self.default_lexicon = {
            "TADF": ["Thermally Activated Delayed Fluorescence", "ì—´ í™œì„±í™” ì§€ì—° í˜•ê´‘", "íƒ€í”„"],
            "OLED": ["Organic Light-Emitting Diode", "ìœ ê¸° ë°œê´‘ ë‹¤ì´ì˜¤ë“œ", "ìœ ê¸° EL"],
            "FRET": ["FÃ¶rster Resonance Energy Transfer", "í¬ìŠ¤í„° ê³µëª… ì—ë„ˆì§€ ì „ë‹¬"],
            "EQE": ["External Quantum Efficiency", "ì™¸ë¶€ ì–‘ì íš¨ìœ¨"],
            # ... ë” ë§ì€ ìš©ì–´ ì¶”ê°€
        }
        self.domain_lexicon.update(self.default_lexicon)

    def rewrite_query(self, question: str, chat_history: List[str] = None,
                      mode: str = "hybrid") -> List[str]:
        """ì¿¼ë¦¬ ì¬ì‘ì„±

        Args:
            question: ì›ë³¸ ì§ˆë¬¸
            chat_history: ëŒ€í™” ê¸°ë¡
            mode: ì¬ì‘ì„± ëª¨ë“œ
                - "simple": ì›ë³¸ + ìš©ì–´ í™•ì¥ë§Œ
                - "hybrid": ì›ë³¸ + ìš©ì–´ í™•ì¥ + HyDE
                - "aggressive": ëª¨ë“  ë°©ë²• ì ìš©

        Returns:
            ì¬ì‘ì„±ëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        rewritten_queries = [question]  # í•­ìƒ ì›ë³¸ í¬í•¨

        # 1. Context-aware Rewriting (ëŒ€í™” ë§¥ë½)
        if chat_history and len(chat_history) > 0:
            contextual_q = self._add_context(question, chat_history)
            if contextual_q != question:
                rewritten_queries.append(contextual_q)

        # 2. Terminology Expansion (ì „ë¬¸ ìš©ì–´ í™•ì¥)
        expanded_queries = self._expand_terminology(question)
        rewritten_queries.extend(expanded_queries)

        if mode in ["hybrid", "aggressive"]:
            # 3. HyDE (Hypothetical Document Embedding)
            hyde_query = self._generate_hypothetical_answer(question)
            if hyde_query:
                rewritten_queries.append(hyde_query)

        if mode == "aggressive":
            # 4. Question Decomposition (ë³µì¡í•œ ì§ˆë¬¸ ë¶„í•´)
            if self._is_complex_question(question):
                sub_queries = self._decompose_question(question)
                rewritten_queries.extend(sub_queries)

        # ì¤‘ë³µ ì œê±°
        unique_queries = []
        seen = set()
        for q in rewritten_queries:
            q_normalized = q.lower().strip()
            if q_normalized not in seen:
                unique_queries.append(q)
                seen.add(q_normalized)

        print(f"  ğŸ”„ Query Rewriting: {len(unique_queries)}ê°œ ì¿¼ë¦¬ ìƒì„±")
        for i, q in enumerate(unique_queries[:5], 1):  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
            print(f"    {i}. {q[:80]}...")

        return unique_queries

    def _add_context(self, question: str, chat_history: List[str]) -> str:
        """ëŒ€í™” ë§¥ë½ì„ ì§ˆë¬¸ì— ì¶”ê°€"""

        # ìµœê·¼ 2ê°œ ëŒ€í™”ë§Œ ì‚¬ìš©
        recent_history = chat_history[-2:] if len(chat_history) > 2 else chat_history

        if not recent_history:
            return question

        # ëŒ€ëª…ì‚¬ í•´ê²° (ì´ê²ƒ, ê·¸ê²ƒ, ì•ì—ì„œ ì–¸ê¸‰í•œ ë“±)
        pronouns = ["ì´ê²ƒ", "ê·¸ê²ƒ", "ì €ê²ƒ", "ê·¸ê±°", "ì´ê±°", "ì•ì—ì„œ", "ìœ„ì—ì„œ"]
        has_pronoun = any(p in question for p in pronouns)

        if has_pronoun:
            # LLMìœ¼ë¡œ ëŒ€ëª…ì‚¬ í•´ê²°
            prompt = f"""ëŒ€í™” ê¸°ë¡:
{chr(10).join(recent_history)}

í˜„ì¬ ì§ˆë¬¸: {question}

í˜„ì¬ ì§ˆë¬¸ì˜ ëŒ€ëª…ì‚¬(ì´ê²ƒ, ê·¸ê²ƒ, ì•ì—ì„œ ì–¸ê¸‰í•œ ë“±)ë¥¼ ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬ êµ¬ì²´ì ìœ¼ë¡œ ë°”ê¿”ì£¼ì„¸ìš”.
ëŒ€ëª…ì‚¬ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

ì¬ì‘ì„±ëœ ì§ˆë¬¸:"""

            try:
                contextual_q = self.llm.invoke(prompt).strip()
                return contextual_q
            except Exception as e:
                print(f"    âš ï¸ ë§¥ë½ ì¶”ê°€ ì‹¤íŒ¨: {e}")
                return question

        return question

    def _expand_terminology(self, question: str) -> List[str]:
        """ì „ë¬¸ ìš©ì–´ í™•ì¥"""

        expanded_queries = []

        # ì§ˆë¬¸ì—ì„œ ë„ë©”ì¸ ìš©ì–´ ì°¾ê¸°
        found_terms = []
        for term, expansions in self.domain_lexicon.items():
            if term.lower() in question.lower():
                found_terms.append((term, expansions))

        # ê° ìš©ì–´ë¥¼ í™•ì¥í˜•ìœ¼ë¡œ ëŒ€ì²´
        for term, expansions in found_terms:
            for expansion in expansions[:2]:  # ìµœëŒ€ 2ê°œ í™•ì¥í˜•ë§Œ
                expanded_q = question.replace(term, expansion)
                if expanded_q != question:
                    expanded_queries.append(expanded_q)

        return expanded_queries[:3]  # ìµœëŒ€ 3ê°œë§Œ

    def _generate_hypothetical_answer(self, question: str) -> Optional[str]:
        """HyDE: ê°€ìƒì˜ ë‹µë³€ ìƒì„± í›„ ì´ë¥¼ ì¿¼ë¦¬ë¡œ ì‚¬ìš©

        ì›ë¦¬: "ì¢‹ì€ ë‹µë³€"ì„ ë¨¼ì € ìƒì„±í•˜ê³ , ê·¸ ë‹µë³€ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ìŒ
        """

        prompt = f"""ì§ˆë¬¸: {question}

ìœ„ ì§ˆë¬¸ì— ëŒ€í•œ ì´ìƒì ì¸ ë‹µë³€ì„ **ì•„ì£¼ ì§§ê²Œ** (2-3ë¬¸ì¥) ì‘ì„±í•´ì£¼ì„¸ìš”.
ì‹¤ì œ ì •ë³´ê°€ ì•„ë‹Œ, ë‹µë³€ì´ ì–´ë–¤ í˜•ì‹ê³¼ ë‚´ìš©ì„ ê°€ì ¸ì•¼ í•˜ëŠ”ì§€ë§Œ ë³´ì—¬ì£¼ì„¸ìš”.

ê°€ìƒ ë‹µë³€:"""

        try:
            hypothetical_answer = self.llm.invoke(prompt).strip()

            # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸° (200ì ì œí•œ)
            if len(hypothetical_answer) > 200:
                hypothetical_answer = hypothetical_answer[:200] + "..."

            return hypothetical_answer

        except Exception as e:
            print(f"    âš ï¸ HyDE ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def _is_complex_question(self, question: str) -> bool:
        """ë³µì¡í•œ ì§ˆë¬¸ì¸ì§€ íŒë‹¨"""

        # ë³µì¡ë„ ì§€í‘œ
        indicators = {
            "multi_part": ["ê·¸ë¦¬ê³ ", "ë˜í•œ", "ê·¸ë¦¬ê³ ", "ê·¸ ë‹¤ìŒ", "ì´í›„"],
            "comparison": ["ë¹„êµ", "ì°¨ì´", "ëŒ€ë¹„", "vs", "versus"],
            "causation": ["ì›ì¸", "ì´ìœ ", "ì™œ", "ì–´ë–»ê²Œ", "ë©”ì»¤ë‹ˆì¦˜"],
            "conjunction": ["ë°", "ì™€", "ê³¼"],
        }

        complexity_score = 0

        # 1. ë‹¤ì¤‘ íŒŒíŠ¸ ì§ˆë¬¸
        for keyword in indicators["multi_part"]:
            if keyword in question:
                complexity_score += 1

        # 2. ë¹„êµ ì§ˆë¬¸
        for keyword in indicators["comparison"]:
            if keyword in question:
                complexity_score += 2

        # 3. ì¸ê³¼ ê´€ê³„ ì§ˆë¬¸
        for keyword in indicators["causation"]:
            if keyword in question:
                complexity_score += 1

        # 4. ì§ˆë¬¸ ê¸¸ì´
        if len(question) > 50:
            complexity_score += 1

        return complexity_score >= 3

    def _decompose_question(self, question: str) -> List[str]:
        """ë³µì¡í•œ ì§ˆë¬¸ì„ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´"""

        prompt = f"""ë‹¤ìŒ ë³µì¡í•œ ì§ˆë¬¸ì„ 2-3ê°œì˜ ê°„ë‹¨í•œ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•´ì£¼ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: {question}

í•˜ìœ„ ì§ˆë¬¸ë“¤ (ê° ì¤„ì— í•˜ë‚˜ì”©, ë²ˆí˜¸ ì—†ì´):"""

        try:
            response = self.llm.invoke(prompt).strip()

            # ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
            sub_queries = [
                line.strip().lstrip('0123456789.-) ')
                for line in response.split('\n')
                if line.strip() and not line.strip().startswith('#')
            ]

            # ìœ íš¨í•œ ì§ˆë¬¸ë§Œ (ìµœì†Œ 5ì ì´ìƒ)
            sub_queries = [q for q in sub_queries if len(q) >= 5]

            return sub_queries[:3]  # ìµœëŒ€ 3ê°œ

        except Exception as e:
            print(f"    âš ï¸ ì§ˆë¬¸ ë¶„í•´ ì‹¤íŒ¨: {e}")
            return []
```

**ì‚¬ìš© ë°©ë²•**:
```python
# RAGChainì— í†µí•©
class RAGChain:
    def __init__(self, ...):
        # ...
        self.query_rewriter = AdvancedQueryRewriter(
            llm=self.llm,
            domain_lexicon=self._domain_lexicon
        )

    def _search_candidates(self, question: str, chat_history: List[str] = None):
        # Query Rewriting ì ìš©
        rewritten_queries = self.query_rewriter.rewrite_query(
            question=question,
            chat_history=chat_history,
            mode="hybrid"  # ë˜ëŠ” "aggressive"
        )

        # ê° ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ í›„ ê²°ê³¼ ë³‘í•©
        all_candidates = []
        for query in rewritten_queries:
            candidates = self._search_single_query(query)
            all_candidates.extend(candidates)

        # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ í•©ì‚°
        merged_candidates = self._merge_and_deduplicate(all_candidates)

        return merged_candidates
```

**í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤**:
```python
# 1. ëŒ€ëª…ì‚¬ í•´ê²° í…ŒìŠ¤íŠ¸
chat_history = ["TADFë€ ë¬´ì—‡ì¸ê°€ìš”?", "ì—´ í™œì„±í™” ì§€ì—° í˜•ê´‘ì…ë‹ˆë‹¤."]
question = "ê·¸ê²ƒì˜ ì–‘ì íš¨ìœ¨ì€?"
# â†’ "TADFì˜ ì–‘ì íš¨ìœ¨ì€?"

# 2. ìš©ì–´ í™•ì¥ í…ŒìŠ¤íŠ¸
question = "TADFì˜ íš¨ìœ¨ì€?"
# â†’ ["TADFì˜ íš¨ìœ¨ì€?", "Thermally Activated Delayed Fluorescenceì˜ íš¨ìœ¨ì€?", "ì—´ í™œì„±í™” ì§€ì—° í˜•ê´‘ì˜ íš¨ìœ¨ì€?"]

# 3. HyDE í…ŒìŠ¤íŠ¸
question = "OLEDì˜ ì™¸ë¶€ ì–‘ì íš¨ìœ¨ì€?"
# â†’ "OLEDì˜ ì™¸ë¶€ ì–‘ì íš¨ìœ¨ì€ ì•½ 20-30% ì •ë„ì´ë©°, ìµœì‹  ê¸°ìˆ ë¡œëŠ” 40%ê¹Œì§€ ë‹¬ì„± ê°€ëŠ¥í•©ë‹ˆë‹¤..."

# 4. ì§ˆë¬¸ ë¶„í•´ í…ŒìŠ¤íŠ¸
question = "TADF ì¬ë£Œì™€ OLED íš¨ìœ¨ì˜ ê´€ê³„ë¥¼ LGë””ìŠ¤í”Œë ˆì´ ë‰´ìŠ¤ì™€ ì—°ê²°í•´ì„œ ì„¤ëª…í•´ì¤˜"
# â†’ ["TADF ì¬ë£Œë€?", "OLED íš¨ìœ¨ì´ë€?", "TADFì™€ OLED íš¨ìœ¨ì˜ ê´€ê³„ëŠ”?", "LGë””ìŠ¤í”Œë ˆì´ ê´€ë ¨ ë‰´ìŠ¤ëŠ”?"]
```

---

### B-2: Confidence Score ì¶”ê°€ â­â­

**ìš°ì„ ìˆœìœ„**: ì¤‘ê°„ (1ì£¼ ì†Œìš”)
**ì˜ˆìƒ íš¨ê³¼**: ì‚¬ìš©ì ì‹ ë¢°ë„ +40%, Hallucination ê°ì§€ ì¦‰ì‹œ ê°€ëŠ¥

**ëª©í‘œ**: NotebookLMì˜ "Confidence-based Response" ìˆ˜ì¤€ ë‹¬ì„±

**êµ¬í˜„ ìœ„ì¹˜**: `utils/rag_chain.py` (ìƒˆ ë©”ì„œë“œ)

**êµ¬í˜„ ë‚´ìš©**:
```python
def generate_with_confidence(self, question: str, chat_history: List[str] = None) -> Dict[str, Any]:
    """ì‹ ë¢°ë„ ì ìˆ˜ì™€ í•¨ê»˜ ë‹µë³€ ìƒì„±

    Returns:
        {
            'answer': ìµœì¢… ë‹µë³€ (ì‹ ë¢°ë„ í‘œì‹œ í¬í•¨),
            'confidence': ì¢…í•© ì‹ ë¢°ë„ (0-100),
            'confidence_factors': {
                'source_relevance': ì¶œì²˜ ê´€ë ¨ì„±,
                'answer_consistency': ë‹µë³€ ì¼ê´€ì„±,
                'category_match': ì¹´í…Œê³ ë¦¬ ì¼ì¹˜ë„,
                'reranker_score': Re-ranker í‰ê·  ì ìˆ˜
            },
            'sources': ì‚¬ìš©ëœ ì¶œì²˜ ë¦¬ìŠ¤íŠ¸
        }
    """

    # 1. ì¼ë°˜ ë‹µë³€ ìƒì„±
    answer_result = self.generate_answer(question, chat_history)

    # 2. ì‹ ë¢°ë„ ê³„ì‚°
    confidence_factors = self._calculate_confidence_factors(
        question=question,
        answer=answer_result['answer'],
        sources=answer_result['sources'],
        categories=answer_result.get('categories', [])
    )

    # 3. ì¢…í•© ì‹ ë¢°ë„ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
    weights = {
        'source_relevance': 0.35,      # ì¶œì²˜ ê´€ë ¨ì„± (ê°€ì¥ ì¤‘ìš”)
        'answer_consistency': 0.25,    # ë‹µë³€ ì¼ê´€ì„±
        'category_match': 0.25,        # ì¹´í…Œê³ ë¦¬ ì¼ì¹˜ë„
        'reranker_score': 0.15         # Re-ranker ì ìˆ˜
    }

    confidence = sum(
        confidence_factors[k] * weights[k]
        for k in weights.keys()
    ) * 100  # 0-100 ìŠ¤ì¼€ì¼

    # 4. ì‹ ë¢°ë„ì— ë”°ë¥¸ ë‹µë³€ ì¡°ì •
    final_answer = self._format_answer_with_confidence(
        answer=answer_result['answer'],
        confidence=confidence,
        confidence_factors=confidence_factors
    )

    return {
        'answer': final_answer,
        'confidence': confidence,
        'confidence_factors': confidence_factors,
        'sources': answer_result['sources']
    }

def _calculate_confidence_factors(self, question: str, answer: str,
                                   sources: List[Document],
                                   categories: List[str]) -> Dict[str, float]:
    """ì‹ ë¢°ë„ êµ¬ì„± ìš”ì†Œ ê³„ì‚° (ê° 0-1)"""

    factors = {}

    # 1. ì¶œì²˜ ê´€ë ¨ì„± (Source Relevance)
    # Re-ranker ì ìˆ˜ ê¸°ë°˜
    if sources:
        avg_score = np.mean([s.metadata.get('score', 0) for s in sources])
        max_expected_score = 1000  # Re-ranker ìµœëŒ€ ì ìˆ˜ (ëŒ€ëµ)
        factors['source_relevance'] = min(avg_score / max_expected_score, 1.0)
    else:
        factors['source_relevance'] = 0.0

    # 2. ë‹µë³€ ì¼ê´€ì„± (Answer Consistency)
    # Self-consistency ê²°ê³¼ í™œìš© (Phase A-3ì—ì„œ êµ¬í˜„)
    if hasattr(self, '_last_consistency_score'):
        factors['answer_consistency'] = self._last_consistency_score
    else:
        # Fallback: ë‹µë³€ ê¸¸ì´ì™€ êµ¬ì¡°í™” ì •ë„ë¡œ ì¶”ì •
        has_structure = any(marker in answer for marker in ['##', '1.', '-', '*'])
        has_citations = '[' in answer and ']' in answer
        length_score = min(len(answer) / 500, 1.0)  # 500ì ê¸°ì¤€

        factors['answer_consistency'] = (
            0.4 * (1.0 if has_structure else 0.5) +
            0.3 * (1.0 if has_citations else 0.3) +
            0.3 * length_score
        )

    # 3. ì¹´í…Œê³ ë¦¬ ì¼ì¹˜ë„ (Category Match)
    # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ì¹´í…Œê³ ë¦¬ ìˆœë„
    if sources and categories:
        source_categories = [s.metadata.get('category', 'unknown') for s in sources]

        # íƒ€ê²Ÿ ì¹´í…Œê³ ë¦¬ì— ì†í•˜ëŠ” ë¹„ìœ¨
        match_count = sum(1 for sc in source_categories if sc in categories)
        factors['category_match'] = match_count / len(sources)
    else:
        factors['category_match'] = 0.5  # ì¹´í…Œê³ ë¦¬ ì‹œìŠ¤í…œ ë¯¸ì‚¬ìš© ì‹œ ì¤‘ë¦½

    # 4. Re-ranker ì ìˆ˜ ì •ê·œí™” (Reranker Score)
    if sources and self.use_reranker:
        scores = [s.metadata.get('score', 0) for s in sources]

        # ìƒìœ„ 3ê°œ ì ìˆ˜ì˜ í‘œì¤€í¸ì°¨ (ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„± ë†’ìŒ)
        top_scores = sorted(scores, reverse=True)[:3]
        if len(top_scores) > 1:
            score_std = np.std(top_scores)
            max_expected_std = 200  # ê²½í—˜ì  ì„ê³„ê°’
            consistency = max(0, 1 - (score_std / max_expected_std))
            factors['reranker_score'] = consistency
        else:
            factors['reranker_score'] = 0.8
    else:
        factors['reranker_score'] = 0.5  # Re-ranker ë¯¸ì‚¬ìš© ì‹œ ì¤‘ë¦½

    return factors

def _format_answer_with_confidence(self, answer: str, confidence: float,
                                    confidence_factors: Dict[str, float]) -> str:
    """ì‹ ë¢°ë„ì— ë”°ë¼ ë‹µë³€ í¬ë§·"""

    # ì‹ ë¢°ë„ ë¼ë²¨
    if confidence >= 80:
        label = "âœ… ë†’ì€ ì‹ ë¢°ë„"
        color = "ğŸŸ¢"
        message = ""
    elif confidence >= 60:
        label = "âš ï¸ ì¤‘ê°„ ì‹ ë¢°ë„"
        color = "ğŸŸ¡"
        message = "\n*ì¼ë¶€ ì¶”ë¡ ì´ í¬í•¨ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.*\n"
    else:
        label = "âš ï¸ ë‚®ì€ ì‹ ë¢°ë„"
        color = "ğŸ”´"
        message = "\n*ì œê³µëœ ë¬¸ì„œì—ì„œ ëª…í™•í•œ ë‹µë³€ì„ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë‹µë³€ì˜ ì •í™•ì„±ì„ ê²€ì¦í•´ì£¼ì„¸ìš”.*\n"

    # ì‹ ë¢°ë„ í—¤ë” ìƒì„±
    confidence_header = f"""
{color} **{label}**: {confidence:.1f}%

**ì‹ ë¢°ë„ ë¶„ì„**:
- ì¶œì²˜ ê´€ë ¨ì„±: {confidence_factors['source_relevance']*100:.0f}%
- ë‹µë³€ ì¼ê´€ì„±: {confidence_factors['answer_consistency']*100:.0f}%
- ì¹´í…Œê³ ë¦¬ ì¼ì¹˜: {confidence_factors['category_match']*100:.0f}%
- ê²€ìƒ‰ í’ˆì§ˆ: {confidence_factors['reranker_score']*100:.0f}%
{message}
---
"""

    return confidence_header + answer
```

**UI í‘œì‹œ ì˜ˆì‹œ**:
```
ğŸŸ¢ **ë†’ì€ ì‹ ë¢°ë„**: 87.3%

**ì‹ ë¢°ë„ ë¶„ì„**:
- ì¶œì²˜ ê´€ë ¨ì„±: 92%
- ë‹µë³€ ì¼ê´€ì„±: 85%
- ì¹´í…Œê³ ë¦¬ ì¼ì¹˜: 100%
- ê²€ìƒ‰ í’ˆì§ˆ: 78%

---

ì œê³µëœ ë¬¸ì„œì— ë”°ë¥´ë©´, kFRET ê°’ì€ 87.8%ì…ë‹ˆë‹¤ [HF_OLED_Nature_Photonics_2024.pptx, slide 5, ì‹ ë¢°ë„: 826.2].
...
```

---

### B-3: Context Window ìµœì í™” â­â­

**ìš°ì„ ìˆœìœ„**: ì¤‘ê°„ (3ì¼ ì†Œìš”)
**ì˜ˆìƒ íš¨ê³¼**: ë‹¨ìˆœ ì§ˆë¬¸ ì‘ë‹µ ì‹œê°„ 40% ë‹¨ì¶•, ë³µì¡í•œ ì§ˆë¬¸ ì •í™•ë„ +20%

**í˜„ì¬ ë¬¸ì œ**:
```python
# ê³ ì •ëœ top_k
self.top_k = 3
self.reranker_initial_k = max(reranker_initial_k, top_k * 5)  # í•­ìƒ 15ê°œ

ë¬¸ì œ:
- ë‹¨ìˆœ ì§ˆë¬¸ì—ë„ 15ê°œ ë¬¸ì„œ ê²€ìƒ‰ (ê³¼ë‹¤)
- ë³µì¡í•œ ì§ˆë¬¸ì— 3ê°œë§Œ ì‚¬ìš© (ë¶€ì¡±)
```

**ê°œì„  ëª©í‘œ**: ì§ˆë¬¸ ë³µì¡ë„ì— ë”°ë¼ ë™ì ìœ¼ë¡œ context í¬ê¸° ì¡°ì •

**êµ¬í˜„ ìœ„ì¹˜**: `utils/rag_chain.py` (ìƒˆ ë©”ì„œë“œ)

**êµ¬í˜„ ë‚´ìš©**:
```python
def _get_optimal_k(self, question: str, question_type: str = None) -> Dict[str, int]:
    """ì§ˆë¬¸ ë³µì¡ë„ì— ë”°ë¼ ìµœì ì˜ k ê°’ ê²°ì •

    Returns:
        {
            'initial_k': Re-rankerì— ì „ë‹¬í•  ë¬¸ì„œ ìˆ˜,
            'final_k': ìµœì¢… LLMì— ì „ë‹¬í•  ë¬¸ì„œ ìˆ˜
        }
    """

    # 1. ì§ˆë¬¸ íƒ€ì… ê°ì§€ (ì—†ìœ¼ë©´ ìë™ ê°ì§€)
    if question_type is None:
        question_type = self._detect_question_type(question)

    # 2. ì§ˆë¬¸ ë³µì¡ë„ ë¶„ì„
    complexity = self._analyze_question_complexity(question)

    # 3. ì§ˆë¬¸ íƒ€ì…ë³„ ê¸°ë³¸ k ê°’
    base_k_map = {
        "factoid": {"initial": 10, "final": 2},     # ë‹¨ìˆœ ì‚¬ì‹¤ ì§ˆë¬¸
        "definition": {"initial": 12, "final": 3},  # ì •ì˜ ì§ˆë¬¸
        "comparison": {"initial": 20, "final": 6},  # ë¹„êµ ì§ˆë¬¸
        "summary": {"initial": 30, "final": 10},    # ìš”ì•½ ì§ˆë¬¸
        "relationship": {"initial": 25, "final": 8}, # ê´€ê³„ ë¶„ì„
        "general": {"initial": 15, "final": 5}      # ì¼ë°˜ ì§ˆë¬¸
    }

    base_k = base_k_map.get(question_type, base_k_map["general"])

    # 4. ë³µì¡ë„ì— ë”°ë¼ ì¡°ì •
    complexity_multiplier = 1.0 + (complexity - 0.5)  # 0.5-1.5 ë²”ìœ„

    initial_k = int(base_k["initial"] * complexity_multiplier)
    final_k = int(base_k["final"] * complexity_multiplier)

    # 5. ìµœì†Œ/ìµœëŒ€ ì œí•œ
    initial_k = max(5, min(initial_k, 50))  # 5-50 ë²”ìœ„
    final_k = max(2, min(final_k, 15))      # 2-15 ë²”ìœ„

    print(f"  ğŸ“Š ìµœì  k ê°’ ê²°ì •: ì§ˆë¬¸ íƒ€ì…={question_type}, ë³µì¡ë„={complexity:.2f}")
    print(f"     â†’ initial_k={initial_k}, final_k={final_k}")

    return {
        'initial_k': initial_k,
        'final_k': final_k
    }

def _analyze_question_complexity(self, question: str) -> float:
    """ì§ˆë¬¸ ë³µì¡ë„ ë¶„ì„ (0-1)

    ê³ ë ¤ ìš”ì†Œ:
    1. ì§ˆë¬¸ ê¸¸ì´
    2. ë³µí•© í‚¤ì›Œë“œ (ê·¸ë¦¬ê³ , ë˜í•œ, ë¹„êµ ë“±)
    3. ì „ë¬¸ ìš©ì–´ ë°€ë„
    4. ì§ˆë¬¸ êµ¬ì¡° (ë‹¨ìˆœ/ë³µí•©)
    """

    complexity_score = 0.5  # ê¸°ë³¸ê°’

    # 1. ì§ˆë¬¸ ê¸¸ì´ (ê¸€ì ìˆ˜ ê¸°ë°˜)
    length = len(question)
    if length < 20:
        length_score = 0.3
    elif length < 50:
        length_score = 0.5
    elif length < 100:
        length_score = 0.7
    else:
        length_score = 0.9

    # 2. ë³µí•© í‚¤ì›Œë“œ ê°œìˆ˜
    compound_keywords = [
        "ê·¸ë¦¬ê³ ", "ë˜í•œ", "ì´í›„", "ê·¸ ë‹¤ìŒ", "ë¨¼ì €", "ë‹¤ìŒìœ¼ë¡œ",
        "ë¹„êµ", "ì°¨ì´", "ëŒ€ë¹„", "vs", "versus",
        "ì›ì¸", "ì´ìœ ", "ì™œ", "ì–´ë–»ê²Œ", "ë©”ì»¤ë‹ˆì¦˜",
        "ê´€ê³„", "ì˜í–¥", "ìƒê´€ê´€ê³„", "ê²½í–¥"
    ]

    compound_count = sum(1 for kw in compound_keywords if kw in question)
    compound_score = min(compound_count * 0.2, 0.9)

    # 3. ì „ë¬¸ ìš©ì–´ ë°€ë„
    domain_terms_found = sum(
        1 for term in self._domain_lexicon
        if term.lower() in question.lower()
    )

    # ì§ˆë¬¸ ë‹¨ì–´ ìˆ˜
    word_count = len(question.split())
    if word_count > 0:
        term_density = domain_terms_found / word_count
        density_score = min(term_density * 5, 0.9)  # 20% ì´ìƒì´ë©´ 0.9
    else:
        density_score = 0.5

    # 4. ì§ˆë¬¸ ë¶€í˜¸ (ë³µìˆ˜ ì§ˆë¬¸)
    question_marks = question.count('?')
    multi_question_score = min(question_marks * 0.2, 0.8)

    # ì¢…í•© ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
    complexity_score = (
        0.3 * length_score +
        0.3 * compound_score +
        0.2 * density_score +
        0.2 * multi_question_score
    )

    return max(0.1, min(complexity_score, 1.0))  # 0.1-1.0 ë²”ìœ„ë¡œ í´ë¦¬í•‘
```

**ì‚¬ìš© ë°©ë²•**:
```python
# RAGChainì˜ ê²€ìƒ‰ ë©”ì„œë“œì—ì„œ í™œìš©
def _search_candidates(self, question: str, ...):
    # ìµœì  k ê°’ ê²°ì •
    optimal_k = self._get_optimal_k(question)

    # Hybrid Search
    if self.enable_hybrid_search:
        candidates = self.hybrid_retriever.search(
            query=question,
            top_k=optimal_k['initial_k']  # ë™ì ìœ¼ë¡œ ê²°ì •ëœ k ì‚¬ìš©
        )

    # Re-ranking
    if self.use_reranker:
        reranked = self.reranker.compress_documents(
            documents=candidates,
            query=question
        )
        # ìƒìœ„ final_kê°œë§Œ ì„ íƒ
        final_docs = reranked[:optimal_k['final_k']]

    return final_docs
```

**í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤**:
```python
# 1. ë‹¨ìˆœ ì§ˆë¬¸
q1 = "TADFë€?"
# â†’ initial_k=10, final_k=2 (ë¹ ë¦„)

# 2. ì¤‘ê°„ ë³µì¡ë„
q2 = "TADF ì¬ë£Œì˜ ì–‘ì íš¨ìœ¨ì€ ì–¼ë§ˆì¸ê°€?"
# â†’ initial_k=15, final_k=5 (ê¸°ë³¸)

# 3. ë†’ì€ ë³µì¡ë„
q3 = "TADF ì¬ë£Œì™€ OLED íš¨ìœ¨ì˜ ê´€ê³„ë¥¼ ë¹„êµí•˜ê³ , LGë””ìŠ¤í”Œë ˆì´ ë‰´ìŠ¤ì™€ ì—°ê²°í•´ì„œ ì„¤ëª…í•´ì¤˜"
# â†’ initial_k=30, final_k=10 (ë§ìŒ)
```

---

### Phase B ì˜ˆìƒ ì„±ê³¼

| ì§€í‘œ | Phase A | Phase B | ê°œì„  |
|------|---------|---------|------|
| **ê²€ìƒ‰ ì •í™•ë„** | - | - | +25-35% |
| **ì‚¬ìš©ì ì‹ ë¢°ë„** | +30% | +70% | +40% |
| **ì‘ë‹µ ì‹œê°„ (ë‹¨ìˆœ)** | 77-82ì´ˆ | 46-49ì´ˆ | -40% |
| **ì‘ë‹µ ì‹œê°„ (ë³µì¡)** | 77-82ì´ˆ | 82-87ì´ˆ | +5ì´ˆ (ì •í™•ë„ ìœ„í•´) |
| **ë³µì¡í•œ ì§ˆë¬¸ ì •í™•ë„** | - | - | +40-50% |

**ì´ ì •í™•ë„ í–¥ìƒ** (Phase A+B): +45-70%
**NotebookLM ëŒ€ë¹„**: ì´ˆê³¼ ë‹¬ì„± (+10-20%)

---

## Phase C: ì¤‘ê¸° ê°œì„  (2-3ê°œì›”)

### C-1: Multi-hop Reasoning ê°•í™” â­â­â­

**ìš°ì„ ìˆœìœ„**: ë†’ìŒ (2ì£¼ ì†Œìš”)
**ì˜ˆìƒ íš¨ê³¼**: ë³µì¡í•œ ì§ˆë¬¸ ì²˜ë¦¬ +40-50%

**í˜„ì¬ í•œê³„**:
```
Query: "TADF ì¬ë£Œì™€ OLED íš¨ìœ¨ì˜ ê´€ê³„ë¥¼ LGë””ìŠ¤í”Œë ˆì´ ë‰´ìŠ¤ì™€ ì—°ê²°í•´ì„œ ì„¤ëª…í•´ì¤˜"
â†’ ë‹¨ì¼ ê²€ìƒ‰ë§Œ ìˆ˜í–‰, ë„ë©”ì¸ ê°„ ì—°ê²° ì•½í•¨
```

**ê°œì„  ëª©í‘œ**: NotebookLMì˜ "Multi-document Integration" ì´ˆê³¼

**êµ¬í˜„ ìœ„ì¹˜**: ìƒˆ íŒŒì¼ `utils/multi_hop_retriever.py`

**êµ¬í˜„ ê°œìš”**:
```python
class MultiHopRetriever:
    """ë‹¤ë‹¨ê³„ ì¶”ë¡  ê²€ìƒ‰ (Chain-of-Thought Retrieval)"""

    def retrieve_multi_hop(self, question: str, max_hops: int = 3) -> List[Document]:
        """ë‹¤ë‹¨ê³„ ê²€ìƒ‰

        Step 1: ì§ˆë¬¸ ë¶„í•´
        Step 2: ê° í•˜ìœ„ ì§ˆë¬¸ ê²€ìƒ‰
        Step 3: ì¤‘ê°„ ê²°ê³¼ í†µí•©
        Step 4: ì¶”ê°€ ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨
        Step 5: ìµœì¢… í†µí•©
        """

        # 1. ì§ˆë¬¸ ë¶„í•´
        sub_queries = self._decompose_question(question)

        # 2. ê° í•˜ìœ„ ì§ˆë¬¸ ê²€ìƒ‰
        hop_results = []
        for hop_num in range(max_hops):
            hop_docs = []

            for sub_q in sub_queries:
                docs = self._search_single_hop(sub_q)
                hop_docs.extend(docs)

            hop_results.append(hop_docs)

            # 3. ì¤‘ê°„ í†µí•© ë° ë‹¤ìŒ ì§ˆë¬¸ ìƒì„±
            if hop_num < max_hops - 1:
                intermediate_context = self._summarize_findings(hop_docs)
                follow_up_queries = self._generate_follow_up_queries(
                    question, intermediate_context
                )

                if not follow_up_queries:
                    break  # ë” ì´ìƒ ê²€ìƒ‰ ë¶ˆí•„ìš”

                sub_queries = follow_up_queries

        # 4. ìµœì¢… í†µí•©
        all_docs = [doc for hop in hop_results for doc in hop]
        integrated_docs = self._integrate_multi_hop_results(all_docs, question)

        return integrated_docs
```

---

### C-2: Phase 5 - ìŠ¬ë¼ì´ë“œ ê´€ê³„ ê·¸ë˜í”„ â­â­

**ìš°ì„ ìˆœìœ„**: ì¤‘ê°„ (2ì£¼ ì†Œìš”)
**ì˜ˆìƒ íš¨ê³¼**: ë¬¸ë§¥ ì´í•´ +10-15%, "ì•ì—ì„œ ì–¸ê¸‰í•œ..." ì§ˆì˜ 100% ëŒ€ì‘

**êµ¬í˜„ ìœ„ì¹˜**: ìƒˆ íŒŒì¼ `utils/slide_relationship_graph.py`

**êµ¬í˜„ ê°œìš”**:
```python
class SlideRelationshipGraph:
    """ìŠ¬ë¼ì´ë“œ ê°„ ì°¸ì¡° ê´€ê³„ ê·¸ë˜í”„"""

    def build_graph(self, slides: List[Slide]) -> nx.DiGraph:
        """ìŠ¬ë¼ì´ë“œ ê´€ê³„ ê·¸ë˜í”„ êµ¬ì¶•"""

        G = nx.DiGraph()

        # 1. ìˆœì°¨ì  ê´€ê³„ (ì´ì „/ë‹¤ìŒ)
        for i in range(len(slides)):
            if i > 0:
                G.add_edge(i-1, i, type="sequential", weight=0.8)

        # 2. ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ê´€ê³„
        for i in range(len(slides)):
            for j in range(i+1, len(slides)):
                similarity = self._calculate_semantic_similarity(slides[i], slides[j])
                if similarity > 0.7:
                    G.add_edge(i, j, type="semantic", weight=similarity)

        # 3. ëª…ì‹œì  ì°¸ì¡° ê°ì§€
        for i, slide in enumerate(slides):
            references = self._detect_explicit_references(slide.text)
            for ref_idx in references:
                if 0 <= ref_idx < len(slides):
                    G.add_edge(i, ref_idx, type="explicit", weight=1.0)

        return G

    def retrieve_with_graph(self, query: str, primary_slides: List[int]) -> List[Document]:
        """ê·¸ë˜í”„ ê¸°ë°˜ í™•ì¥ ê²€ìƒ‰"""

        expanded_slides = set(primary_slides)

        # ê° primary ìŠ¬ë¼ì´ë“œì˜ ì´ì›ƒ ì¶”ê°€
        for slide_idx in primary_slides:
            neighbors = list(self.graph.neighbors(slide_idx))
            # ê°€ì¤‘ì¹˜ ë†’ì€ ì´ì›ƒë§Œ
            for neighbor in neighbors:
                edge_data = self.graph[slide_idx][neighbor]
                if edge_data['weight'] > 0.7:
                    expanded_slides.add(neighbor)

        return list(expanded_slides)
```

---

### C-3: Smart Vision ì ìš© (ë¹„ìš© íš¨ìœ¨ì ) â­â­

**ìš°ì„ ìˆœìœ„**: ì¤‘ê°„ (1ì£¼ ì†Œìš”)
**ì˜ˆìƒ íš¨ê³¼**: ì •í™•ë„ +10-20%, ë¹„ìš© 70% ì ˆê°

**êµ¬í˜„ ìœ„ì¹˜**: `utils/pptx_chunking_engine.py` ìˆ˜ì •

**êµ¬í˜„ ê°œìš”**:
```python
class SmartVisionChunker(PPTXChunkingEngine):
    """Phase 3 ê²°ê³¼ ê¸°ë°˜ ì„ íƒì  Vision ì ìš©"""

    def process_slide(self, slide, slide_index):
        # Phase 3: ìŠ¬ë¼ì´ë“œ íƒ€ì… ë¶„ë¥˜
        slide_type = self._classify_slide_type(slide)

        # ìŠ¤ë§ˆíŠ¸ Vision ì ìš© (ë³µì¡í•œ ìŠ¬ë¼ì´ë“œë§Œ)
        if slide_type in ["table_heavy", "chart_heavy"]:
            print(f"  ğŸ” [Vision] ë³µì¡í•œ ìŠ¬ë¼ì´ë“œ ê°ì§€, Vision ì ìš©")
            return self._process_with_vision(slide, slide_index)
        else:
            print(f"  ğŸ“ [Algorithm] ë‹¨ìˆœ ìŠ¬ë¼ì´ë“œ, ì•Œê³ ë¦¬ì¦˜ ì²˜ë¦¬")
            return self._process_without_vision(slide)
```

**ì˜ˆìƒ ë¹„ìš©**:
- ì „ì²´ Vision: $1-3 (100 ìŠ¬ë¼ì´ë“œ)
- Smart Vision: $0.3-1 (30% ìŠ¬ë¼ì´ë“œë§Œ)
- ì ˆê°: **70%**

---

### Phase C ì˜ˆìƒ ì„±ê³¼

| ì§€í‘œ | Phase B | Phase C | ê°œì„  |
|------|---------|---------|------|
| **ë³µì¡í•œ ì§ˆë¬¸** | +40-50% | +80-100% | +40-50% |
| **ë¬¸ë§¥ ì´í•´** | - | +10-15% | +10-15% |
| **í‘œ/ì°¨íŠ¸ ì •í™•ë„** | - | +10-20% | +10-20% |
| **Vision ë¹„ìš©** | - | -70% | ì ˆê° |

**ì´ ì •í™•ë„ í–¥ìƒ** (Phase A+B+C): **+100-150%**
**NotebookLM ëŒ€ë¹„**: **+40-70% ì´ˆê³¼ ë‹¬ì„±**

---

## ì˜ˆìƒ ì„±ê³¼

### ë‹¨ê³„ë³„ ì •í™•ë„ í–¥ìƒ (ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„)

| ë‹¨ê³„ | ê¸°ëŠ¥ | ì •í™•ë„ | NotebookLM (86%) ëŒ€ë¹„ |
|------|------|--------|---------------------|
| **í˜„ì¬ (v3.1)** | Phase 1-4 + ì¹´í…Œê³ ë¦¬ | 100% (11/11, ì†Œê·œëª¨) | +14% |
| **Phase A (ì¦‰ì‹œ)** | +Citation +Verification +Filter | +115-125% | +29-39% |
| **Phase B (ë‹¨ê¸°)** | +Query +Confidence +Context | +145-195% | +59-109% |
| **Phase C (ì¤‘ê¸°)** | +Multi-hop +Graph +Vision | +200-250% | +114-164% |

### NotebookLM í•­ëª©ë³„ ë¹„êµ (Phase C ì™„ë£Œ í›„)

| í•­ëª© | NotebookLM | ëª©í‘œ (Phase C) | ìš°ìœ„ |
|------|-----------|---------------|------|
| **ì •í™•ë„** | 86% | **200-250%** (ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„) | âœ… ì´ˆê³¼ |
| **ì¶œì²˜ ì •í™•ë„** | 95% | **95%+** (Phase A) | âœ… ë™ë“± ì´ìƒ |
| **Hallucination ë°©ì§€** | ê°•í•¨ | **ê°•í•¨** (ì¹´í…Œê³ ë¦¬ í•„í„°ë§ 100%) | âœ… ë™ë“± |
| **ëŒ€ëŸ‰ ë¬¸ì„œ** | âŒ 100+ ì•½í•¨ | âœ… **ë¬´ì œí•œ** (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰) | âœ… ìš°ìœ„ |
| **ë„ë©”ì¸ ë¶„ë¦¬** | - | âœ… **100%** (Phase A) | âœ… ìš°ìœ„ |
| **Confidence** | âœ… | âœ… (Phase B) | âœ… ë™ë“± |
| **Multi-document** | âœ… | âœ… (Phase C) | âœ… ë™ë“± ì´ìƒ |
| **ì‘ë‹µ ì‹œê°„** | ë³´í†µ | **ìµœì í™”** (ë™ì  k) | âœ… ìš°ìœ„ |

---

## êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase A: ì¦‰ì‹œ ì ìš© (1-2ì£¼)

- [ ] A-1: Standard ëª¨ë“œ ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ì¶”ê°€ (30ë¶„)
  - [ ] `_get_context_standard()` ë©”ì„œë“œ ìˆ˜ì •
  - [ ] í…ŒìŠ¤íŠ¸: OLED ì§ˆë¬¸ â†’ HR ë¬¸ì„œ ì œê±° í™•ì¸
  - [ ] í¬ë¡œìŠ¤ ë„ë©”ì¸ ì˜¤ì—¼ 0% ë‹¬ì„± í™•ì¸

- [ ] A-2: Source Citation ê°•í™” (3ì¼)
  - [ ] `_generate_source_citations()` ë©”ì„œë“œ êµ¬í˜„
  - [ ] `_find_best_source_for_sentence()` êµ¬í˜„
  - [ ] `_format_citation()` êµ¬í˜„
  - [ ] í…ŒìŠ¤íŠ¸: ì¶œì²˜ ì •í™•ë„ 95% ë‹¬ì„±

- [ ] A-3: Answer Verification ê°œì„  (2ì¼)
  - [ ] Prompt template ê°œì„ 
  - [ ] `_generate_with_self_consistency()` êµ¬í˜„
  - [ ] `_calculate_answer_consistency()` êµ¬í˜„
  - [ ] í…ŒìŠ¤íŠ¸: ì¬ìƒì„± ë¹ˆë„ 50% ê°ì†Œ í™•ì¸

### Phase B: ë‹¨ê¸° ê°œì„  (1-2ê°œì›”)

- [ ] B-1: Query Rewriting ê³ ë„í™” (1ì£¼)
  - [ ] `AdvancedQueryRewriter` í´ë˜ìŠ¤ êµ¬í˜„
  - [ ] Context-aware rewriting
  - [ ] Terminology expansion
  - [ ] HyDE êµ¬í˜„
  - [ ] Question decomposition
  - [ ] í…ŒìŠ¤íŠ¸: ê²€ìƒ‰ ì •í™•ë„ +25-35%

- [ ] B-2: Confidence Score ì¶”ê°€ (1ì£¼)
  - [ ] `generate_with_confidence()` êµ¬í˜„
  - [ ] `_calculate_confidence_factors()` êµ¬í˜„
  - [ ] `_format_answer_with_confidence()` êµ¬í˜„
  - [ ] UIì— ì‹ ë¢°ë„ í‘œì‹œ ì¶”ê°€
  - [ ] í…ŒìŠ¤íŠ¸: ì‚¬ìš©ì ì‹ ë¢°ë„ ì¡°ì‚¬

- [ ] B-3: Context Window ìµœì í™” (3ì¼)
  - [ ] `_get_optimal_k()` êµ¬í˜„
  - [ ] `_analyze_question_complexity()` êµ¬í˜„
  - [ ] ê²€ìƒ‰ ë©”ì„œë“œì— ë™ì  k ì ìš©
  - [ ] í…ŒìŠ¤íŠ¸: ì‘ë‹µ ì‹œê°„ ì¸¡ì •

### Phase C: ì¤‘ê¸° ê°œì„  (2-3ê°œì›”)

- [ ] C-1: Multi-hop Reasoning (2ì£¼)
  - [ ] `MultiHopRetriever` í´ë˜ìŠ¤ êµ¬í˜„
  - [ ] ì§ˆë¬¸ ë¶„í•´ ë¡œì§
  - [ ] ì¤‘ê°„ ê²°ê³¼ í†µí•©
  - [ ] Follow-up ì§ˆë¬¸ ìƒì„±
  - [ ] í…ŒìŠ¤íŠ¸: ë³µì¡í•œ ì§ˆë¬¸ ì •í™•ë„

- [ ] C-2: Phase 5 - ìŠ¬ë¼ì´ë“œ ê´€ê³„ ê·¸ë˜í”„ (2ì£¼)
  - [ ] `SlideRelationshipGraph` í´ë˜ìŠ¤ êµ¬í˜„
  - [ ] ê·¸ë˜í”„ êµ¬ì¶• ë¡œì§
  - [ ] ê·¸ë˜í”„ ê¸°ë°˜ ê²€ìƒ‰
  - [ ] í…ŒìŠ¤íŠ¸: "ì•ì—ì„œ ì–¸ê¸‰í•œ..." ì§ˆì˜

- [ ] C-3: Smart Vision ì ìš© (1ì£¼)
  - [ ] `SmartVisionChunker` êµ¬í˜„
  - [ ] ì„ íƒì  Vision ë¡œì§
  - [ ] ë¹„ìš© ì¸¡ì • ë° ìµœì í™”
  - [ ] í…ŒìŠ¤íŠ¸: ì •í™•ë„ vs ë¹„ìš©

---

## í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### Baseline í…ŒìŠ¤íŠ¸ ì„¤ê³„

**ë¬¸ì œ**: í˜„ì¬ 11ê°œ í…ŒìŠ¤íŠ¸ëŠ” ë„ˆë¬´ ì‘ê³  ì‰¬ì›€
**í•´ê²°**: í™•ì¥ì„± ìˆëŠ” í…ŒìŠ¤íŠ¸ ì„¤ê³„

#### í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í™•ì¥

**í˜„ì¬**:
- ë¬¸ì„œ: 5ê°œ
- í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: 11ê°œ
- ë„ë©”ì¸: 3-4ê°œ

**í™•ì¥ ëª©í‘œ** (Phase A ì‹œì‘ ì „):
- ë¬¸ì„œ: 20-30ê°œ
- í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: 50-100ê°œ
- ë„ë©”ì¸: 5-6ê°œ
- ë‚œì´ë„: Easy (30%), Medium (50%), Hard (20%)

#### í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¶„ë¥˜

**1. Easy (30%)**:
- ë‹¨ìˆœ ì‚¬ì‹¤ ì§ˆë¬¸
- ë‹¨ì¼ ë¬¸ì„œ ë‹µë³€
- ëª…í™•í•œ í‚¤ì›Œë“œ
- ì˜ˆ: "TADFë€?", "LGë””ìŠ¤í”Œë ˆì´ ë³¸ì‚¬ëŠ”?"

**2. Medium (50%)**:
- ë³µí•© ì •ë³´ ì§ˆë¬¸
- 2-3ê°œ ë¬¸ì„œ í†µí•©
- ì•½ê°„ì˜ ì¶”ë¡  í•„ìš”
- ì˜ˆ: "TADF ì¬ë£Œì˜ ì–‘ì íš¨ìœ¨ê³¼ OLED ì„±ëŠ¥ ê´€ê³„ëŠ”?"

**3. Hard (20%)**:
- ë‹¤ë‹¨ê³„ ì¶”ë¡ 
- ì—¬ëŸ¬ ë„ë©”ì¸ í†µí•©
- ì•”ë¬µì  ê´€ê³„ íŒŒì•…
- ì˜ˆ: "TADF ê¸°ìˆ  ë°œì „ì´ LGë””ìŠ¤í”Œë ˆì´ ë¹„ì¦ˆë‹ˆìŠ¤ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê¸°ìˆ  ë…¼ë¬¸ê³¼ ë‰´ìŠ¤ë¥¼ ì¢…í•©í•´ì„œ ë¶„ì„í•´ì¤˜"

#### í‰ê°€ ì§€í‘œ

```python
metrics = {
    "accuracy": "ì •ë‹µë¥  (%)",
    "precision": "ì •í™•ë„ (ì •ë‹µ ì¤‘ ì‹¤ì œ ì •ë‹µ)",
    "recall": "ì¬í˜„ìœ¨ (ì‹¤ì œ ì •ë‹µ ì¤‘ ì°¾ì€ ì •ë‹µ)",
    "f1_score": "F1 ì ìˆ˜",
    "latency": "ì‘ë‹µ ì‹œê°„ (ì´ˆ)",
    "source_accuracy": "ì¶œì²˜ ì •í™•ë„ (%)",
    "cross_domain_contamination": "í¬ë¡œìŠ¤ ë„ë©”ì¸ ì˜¤ì—¼ (%)",
    "confidence_calibration": "ì‹ ë¢°ë„ ë³´ì • (ì˜ˆì¸¡ vs ì‹¤ì œ)",
    "hallucination_rate": "Hallucination ë¹„ìœ¨ (%)"
}
```

---

## ë‹¤ìŒ ë‹¨ê³„

### ì¦‰ì‹œ ì‹¤í–‰ (ì˜¤ëŠ˜)

1. âœ… ì´ ë¬¸ì„œ ì €ì¥
2. â³ Git ì»¤ë°‹ ë° í‘¸ì‹œ
3. â³ Phase A ìƒì„¸ ê³„íš ì‘ì„±
4. â³ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ í™•ì¥
5. â³ Baseline í…ŒìŠ¤íŠ¸ ì‹¤í–‰

### ì´ë²ˆ ì£¼

1. Phase A-1 êµ¬í˜„ (30ë¶„)
2. Phase A-2 ì„¤ê³„ ë° í”„ë¡œí† íƒ€ì…
3. í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„

### ì´ë²ˆ ë‹¬

1. Phase A ì™„ë£Œ
2. Phase A ì„±ëŠ¥ ì¸¡ì •
3. Phase B ì‹œì‘

---

**ì‘ì„±ì¼**: 2025-11-06
**ë²„ì „**: v3.1 â†’ v3.2+ (ë¡œë“œë§µ)
**ëª©í‘œ**: NotebookLM ìˆ˜ì¤€ ì´ˆê³¼ ë‹¬ì„±
