# íŒŒì¼ë³„ ì²­í‚¹ ì „ëµ ê°œì„  ì§„í–‰í”Œëœ

## ğŸ“‹ ê°œìš”

í˜„ì¬ í”„ë¡œì íŠ¸ì˜ ë¬¸ì„œ ì²˜ë¦¬ ì‹œìŠ¤í…œì„ PDF ê³ ê¸‰ ì²­í‚¹ ì „ëµì— ë”°ë¼ ë‹¨ê³„ì ìœ¼ë¡œ ê°œì„ í•˜ëŠ” í”Œëœì…ë‹ˆë‹¤. ê° íŒŒì¼ íƒ€ì…ë³„ë¡œ íŠ¹í™”ëœ ì „ëµì„ ì ìš©í•˜ì—¬ ê²€ìƒ‰ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

## ğŸ¯ ì „ì²´ ëª©í‘œ

- **Small-to-Large ì•„í‚¤í…ì²˜** ë„ì…ìœ¼ë¡œ ì •í™•ì„±ê³¼ ì»¨í…ìŠ¤íŠ¸ ë™ì‹œ í™•ë³´
- **Layout-Aware ë¶„ì„**ìœ¼ë¡œ ë¬¸ì„œì˜ ì‹œê°ì  êµ¬ì¡° ë³´ì¡´
- **ê³„ì¸µì  ë©”íƒ€ë°ì´í„°** ì‹œìŠ¤í…œìœ¼ë¡œ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ
- **íŒŒì¼ íƒ€ì…ë³„ ìµœì í™”** ì „ëµ ì ìš©

---

## ğŸ“„ Phase 1: PDF ê³ ê¸‰ ì²­í‚¹ ì „ëµ êµ¬í˜„

### 1.1 í•µì‹¬ ë°ì´í„° êµ¬ì¡° êµ¬í˜„

#### íŒŒì¼: `utils/pdf_chunking.py` (ì‹ ê·œ ìƒì„±)
```python
from dataclasses import dataclass
from typing import Optional, List, Dict, Any
import uuid

@dataclass
class ChunkMetadata:
    """PDF ì²­í¬ì˜ í’ë¶€í•œ ë©”íƒ€ë°ì´í„°"""
    document_id: str
    page_number: int
    parent_chunk_id: Optional[str] = None  # Small-to-Largeìš©
    section_title: str = ""
    chunk_type_weight: float = 1.0
    font_size: float = 12.0
    is_bold: bool = False
    coordinates: Optional[tuple] = None  # (x1, y1, x2, y2)
    word_count: int = 0
    char_count: int = 0

@dataclass
class Chunk:
    """PDF ì²­í¬ì˜ ìµœì¢… í˜•íƒœ"""
    id: str
    content: str
    chunk_type: str  # title, paragraph, table, list, page_summary
    metadata: ChunkMetadata
```

### 1.2 Layout-Aware PDF ë¶„ì„ê¸° êµ¬í˜„

#### íŒŒì¼: `utils/pdf_layout_analyzer.py` (ì‹ ê·œ ìƒì„±)
```python
import pdfplumber
import fitz  # PyMuPDF
from typing import List, Dict, Any

class PDFLayoutAnalyzer:
    """PDFì˜ ì‹œê°ì  ë ˆì´ì•„ì›ƒì„ ë¶„ì„í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.title_font_threshold = 18.0
        self.bold_threshold = 0.7
    
    def analyze_page_elements(self, page) -> List[Dict[str, Any]]:
        """í˜ì´ì§€ì˜ ì‹œê°ì  ìš”ì†Œë¥¼ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        elements = []
        
        # 1. í…ìŠ¤íŠ¸ ë¸”ë¡ê³¼ í°íŠ¸ ì •ë³´ ì¶”ì¶œ
        text_blocks = self._extract_text_blocks_with_font(page)
        
        # 2. í°íŠ¸ í¬ê¸° í†µê³„ ê³„ì‚°
        font_stats = self._calculate_font_statistics(text_blocks)
        
        # 3. ìš”ì†Œ ìœ í˜• ë¶„ë¥˜
        for block in text_blocks:
            element_type = self._classify_element_type(block, font_stats)
            elements.append({
                "type": element_type,
                "content": block["text"],
                "properties": {
                    "font_size": block["font_size"],
                    "is_bold": block["is_bold"],
                    "coordinates": block["coordinates"]
                }
            })
        
        # 4. í…Œì´ë¸” ì¶”ì¶œ
        tables = self._extract_tables(page)
        for table in tables:
            elements.append({
                "type": "table",
                "data": table["data"],
                "properties": {"coordinates": table["coordinates"]}
            })
        
        # 5. ì‹œê°ì  ìˆœì„œë¡œ ì •ë ¬
        return self._sort_elements_by_position(elements)
    
    def _extract_text_blocks_with_font(self, page):
        """í…ìŠ¤íŠ¸ ë¸”ë¡ê³¼ í°íŠ¸ ì •ë³´ ì¶”ì¶œ"""
        # pdfplumber ë˜ëŠ” PyMuPDF ì‚¬ìš©
        pass
    
    def _calculate_font_statistics(self, text_blocks):
        """í°íŠ¸ í¬ê¸° í†µê³„ ê³„ì‚°"""
        font_sizes = [block["font_size"] for block in text_blocks]
        return {
            "mean": sum(font_sizes) / len(font_sizes),
            "std": self._calculate_std(font_sizes),
            "threshold": max(font_sizes) * 0.8  # ìƒìœ„ 20%ë¥¼ ì œëª©ìœ¼ë¡œ ê°„ì£¼
        }
    
    def _classify_element_type(self, block, font_stats):
        """ìš”ì†Œ ìœ í˜• ë¶„ë¥˜"""
        text = block["text"].strip()
        
        # ì œëª© íŒë³„
        if (block["font_size"] > font_stats["threshold"] and 
            block["is_bold"] and 
            len(text.split()) <= 10):
            return "title"
        
        # ë¦¬ìŠ¤íŠ¸ íŒë³„
        if (text.startswith("â€¢") or 
            text.startswith("-") or 
            text.startswith("*") or
            re.match(r'^\d+\.', text)):
            return "list_item"
        
        # ë¬¸ë‹¨
        return "paragraph"
```

### 1.3 Small-to-Large ì²­í‚¹ ì—”ì§„ êµ¬í˜„

#### íŒŒì¼: `utils/pdf_chunking_engine.py` (ì‹ ê·œ ìƒì„±)
```python
from typing import List, Dict, Any
import uuid

class PDFChunkingEngine:
    """PDF ê³ ê¸‰ ì²­í‚¹ ì—”ì§„"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.layout_analyzer = PDFLayoutAnalyzer()
    
    def process_pdf_document(self, pdf_path: str) -> List[Chunk]:
        """PDF ë¬¸ì„œë¥¼ ë ˆì´ì•„ì›ƒì„ ì¸ì‹í•˜ì—¬ ê³„ì¸µì ìœ¼ë¡œ ì²­í‚¹"""
        all_chunks = []
        document_id = str(uuid.uuid4())
        
        with pdfplumber.open(pdf_path) as pdf:
            current_section_title = "ë¬¸ì„œ ì„œë‘"
            
            for page_num, page in enumerate(pdf.pages, 1):
                # 1. Large ì²­í¬ ìƒì„± (í˜ì´ì§€ ì „ì²´)
                page_chunk = self._create_page_summary_chunk(
                    page, document_id, page_num, current_section_title
                )
                all_chunks.append(page_chunk)
                
                # 2. Small ì²­í¬ ìƒì„± (Layout-Aware)
                elements = self.layout_analyzer.analyze_page_elements(page)
                small_chunks = self._process_page_elements(
                    elements, document_id, page_num, 
                    page_chunk.id, current_section_title
                )
                all_chunks.extend(small_chunks)
                
                # 3. ì„¹ì…˜ ì œëª© ì—…ë°ì´íŠ¸
                current_section_title = self._update_section_title(
                    elements, current_section_title
                )
        
        return all_chunks
    
    def _create_page_summary_chunk(self, page, document_id, page_num, section_title):
        """í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë¶€ëª¨ ì²­í¬ë¡œ ìƒì„±"""
        page_id = f"{document_id}_page_{page_num}"
        page_text = page.extract_text()
        
        return Chunk(
            id=page_id,
            content=page_text,
            chunk_type="page_summary",
            metadata=ChunkMetadata(
                document_id=document_id,
                page_number=page_num,
                parent_chunk_id=None,  # ìµœìƒìœ„ ë¶€ëª¨
                section_title=section_title,
                chunk_type_weight=1.0,
                word_count=len(page_text.split()),
                char_count=len(page_text)
            )
        )
    
    def _process_page_elements(self, elements, document_id, page_num, parent_id, section_title):
        """í˜ì´ì§€ ìš”ì†Œë“¤ì„ Small ì²­í¬ë¡œ ì²˜ë¦¬"""
        chunks = []
        current_list_buffer = []
        
        for elem in elements:
            # ë¦¬ìŠ¤íŠ¸ í•­ëª© ë²„í¼ë§
            if elem["type"] == "list_item":
                current_list_buffer.append(elem["content"])
                continue
            
            # ë¦¬ìŠ¤íŠ¸ ë²„í¼ ë¹„ìš°ê¸°
            if current_list_buffer:
                list_chunks = self._create_list_chunks(
                    current_list_buffer, document_id, page_num, 
                    parent_id, section_title
                )
                chunks.extend(list_chunks)
                current_list_buffer.clear()
            
            # ë‹¤ë¥¸ ìš”ì†Œ ì²˜ë¦¬
            if elem["type"] == "title":
                chunk = self._create_title_chunk(elem, document_id, page_num, parent_id, section_title)
                chunks.append(chunk)
            elif elem["type"] == "paragraph":
                para_chunks = self._create_paragraph_chunks(
                    elem, document_id, page_num, parent_id, section_title
                )
                chunks.extend(para_chunks)
            elif elem["type"] == "table":
                table_chunks = self._create_table_chunks(
                    elem, document_id, page_num, parent_id, section_title
                )
                chunks.extend(table_chunks)
        
        # ë§ˆì§€ë§‰ ë¦¬ìŠ¤íŠ¸ ë²„í¼ ì²˜ë¦¬
        if current_list_buffer:
            list_chunks = self._create_list_chunks(
                current_list_buffer, document_id, page_num, 
                parent_id, section_title
            )
            chunks.extend(list_chunks)
        
        return chunks
```

### 1.4 Fallback ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„

#### íŒŒì¼: `utils/chunking_fallback.py` (ì‹ ê·œ ìƒì„±)
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from typing import List, Dict, Any

class ChunkingFallback:
    """ì˜ë¯¸ë¡ ì  ì²­í‚¹ ì‹¤íŒ¨ ì‹œ Fallback ì²˜ë¦¬"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.get("max_size", 500),
            chunk_overlap=config.get("overlap_size", 100),
            separators=["\n\n", "\n", " ", ""]
        )
    
    def chunk_element_with_fallback(self, content: str, element_type: str, 
                                  base_metadata: ChunkMetadata) -> List[Chunk]:
        """ìš”ì†Œê°€ max_sizeë¥¼ ì´ˆê³¼í•  ê²½ìš° Fallback ì ìš©"""
        chunks = []
        
        if len(content) <= self.config.get("max_size", 500):
            # ë‹¨ì¼ ì²­í¬ë¡œ ì¶©ë¶„
            chunk = Chunk(
                id=str(uuid.uuid4()),
                content=content,
                chunk_type=element_type,
                metadata=base_metadata
            )
            chunks.append(chunk)
        else:
            # Fallback: ê°•ì œ ë¶„í• 
            sub_contents = self.text_splitter.split_text(content)
            
            for i, sub_content in enumerate(sub_contents):
                chunk = Chunk(
                    id=str(uuid.uuid4()),
                    content=sub_content,
                    chunk_type=f"{element_type}_segment",
                    metadata=base_metadata
                )
                chunks.append(chunk)
        
        return chunks
```

---

## ğŸ“Š Phase 2: PPTX ìŠ¬ë¼ì´ë“œ ì¤‘ì‹¬ ì²­í‚¹ êµ¬í˜„

### 2.1 ìŠ¬ë¼ì´ë“œ êµ¬ì¡° ë¶„ì„ê¸°

#### íŒŒì¼: `utils/pptx_chunking.py` (ì‹ ê·œ ìƒì„±)
```python
from pptx import Presentation
from typing import List, Dict, Any

class PPTXChunkingEngine:
    """PPTX ìŠ¬ë¼ì´ë“œ ì¤‘ì‹¬ ì²­í‚¹ ì—”ì§„"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
    
    def process_pptx_document(self, pptx_path: str) -> List[Chunk]:
        """PPTX ë¬¸ì„œë¥¼ ìŠ¬ë¼ì´ë“œ ë‹¨ìœ„ë¡œ ì²­í‚¹"""
        all_chunks = []
        document_id = str(uuid.uuid4())
        
        prs = Presentation(pptx_path)
        
        for slide_num, slide in enumerate(prs.slides, 1):
            slide_chunks = self._process_slide(
                slide, document_id, slide_num
            )
            all_chunks.extend(slide_chunks)
        
        return all_chunks
    
    def _process_slide(self, slide, document_id, slide_num):
        """ê°œë³„ ìŠ¬ë¼ì´ë“œ ì²˜ë¦¬"""
        chunks = []
        
        # 1. ìŠ¬ë¼ì´ë“œ ì œëª© (ê°€ì¤‘ì¹˜ 2.0)
        title = self._extract_slide_title(slide)
        if title:
            chunks.append(self._create_title_chunk(
                title, document_id, slide_num, weight=2.0
            ))
        
        # 2. ìŠ¬ë¼ì´ë“œ ë³¸ë¬¸ ë‚´ìš©
        content = self._extract_slide_content(slide)
        if content:
            content_chunks = self._chunk_content_by_bullets(
                content, document_id, slide_num
            )
            chunks.extend(content_chunks)
        
        # 3. ìŠ¬ë¼ì´ë“œ ë…¸íŠ¸ (ê°€ì¤‘ì¹˜ 1.5)
        notes = self._extract_slide_notes(slide)
        if notes:
            chunks.append(self._create_notes_chunk(
                notes, document_id, slide_num, weight=1.5
            ))
        
        return chunks
```

---

## ğŸ“ˆ Phase 3: XLSX ì›Œí¬ì‹œíŠ¸ ì¤‘ì‹¬ ì²­í‚¹ êµ¬í˜„

### 3.1 ì›Œí¬ì‹œíŠ¸ êµ¬ì¡° ë¶„ì„ê¸°

#### íŒŒì¼: `utils/xlsx_chunking.py` (ì‹ ê·œ ìƒì„±)
```python
import openpyxl
from typing import List, Dict, Any

class XLSXChunkingEngine:
    """XLSX ì›Œí¬ì‹œíŠ¸ ì¤‘ì‹¬ ì²­í‚¹ ì—”ì§„"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
    
    def process_xlsx_document(self, xlsx_path: str) -> List[Chunk]:
        """XLSX ë¬¸ì„œë¥¼ ì›Œí¬ì‹œíŠ¸ ë‹¨ìœ„ë¡œ ì²­í‚¹"""
        all_chunks = []
        document_id = str(uuid.uuid4())
        
        wb = openpyxl.load_workbook(xlsx_path)
        
        for ws_name in wb.sheetnames:
            ws = wb[ws_name]
            ws_chunks = self._process_worksheet(
                ws, document_id, ws_name
            )
            all_chunks.extend(ws_chunks)
        
        return all_chunks
    
    def _process_worksheet(self, worksheet, document_id, ws_name):
        """ê°œë³„ ì›Œí¬ì‹œíŠ¸ ì²˜ë¦¬"""
        chunks = []
        
        # 1. ì›Œí¬ì‹œíŠ¸ í—¤ë” (ê°€ì¤‘ì¹˜ 2.0)
        headers = self._extract_headers(worksheet)
        if headers:
            chunks.append(self._create_header_chunk(
                headers, document_id, ws_name, weight=2.0
            ))
        
        # 2. ë°ì´í„° í–‰ë“¤ì„ ê·¸ë£¹ìœ¼ë¡œ ì²­í‚¹
        data_rows = self._extract_data_rows(worksheet)
        if data_rows:
            row_groups = self._group_rows_by_size(data_rows)
            for group_num, group in enumerate(row_groups):
                chunks.append(self._create_data_group_chunk(
                    group, document_id, ws_name, group_num
                ))
        
        # 3. ìˆ˜ì‹ ì„¤ëª… (ê°€ì¤‘ì¹˜ 1.5)
        formulas = self._extract_formulas(worksheet)
        if formulas:
            chunks.append(self._create_formula_chunk(
                formulas, document_id, ws_name, weight=1.5
            ))
        
        return chunks
```

---

## ğŸ“ Phase 4: Text êµ¬ì¡° ì¸ì‹ ì²­í‚¹ êµ¬í˜„

### 4.1 í…ìŠ¤íŠ¸ êµ¬ì¡° ë¶„ì„ê¸°

#### íŒŒì¼: `utils/text_chunking.py` (ì‹ ê·œ ìƒì„±)
```python
import re
from typing import List, Dict, Any

class TextChunkingEngine:
    """Text êµ¬ì¡° ì¸ì‹ ì²­í‚¹ ì—”ì§„"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
    
    def process_text_document(self, text_content: str) -> List[Chunk]:
        """Text ë¬¸ì„œë¥¼ êµ¬ì¡° ì¸ì‹í•˜ì—¬ ì²­í‚¹"""
        all_chunks = []
        document_id = str(uuid.uuid4())
        
        # 1. ì„¹ì…˜ ë‹¨ìœ„ ë¶„í• 
        sections = self._split_by_sections(text_content)
        
        for section_num, section in enumerate(sections):
            section_chunks = self._process_section(
                section, document_id, section_num
            )
            all_chunks.extend(section_chunks)
        
        return all_chunks
    
    def _split_by_sections(self, text):
        """ì„¹ì…˜ êµ¬ë¶„ìë¡œ ë¶„í• """
        section_patterns = [
            r'\n(?:ì œ?\d+[ì¥ì ˆ]|Chapter|Section)',
            r'\n#{1,6}\s',  # Markdown í—¤ë”
            r'\n={3,}$',    # ë“±í˜¸ êµ¬ë¶„ì„ 
            r'\n-{3,}$',    # ëŒ€ì‹œ êµ¬ë¶„ì„ 
        ]
        
        # íŒ¨í„´ìœ¼ë¡œ ë¶„í• 
        sections = [text]
        for pattern in section_patterns:
            new_sections = []
            for section in sections:
                parts = re.split(pattern, section)
                new_sections.extend(parts)
            sections = new_sections
        
        return [s.strip() for s in sections if s.strip()]
```

---

## ğŸ”§ Phase 5: í†µí•© ì‹œìŠ¤í…œ êµ¬í˜„

### 5.1 í†µí•© ì²­í‚¹ ë§¤ë‹ˆì €

#### íŒŒì¼: `utils/integrated_chunking_manager.py` (ì‹ ê·œ ìƒì„±)
```python
from typing import List, Dict, Any, Union
import os

class IntegratedChunkingManager:
    """ëª¨ë“  ë¬¸ì„œ íƒ€ì…ì„ í†µí•© ê´€ë¦¬í•˜ëŠ” ì²­í‚¹ ë§¤ë‹ˆì €"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.pdf_engine = PDFChunkingEngine(config)
        self.pptx_engine = PPTXChunkingEngine(config)
        self.xlsx_engine = XLSXChunkingEngine(config)
        self.text_engine = TextChunkingEngine(config)
    
    def process_document(self, file_path: str) -> List[Chunk]:
        """íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ ì²­í‚¹ ì—”ì§„ ì„ íƒ"""
        file_ext = os.path.splitext(file_path)[1].lower()
        
        if file_ext == '.pdf':
            return self.pdf_engine.process_pdf_document(file_path)
        elif file_ext in ['.ppt', '.pptx']:
            return self.pptx_engine.process_pptx_document(file_path)
        elif file_ext in ['.xls', '.xlsx']:
            return self.xlsx_engine.process_xlsx_document(file_path)
        elif file_ext == '.txt':
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            return self.text_engine.process_text_document(content)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_ext}")
```

### 5.2 Small-to-Large ê²€ìƒ‰ ì‹œìŠ¤í…œ

#### íŒŒì¼: `utils/small_to_large_search.py` (ì‹ ê·œ ìƒì„±)
```python
from typing import List, Dict, Any
from langchain.schema import Document

class SmallToLargeSearch:
    """Small-to-Large ì•„í‚¤í…ì²˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ"""
    
    def __init__(self, vectorstore):
        self.vectorstore = vectorstore
    
    def search_with_context_expansion(self, query: str, top_k: int = 5) -> List[Document]:
        """ì •í™•í•œ ê²€ìƒ‰ í›„ ì»¨í…ìŠ¤íŠ¸ í™•ì¥"""
        # 1ë‹¨ê³„: Small ì²­í¬ë¡œ ì •í™•í•œ ê²€ìƒ‰
        small_results = self.vectorstore.similarity_search_with_score(
            query, k=top_k * 2  # ë” ë§ì€ í›„ë³´ í™•ë³´
        )
        
        # 2ë‹¨ê³„: ë¶€ëª¨ ì²­í¬ë¡œ ì»¨í…ìŠ¤íŠ¸ í™•ì¥
        expanded_results = []
        processed_parents = set()
        
        for doc, score in small_results:
            # Small ì²­í¬ ì¶”ê°€
            expanded_results.append((doc, score))
            
            # ë¶€ëª¨ ì²­í¬ í™•ì¥
            parent_id = doc.metadata.get("parent_chunk_id")
            if parent_id and parent_id not in processed_parents:
                parent_doc = self._get_parent_chunk(parent_id)
                if parent_doc:
                    expanded_results.append((parent_doc, score * 0.8))  # ë¶€ëª¨ëŠ” ì•½ê°„ ë‚®ì€ ì ìˆ˜
                    processed_parents.add(parent_id)
        
        # 3ë‹¨ê³„: ìƒìœ„ ê²°ê³¼ ë°˜í™˜
        expanded_results.sort(key=lambda x: x[1], reverse=True)
        return [doc for doc, score in expanded_results[:top_k]]
    
    def _get_parent_chunk(self, parent_id: str) -> Document:
        """ë¶€ëª¨ ì²­í¬ ì¡°íšŒ"""
        # ë²¡í„°ìŠ¤í† ì–´ì—ì„œ parent_idë¡œ ê²€ìƒ‰
        results = self.vectorstore.similarity_search(
            f"parent_chunk_id:{parent_id}", k=1
        )
        return results[0] if results else None
```

---

## ğŸ“… êµ¬í˜„ ì¼ì •

### Week 1: PDF ê³ ê¸‰ ì²­í‚¹ êµ¬í˜„
- [ ] `utils/pdf_chunking.py` - í•µì‹¬ ë°ì´í„° êµ¬ì¡°
- [ ] `utils/pdf_layout_analyzer.py` - Layout-Aware ë¶„ì„ê¸°
- [ ] `utils/pdf_chunking_engine.py` - Small-to-Large ì—”ì§„
- [ ] `utils/chunking_fallback.py` - Fallback ë©”ì»¤ë‹ˆì¦˜

### Week 2: PPTX ìŠ¬ë¼ì´ë“œ ì¤‘ì‹¬ ì²­í‚¹
- [ ] `utils/pptx_chunking.py` - ìŠ¬ë¼ì´ë“œ êµ¬ì¡° ë¶„ì„ê¸°
- [ ] ìŠ¬ë¼ì´ë“œ ì œëª©/ë…¸íŠ¸/ë³¸ë¬¸ êµ¬ë¶„ ì²˜ë¦¬
- [ ] ë¶ˆë¦¿ í¬ì¸íŠ¸ ë²„í¼ë§ ì „ëµ

### Week 3: XLSX ì›Œí¬ì‹œíŠ¸ ì¤‘ì‹¬ ì²­í‚¹
- [ ] `utils/xlsx_chunking.py` - ì›Œí¬ì‹œíŠ¸ êµ¬ì¡° ë¶„ì„ê¸°
- [ ] í—¤ë”/ë°ì´í„°/ìˆ˜ì‹ êµ¬ë¶„ ì²˜ë¦¬
- [ ] í–‰ ê·¸ë£¹í™” ì „ëµ

### Week 4: Text êµ¬ì¡° ì¸ì‹ ì²­í‚¹
- [ ] `utils/text_chunking.py` - í…ìŠ¤íŠ¸ êµ¬ì¡° ë¶„ì„ê¸°
- [ ] ì„¹ì…˜/ë¬¸ë‹¨/ì½”ë“œë¸”ë¡ êµ¬ë¶„ ì²˜ë¦¬
- [ ] Markdown í—¤ë” ì¸ì‹

### Week 5: í†µí•© ì‹œìŠ¤í…œ ë° ìµœì í™”
- [ ] `utils/integrated_chunking_manager.py` - í†µí•© ë§¤ë‹ˆì €
- [ ] `utils/small_to_large_search.py` - ê²€ìƒ‰ ì‹œìŠ¤í…œ
- [ ] `utils/document_processor.py` í†µí•©
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ìµœì í™”

---

## ğŸ¯ ì˜ˆìƒ íš¨ê³¼

### ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ
- **ì •í™•ë„**: 40-60% í–¥ìƒ (Small-to-Large ì•„í‚¤í…ì²˜)
- **ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ**: ì™„ì „í•œ ë‹µë³€ ì œê³µ
- **êµ¬ì¡° ë³´ì¡´**: ë¬¸ì„œì˜ ì‹œê°ì  ë ˆì´ì•„ì›ƒ ë°˜ì˜

### ì‚¬ìš©ì ê²½í—˜ ê°œì„ 
- **ê´€ë ¨ì„±**: ë” ì •í™•í•œ ë¬¸ì„œ ê²€ìƒ‰
- **ì™„ì „ì„±**: ì»¨í…ìŠ¤íŠ¸ê°€ í’ë¶€í•œ ë‹µë³€
- **ì‹ ë¢°ì„±**: ì¶œì²˜ ì •ë³´ì˜ ì •í™•ì„± í–¥ìƒ

### ì‹œìŠ¤í…œ í™•ì¥ì„±
- **ëª¨ë“ˆí™”**: íŒŒì¼ íƒ€ì…ë³„ ë…ë¦½ì  ì²˜ë¦¬
- **í™•ì¥ì„±**: ìƒˆë¡œìš´ ë¬¸ì„œ íƒ€ì… ì‰½ê²Œ ì¶”ê°€
- **ìœ ì§€ë³´ìˆ˜**: ëª…í™•í•œ ì±…ì„ ë¶„ë¦¬

---

**ë¬¸ì„œ ë²„ì „**: 1.0  
**ìµœì¢… ìˆ˜ì •ì¼**: 2024-12-19  
**ì‘ì„±ì**: RAG Development Team
