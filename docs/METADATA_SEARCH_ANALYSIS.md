# ğŸ“Š ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰ ì•„í‚¤í…ì²˜ ë¶„ì„

**ì‘ì„±ì¼**: 2025-01-09
**ëª©ì **: ìƒìš© RAG ì„œë¹„ìŠ¤ì˜ ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ ì „ëµ ë¶„ì„ ë° í˜„ì¬ í”„ë¡œì íŠ¸ ì ìš© ë°©ì•ˆ

---

## ğŸ¯ ë¬¸ì œ ì •ì˜

### í˜„ì¬ ì‹œìŠ¤í…œì˜ í•œê³„

í˜„ì¬ ì‹œìŠ¤í…œì€ **ë²¡í„° ì„ë² ë”© ê¸°ë°˜ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰(Semantic Search)**ì— ìµœì í™”ë˜ì–´ ìˆì–´, ë‹¤ìŒê³¼ ê°™ì€ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì¿¼ë¦¬ì— ì·¨ì•½í•©ë‹ˆë‹¤:

**ë¬¸ì œ ì‚¬ë¡€**:
```
ì‚¬ìš©ì: "ê¹€ì² ìˆ˜ ì €ìê°€ ì“´ ë…¼ë¬¸ì„ ì°¾ì•„ì„œ ìš”ì•½í•´ì¤˜"

[í˜„ì¬ ì‹œìŠ¤í…œì˜ ë™ì‘]
1. "ê¹€ì² ìˆ˜ ì €ìê°€ ì“´ ë…¼ë¬¸" â†’ ë²¡í„° ì„ë² ë”© ìƒì„±
2. ìœ ì‚¬ë„ ê²€ìƒ‰ â†’ ë³¸ë¬¸ì— "ê¹€ì² ìˆ˜"ê°€ ë§ì´ ë“±ì¥í•˜ëŠ” ì²­í¬ ê²€ìƒ‰
3. ë¬¸ì œ: ì €ìëª…ì€ ì£¼ë¡œ ì²« í˜ì´ì§€ì—ë§Œ ë‚˜ì˜¤ë¯€ë¡œ ê²€ìƒ‰ ëˆ„ë½ ë°œìƒ
4. ë¬¸ì œ: ë…¼ë¬¸ì´ 20ê°œ ì²­í¬ë¡œ ë‚˜ë‰˜ì–´ ìˆì„ ë•Œ, 3ê°œ ì²­í¬ë§Œ ë°˜í™˜ â†’ ë¶ˆì™„ì „í•œ ìš”ì•½
```

### ë©”íƒ€ë°ì´í„° ì¿¼ë¦¬ì˜ íŠ¹ì„±

ì‚¬ìš©ìê°€ ì•Œê³  ìˆëŠ” **êµ¬ì¡°í™”ëœ ì •ë³´**ë¡œ ê²€ìƒ‰í•˜ëŠ” ê²½ìš°:
- **ì €ìëª…**: "ê¹€ì² ìˆ˜", "ì´ì˜í¬"
- **íŒŒì¼ëª…**: "OLED_efficiency_2024.pdf"
- **ì œëª©**: "ê³ íš¨ìœ¨ OLED ì†Œì ê°œë°œ"
- **ì €ë„ëª…**: "Nature Photonics", "Applied Physics Letters"
- **ì†Œì† ê¸°ê´€**: "LG Display", "ì„œìš¸ëŒ€í•™êµ"
- **ì—°ë„**: 2023, 2024

**ê¸°ì¡´ ë²¡í„° ê²€ìƒ‰ì˜ í•œê³„**:
- âŒ ì •í™•í•œ ë¬¸ìì—´ ë§¤ì¹­(Exact Match) ë¶ˆê°€
- âŒ ë©”íƒ€ë°ì´í„°ëŠ” ë³¸ë¬¸ì— ë“±ì¥ ë¹ˆë„ê°€ ë‚®ì•„ ìœ ì‚¬ë„ ë‚®ìŒ
- âŒ ë…¼ë¬¸ ì „ì²´ë¥¼ ëŒ€í‘œí•˜ëŠ” ì •ë³´ì´ì§€ë§Œ, ì²­í¬ ë‹¨ìœ„ ê²€ìƒ‰ì—ì„œ ì†ì‹¤

---

## ğŸ¢ ìƒìš© ì„œë¹„ìŠ¤ ë²¤ì¹˜ë§ˆí¬

### 1. Semantic Scholar (í•™ìˆ  ê²€ìƒ‰ ì—”ì§„)

**ì•„í‚¤í…ì²˜**: 3-Index ë³‘ë ¬ ê²€ìƒ‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Query Router (LLM)                  â”‚
â”‚  "ê¹€ì² ìˆ˜ì˜ OLED ì—°êµ¬" â†’ [Metadata] + [Vector]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â–¼                 â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Metadata â”‚     â”‚Full-textâ”‚   â”‚ Vector  â”‚
â”‚ Index   â”‚     â”‚ Index   â”‚   â”‚ Index   â”‚
â”‚(Elastic)â”‚     â”‚ (BM25)  â”‚   â”‚(SPECTER)â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚               â”‚             â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
              [Fusion Ranking]
                     â”‚
              [Final Results]
```

**í•µì‹¬ ì „ëµ**:
1. **Elasticsearch ë©”íƒ€ë°ì´í„° ì¸ë±ìŠ¤**: ì €ì, ì œëª©, ê¸°ê´€, ì—°ë„ ë“± êµ¬ì¡°í™”ëœ í•„ë“œ
2. **BM25 í‚¤ì›Œë“œ ê²€ìƒ‰**: ì „ë¬¸ ê²€ìƒ‰ (ìš©ì–´ ì •í™•ë„ ì¤‘ìš”)
3. **SPECTER ë²¡í„° ê²€ìƒ‰**: ë…¼ë¬¸ ì„ë² ë”© (ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„)
4. **Reciprocal Rank Fusion (RRF)**: 3ê°œ ê²°ê³¼ ì¢…í•© ìˆœìœ„í™”

**ë©”íƒ€ë°ì´í„° í•„ë“œ**:
```json
{
  "paperId": "abc123",
  "title": "High-Efficiency OLED Devices",
  "authors": [
    {
      "authorId": "1234",
      "name": "ê¹€ì² ìˆ˜",
      "affiliation": "LG Display"
    }
  ],
  "year": 2024,
  "venue": "Nature Photonics",
  "citationCount": 42,
  "influentialCitationCount": 15,
  "references": ["paper_id_1", "paper_id_2"]
}
```

---

### 2. Perplexity AI (ëŒ€í™”í˜• ê²€ìƒ‰)

**ì•„í‚¤í…ì²˜**: Query Decomposition + Multi-step Retrieval

```
User Query: "LG Display ì†Œì† ì €ìë“¤ì˜ OLED ì—°êµ¬ë¥¼ ìš”ì•½í•´ì¤˜"
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLM Query Decomposition            â”‚
â”‚   (ì¿¼ë¦¬ë¥¼ ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹¨ê³„ë¡œ ë¶„í•´)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Step 1: Metadata í•„í„°ë§â”‚
   â”‚   WHERE affiliation    â”‚
   â”‚   = "LG Display"       â”‚
   â”‚   AND topic = "OLED"   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Step 2: Document Load â”‚
   â”‚   ì‹ë³„ëœ ë…¼ë¬¸ì˜        â”‚
   â”‚   ì „ì²´ ë‚´ìš© ë¡œë“œ       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Step 3: Synthesis     â”‚
   â”‚   ëª¨ë“  ë…¼ë¬¸ ì¢…í•© ìš”ì•½  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•µì‹¬ ì „ëµ**:
- **Query Planning**: LLMì´ ë³µì¡í•œ ì¿¼ë¦¬ë¥¼ ë‹¤ë‹¨ê³„ ì‹¤í–‰ ê³„íšìœ¼ë¡œ ë³€í™˜
- **Tool Use**: ê²€ìƒ‰, í•„í„°ë§, ìš”ì•½ ë“± ê° ë‹¨ê³„ë³„ ë„êµ¬ ì‚¬ìš©
- **Context Aggregation**: í•„í„°ë§ëœ ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨

---

### 3. Elicit (AI Research Assistant)

**ì•„í‚¤í…ì²˜**: Paper-centric Structured Extraction

```
User Query: "OLED íš¨ìœ¨ í–¥ìƒ ë°©ë²• ë¹„êµ"
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Semantic Scholar API í˜¸ì¶œ           â”‚
â”‚  ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ê²€ìƒ‰                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GROBID PDF Parsing                 â”‚
â”‚  - Abstract                         â”‚
â”‚  - Methods                          â”‚
â”‚  - Results                          â”‚
â”‚  - Discussion                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Section-wise Summarization         â”‚
â”‚  ê° ì„¹ì…˜ë³„ ìš”ì•½ ì¶”ì¶œ                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Structured Table Output            â”‚
â”‚  ì €ìâ”‚ì—°ë„â”‚ë°©ë²•ë¡ â”‚ì£¼ìš” ë°œê²¬â”‚ì¸ìš©     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•µì‹¬ ì „ëµ**:
- **ë…¼ë¬¸ êµ¬ì¡° ì¸ì‹**: GROBIDë¡œ ì„¹ì…˜ ìë™ ì¶”ì¶œ
- **êµ¬ì¡°í™”ëœ ì¶œë ¥**: í‘œ í˜•íƒœë¡œ ë¹„êµ ê°€ëŠ¥í•œ ì •ë³´ ì œê³µ
- **ë©”íƒ€ë°ì´í„° ì¤‘ì‹¬**: ì €ì, ì—°ë„, ë°©ë²•ë¡  ë“± í•„ë“œë³„ ê²€ìƒ‰

---

### 4. ChatGPT Enterprise (File Upload)

**ì•„í‚¤í…ì²˜**: Hybrid RAG with Automatic Metadata Extraction

```
File Upload (PDF)
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Automatic Metadata Extraction      â”‚
â”‚  - Title, Authors (from PDF props)  â”‚
â”‚  - Content Analysis (keywords)      â”‚
â”‚  - Entity Recognition (NER)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dual Index Creation                â”‚
â”‚  - Metadata Index                   â”‚
â”‚  - Vector Index (chunks)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Query Analysis & Strategy Selectionâ”‚
â”‚  - Metadata Query â†’ Exact Filter    â”‚
â”‚  - Content Query â†’ Vector Search    â”‚
â”‚  - Hybrid Query â†’ Both              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Generation (GPT-4)             â”‚
â”‚  ìµœëŒ€ 100ê°œ ì²­í¬ ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ê°€ëŠ¥  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í•µì‹¬ ì „ëµ**:
- **ìë™ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ**: íŒŒì¼ ì—…ë¡œë“œ ì‹œ ìë™ íŒŒì‹±
- **ì¿¼ë¦¬ íƒ€ì… ìë™ ê°ì§€**: LLMì´ ì¿¼ë¦¬ íŠ¹ì„± ë¶„ì„
- **Large Context Window**: GPT-4 Turbo 128K ì»¨í…ìŠ¤íŠ¸ í™œìš©

---

## ğŸ› ï¸ ê³µí†µ í•µì‹¬ ê¸°ìˆ 

### 1. ë©”íƒ€ë°ì´í„° ìë™ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸

**GROBID (GeneRation Of BIbliographic Data)**:
```python
def extract_metadata_with_grobid(pdf_path: str) -> dict:
    """GROBIDë¥¼ ì‚¬ìš©í•œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    # GROBID ì„œë²„ í˜¸ì¶œ (ë¡œì»¬ ë˜ëŠ” ì›ê²©)
    response = requests.post(
        "http://localhost:8070/api/processHeaderDocument",
        files={"input": open(pdf_path, "rb")}
    )

    # XML ì‘ë‹µ íŒŒì‹±
    xml_data = response.text
    soup = BeautifulSoup(xml_data, "xml")

    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    metadata = {
        "title": soup.find("title").text,
        "authors": [
            {
                "name": author.find("persName").text,
                "affiliation": author.find("affiliation").text if author.find("affiliation") else ""
            }
            for author in soup.find_all("author")
        ],
        "abstract": soup.find("abstract").text if soup.find("abstract") else "",
        "journal": soup.find("title", {"level": "j"}).text if soup.find("title", {"level": "j"}) else "",
        "year": soup.find("date").get("when") if soup.find("date") else ""
    }

    return metadata
```

**ëŒ€ì•ˆ: PyPDF2 + Regex (ì˜¤í”„ë¼ì¸)**:
```python
def extract_metadata_simple(pdf_path: str) -> dict:
    """PyPDF2 ê¸°ë³¸ ì •ë³´ + ì •ê·œì‹ ì¶”ì¶œ"""
    import PyPDF2
    import re

    with open(pdf_path, "rb") as f:
        pdf = PyPDF2.PdfReader(f)

        # PDF ë©”íƒ€ë°ì´í„° (íŒŒì¼ ì†ì„±)
        info = pdf.metadata

        # ì²« 2í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        first_pages = ""
        for i in range(min(2, len(pdf.pages))):
            first_pages += pdf.pages[i].extract_text()

        # ì •ê·œì‹ìœ¼ë¡œ ì €ì/ì†Œì† ì¶”ì¶œ (ë…¼ë¬¸ë§ˆë‹¤ í˜•ì‹ ìƒì´)
        authors = re.findall(r"([A-Z][a-z]+ [A-Z][a-z]+)", first_pages)  # ì˜ë¬¸ ì´ë¦„
        affiliations = re.findall(r"(University|Institute|Display|Laboratory)", first_pages)

        return {
            "title": info.get("/Title", ""),
            "authors": authors[:5],  # ìµœëŒ€ 5ëª…
            "affiliations": list(set(affiliations)),
            "creation_date": info.get("/CreationDate", "")
        }
```

---

### 2. ChromaDB ë©”íƒ€ë°ì´í„° í•„í„°ë§

**Chromaì˜ `where` ì ˆ í™œìš©**:
```python
# ì €ìëª…ìœ¼ë¡œ í•„í„°ë§
results = collection.get(
    where={"authors": {"$contains": "ê¹€ì² ìˆ˜"}},
    include=["metadatas", "documents", "embeddings"]
)

# ë³µí•© ì¡°ê±´ (ì €ì + ì—°ë„)
results = collection.get(
    where={
        "$and": [
            {"authors": {"$contains": "ê¹€ì² ìˆ˜"}},
            {"year": {"$gte": 2020}}
        ]
    }
)

# OR ì¡°ê±´ (ì—¬ëŸ¬ ì €ì)
results = collection.get(
    where={
        "$or": [
            {"authors": {"$contains": "ê¹€ì² ìˆ˜"}},
            {"authors": {"$contains": "ì´ì˜í¬"}}
        ]
    }
)
```

**ë©”íƒ€ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì„¤ê³„**:
```python
# í˜„ì¬ (v3.6.1)
metadata = {
    "source": "paper.pdf",
    "page": 1,
    "chunk_id": "paper_chunk_001"
}

# í™•ì¥ ë²„ì „ (Phase 2.5.6)
metadata = {
    "source": "paper.pdf",
    "page": 1,
    "chunk_id": "paper_chunk_001",

    # ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° (ë¬¸ì„œ ë ˆë²¨)
    "document_id": "doc_12345",  # ë…¼ë¬¸ ê³ ìœ  ID
    "title": "ê³ íš¨ìœ¨ OLED ì†Œì ê°œë°œ",
    "authors": ["ê¹€ì² ìˆ˜", "ì´ì˜í¬"],
    "author_affiliations": ["LG Display", "ì„œìš¸ëŒ€í•™êµ"],
    "journal": "Nature Photonics",
    "year": 2024,
    "doi": "10.1038/nphoton.2024.123",

    # ì²­í¬ ë©”íƒ€ë°ì´í„° (ì²­í¬ ë ˆë²¨)
    "section": "Results",  # Introduction, Methods, Results, Discussion
    "has_table": True,
    "has_figure": True
}
```

---

### 3. Query Router (ì¿¼ë¦¬ ë¶„ë¥˜ê¸°)

**LLM ê¸°ë°˜ ì¿¼ë¦¬ íƒ€ì… ë¶„ë¥˜**:
```python
def classify_query(query: str, llm) -> dict:
    """ì¿¼ë¦¬ íƒ€ì…ì„ ë¶„ë¥˜í•˜ê³  ê²€ìƒ‰ ì „ëµ ê²°ì •"""

    prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ ì „ëµì„ ê²°ì •í•˜ì„¸ìš”.

ì¿¼ë¦¬: "{query}"

ë¶„ë¥˜ ê¸°ì¤€:
1. metadata_search: ì €ìëª…, ì œëª©, ê¸°ê´€ ë“± ë©”íƒ€ë°ì´í„°ë¡œ ê²€ìƒ‰
2. semantic_search: ë‚´ìš© ê¸°ë°˜ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰
3. hybrid_search: ë©”íƒ€ë°ì´í„° + ë‚´ìš© ëª¨ë‘ í•„ìš”

ì¶œë ¥ í˜•ì‹ (JSON):
{{
    "type": "metadata_search" | "semantic_search" | "hybrid_search",
    "metadata_filters": {{
        "authors": ["ê¹€ì² ìˆ˜"],
        "year": {{"$gte": 2020}}
    }},
    "semantic_query": "ì¬êµ¬ì„±ëœ ì˜ë¯¸ë¡ ì  ì¿¼ë¦¬",
    "needs_full_document": true | false
}}
"""

    response = llm.invoke(prompt)
    return json.loads(response)
```

**í˜„ì¬ í”„ë¡œì íŠ¸ ì ìš© (Question Classifier í™•ì¥)**:
```python
# utils/question_classifier.py í™•ì¥
QUERY_TYPES = {
    "metadata_search": {
        "description": "ì €ìëª…, ì œëª©, ê¸°ê´€ ë“±ìœ¼ë¡œ ë…¼ë¬¸ ê²€ìƒ‰",
        "keywords": ["ì €ì", "ì‘ì„±ì", "ì†Œì†", "ì œëª©", "ì €ë„", "ë°œí‘œ", "ë…¼ë¬¸"],
        "examples": [
            "ê¹€ì² ìˆ˜ê°€ ì“´ ë…¼ë¬¸ ì°¾ì•„ì¤˜",
            "LG Display ì†Œì† ì—°êµ¬ìì˜ OLED ë…¼ë¬¸",
            "Natureì— ì‹¤ë¦° ìµœì‹  ì—°êµ¬"
        ],
        "search_strategy": "metadata_filter_then_load_full_document"
    },
    # ê¸°ì¡´ íƒ€ì…ë“¤...
}
```

---

### 4. 2ë‹¨ê³„ ê²€ìƒ‰ ì „ëµ (Metadata â†’ Full Document)

**êµ¬í˜„ íŒ¨í„´**:
```python
def metadata_based_search(query: str, metadata_filter: dict, vectorstore, llm):
    """ë©”íƒ€ë°ì´í„° ê¸°ë°˜ 2ë‹¨ê³„ ê²€ìƒ‰"""

    # Stage 1: ë©”íƒ€ë°ì´í„° í•„í„°ë§ìœ¼ë¡œ ë…¼ë¬¸ ì‹ë³„
    filtered_papers = vectorstore.get(
        where=metadata_filter,
        include=["metadatas"]
    )

    # ë…¼ë¬¸ ID ì¶”ì¶œ (ì¤‘ë³µ ì œê±°)
    paper_ids = set()
    for meta in filtered_papers["metadatas"]:
        paper_ids.add(meta["document_id"])

    print(f"[Stage 1] {len(paper_ids)}ê°œ ë…¼ë¬¸ ì‹ë³„ë¨")

    # Stage 2: ê° ë…¼ë¬¸ì˜ ëª¨ë“  ì²­í¬ ë¡œë“œ
    all_chunks = []
    for paper_id in paper_ids:
        chunks = vectorstore.get(
            where={"document_id": paper_id},
            include=["documents", "metadatas"]
        )
        all_chunks.extend(zip(chunks["documents"], chunks["metadatas"]))

    print(f"[Stage 2] {len(all_chunks)}ê°œ ì²­í¬ ë¡œë“œë¨")

    # Stage 3: LLMì— ì „ë‹¬í•˜ì—¬ ìš”ì•½
    # (ì»¨í…ìŠ¤íŠ¸ í¬ê¸° ì œí•œ ê³ ë ¤ - í•„ìš”ì‹œ ì„¹ì…˜ë³„ ìš”ì•½ í›„ ë³‘í•©)
    if len(all_chunks) > 50:
        # ë…¼ë¬¸ë³„ë¡œ ë¨¼ì € ìš”ì•½
        paper_summaries = []
        for paper_id in paper_ids:
            paper_chunks = [c for c in all_chunks if c[1]["document_id"] == paper_id]
            summary = llm.summarize(paper_chunks)
            paper_summaries.append(summary)

        # ì „ì²´ ìš”ì•½ ë³‘í•©
        final_summary = llm.synthesize(paper_summaries)
    else:
        # ì§ì ‘ ìš”ì•½
        final_summary = llm.summarize(all_chunks)

    return final_summary
```

---

## ğŸ“‹ í˜„ì¬ í”„ë¡œì íŠ¸ ì ìš© ë¡œë“œë§µ

### Phase 1: ë¹ ë¥¸ êµ¬í˜„ (1-2ì¼) âœ… ìš°ì„ ìˆœìœ„ High

**ëª©í‘œ**: ì €ìëª… ê²€ìƒ‰ ê¸°ë³¸ ê¸°ëŠ¥ êµ¬í˜„

1. **Question Classifier í™•ì¥**:
   - `metadata_search` íƒ€ì… ì¶”ê°€
   - í‚¤ì›Œë“œ: ["ì €ì", "ì‘ì„±ì", "íŒŒì¼ëª…"]
   - ê¸°ì¡´ LLM ê¸°ë°˜ ë¶„ë¥˜ê¸° í™œìš©

2. **RAG Chainì— ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ ë¡œì§ ì¶”ê°€**:
   ```python
   # utils/rag_chain.py
   def _handle_metadata_search(self, query, metadata_filter):
       # í˜„ì¬ëŠ” source(íŒŒì¼ëª…)ë§Œ í•„í„°ë§ ê°€ëŠ¥
       papers = self.vectorstore.get(
           where={"source": {"$contains": metadata_filter["filename"]}}
       )
       # ì „ì²´ ì²­í¬ ë¡œë“œ ë° ìš”ì•½
   ```

3. **í…ŒìŠ¤íŠ¸**:
   - "Balkenhol ë…¼ë¬¸ ìš”ì•½í•´ì¤˜" (íŒŒì¼ëª… ê²€ìƒ‰)
   - "cosmology íŒŒì¼ ì°¾ì•„ì¤˜" (íŒŒì¼ëª… ê²€ìƒ‰)

**ì œì•½ì‚¬í•­**: í˜„ì¬ëŠ” `source` í•„ë“œ(íŒŒì¼ëª…)ë§Œ ì‚¬ìš© ê°€ëŠ¥

---

### Phase 2: ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (3-4ì¼) âœ… ìš°ì„ ìˆœìœ„ Medium

**ëª©í‘œ**: PDFì—ì„œ ì €ì, ì œëª©, ì €ë„ ìë™ ì¶”ì¶œ

1. **PyPDF2 ê¸°ë°˜ ê°„ë‹¨í•œ ì¶”ì¶œê¸° ì‘ì„±**:
   ```python
   # utils/pdf_metadata_extractor.py
   def extract_metadata(pdf_path: str) -> dict:
       # PDF ì†ì„± ì½ê¸°
       # ì²« 2í˜ì´ì§€ ì •ê·œì‹ ë§¤ì¹­
       # ì €ìëª… ì¶”ì¶œ (ì˜ë¬¸ ì´ë¦„ íŒ¨í„´)
   ```

2. **ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìˆ˜ì •**:
   ```python
   # utils/document_processor.py
   def process_pdf(self, pdf_path):
       # ê¸°ì¡´ ì²­í‚¹ ë¡œì§
       chunks = self._split_text(text)

       # NEW: ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
       doc_metadata = extract_metadata(pdf_path)

       # ê° ì²­í¬ì— ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
       for chunk in chunks:
           chunk.metadata.update({
               "document_id": generate_doc_id(pdf_path),
               "title": doc_metadata.get("title", ""),
               "authors": doc_metadata.get("authors", [])
           })
   ```

3. **DB ì¬êµ¬ì¶•**:
   - ê¸°ì¡´ DB ë°±ì—…
   - ìƒˆ ë©”íƒ€ë°ì´í„° í¬í•¨í•˜ì—¬ ì¬ì„ë² ë”©

**í•œê³„**: ì •ê·œì‹ ê¸°ë°˜ì´ë¼ ì •í™•ë„ ì œí•œì  (60-70% ì˜ˆìƒ)

---

### Phase 3: GROBID í†µí•© (ì„ íƒ ì‚¬í•­, 1ì£¼) âš ï¸ ìš°ì„ ìˆœìœ„ Low

**ëª©í‘œ**: ë†’ì€ ì •í™•ë„ì˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

1. **GROBID ì„œë²„ ì„¤ì¹˜** (Docker ê¶Œì¥):
   ```bash
   docker pull lfoppiano/grobid:0.8.0
   docker run -t --rm -p 8070:8070 lfoppiano/grobid:0.8.0
   ```

2. **GROBID API ì—°ë™**:
   ```python
   def extract_metadata_grobid(pdf_path: str) -> dict:
       # GROBID API í˜¸ì¶œ
       # XML íŒŒì‹±
       # ì €ì, ì†Œì†, ì œëª©, ì´ˆë¡, ì°¸ê³ ë¬¸í—Œ ì¶”ì¶œ
   ```

3. **ì •í™•ë„ ê²€ì¦**:
   - ìƒ˜í”Œ 100ê°œ ë…¼ë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
   - PyPDF2 ë°©ì‹ê³¼ ë¹„êµ

**ì¥ì **: 90% ì´ìƒ ì •í™•ë„
**ë‹¨ì **: ì™¸ë¶€ ì„œë²„ ì˜ì¡´ì„±, ì„¤ì¹˜ ë³µì¡ë„

---

### Phase 4: ê³ ê¸‰ ê²€ìƒ‰ (ì¥ê¸°, Phase 3-4) ğŸš€ ìš°ì„ ìˆœìœ„ Future

**ëª©í‘œ**: Knowledge Graph + Agentic RAG

1. **Knowledge Graph êµ¬ì¶•**:
   - Neo4j ë˜ëŠ” NetworkX
   - ë…¼ë¬¸-ì €ì-ê¸°ê´€-ì£¼ì œ ê´€ê³„ ë§¤í•‘
   - ì¸ìš© ê´€ê³„ ë¶„ì„

2. **Agentic RAG (LangGraph)**:
   ```python
   from langgraph.graph import StateGraph

   # Agentê°€ ìë™ìœ¼ë¡œ ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½
   graph = StateGraph()
   graph.add_node("plan", plan_search_strategy)
   graph.add_node("metadata_search", metadata_search_tool)
   graph.add_node("semantic_search", semantic_search_tool)
   graph.add_node("synthesize", synthesize_results)
   ```

3. **Multi-hop Reasoning**:
   - "ê¹€ì² ìˆ˜ì™€ ê³µì €ìê°€ ë§ì€ ì‚¬ëŒì˜ ìµœì‹  ì—°êµ¬"
   - Graph query + Vector search ê²°í•©

---

## âœ… ê¶Œì¥ ì‚¬í•­

### í˜„ì¬ í”„ë¡œì íŠ¸ì— ì¦‰ì‹œ ì ìš© (Phase 1)

1. âœ… **Question Classifierì— `metadata_search` íƒ€ì… ì¶”ê°€**
   - ê¸°ì¡´ ì¸í”„ë¼ í™œìš©
   - êµ¬í˜„ ì‹œê°„: 4-6ì‹œê°„

2. âœ… **íŒŒì¼ëª… ê¸°ë°˜ ê²€ìƒ‰ ë¨¼ì € êµ¬í˜„**
   - í˜„ì¬ `source` í•„ë“œ í™œìš© ê°€ëŠ¥
   - ì‚¬ìš©ì: "Balkenhol ë…¼ë¬¸ ìš”ì•½í•´ì¤˜"

3. âœ… **2ë‹¨ê³„ ê²€ìƒ‰ ë¡œì§ ì¶”ê°€**
   - ë©”íƒ€ë°ì´í„° í•„í„° â†’ ì „ì²´ ì²­í¬ ë¡œë“œ â†’ ìš”ì•½
   - ê¸°ì¡´ RAG Chain í™•ì¥

### ì¤‘ê¸° ëª©í‘œ (Phase 2.5.6)

1. **PyPDF2 ê¸°ë°˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ**
   - ì €ì, ì œëª© ê¸°ë³¸ ì¶”ì¶œ
   - ì •í™•ë„ 60-70%ë¡œ ì‹œì‘

2. **DB ì¬êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸**
   - ê¸°ì¡´ PDF ì¬ì²˜ë¦¬
   - ë©”íƒ€ë°ì´í„° ì¶”ê°€

3. **í…ŒìŠ¤íŠ¸ ë° ê²€ì¦**
   - ìƒ˜í”Œ 20ê°œ ë…¼ë¬¸ìœ¼ë¡œ ì •í™•ë„ í™•ì¸
   - ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘

### ì¥ê¸° ëª©í‘œ (Phase 4)

1. **GROBID ë„ì… ê²€í† ** (ì •í™•ë„ ì¤‘ìš”ì‹œ)
2. **Knowledge Graph** (ê´€ê³„ ê¸°ë°˜ ê²€ìƒ‰)
3. **Agentic RAG** (ë³µì¡í•œ ë©€í‹°ìŠ¤í… ì¿¼ë¦¬)

---

## ğŸ“š ì°¸ê³  ìë£Œ

- **Semantic Scholar API**: https://api.semanticscholar.org/
- **GROBID**: https://github.com/kermitt2/grobid
- **LangGraph**: https://langchain-ai.github.io/langgraph/
- **ChromaDB Metadata Filtering**: https://docs.trychroma.com/usage-guide#filtering-by-metadata
- **Perplexity AI Blog**: https://www.perplexity.ai/hub/blog
- **Elicit Research**: https://elicit.com/

---

**ì‘ì„±ì**: Claude Code + OC Papers Team
**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-01-09
