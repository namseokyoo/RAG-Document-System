# ì—°êµ¬ììš© ê³ ì„±ëŠ¥ RAG ì‹œìŠ¤í…œ ì „ëµ ë¬¸ì„œ

## ğŸ“‹ ê°œìš”

ì´ ë¬¸ì„œëŠ” ì—°êµ¬ìì™€ ì—”ì§€ë‹ˆì–´ë¥¼ ìœ„í•œ ê³ ì„±ëŠ¥ RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œ êµ¬ì¶• ì „ëµì„ ì œì‹œí•©ë‹ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ ë‚´ìš©ì„ íš¨ê³¼ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ëŠ” **ë³´ì¡° ì—°êµ¬ì**ë¡œì„œì˜ ì—­í• ì„ ìˆ˜í–‰í•˜ëŠ” ì‹œìŠ¤í…œì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

## ğŸ¯ í•µì‹¬ ëª©í‘œ

### 1. ì—°êµ¬ ì§€ì› íŠ¹í™”
- **ë¬¸í—Œ ê²€ìƒ‰ ë° ë¶„ì„**: ê³¼ê±° ì—°êµ¬ ë…¼ë¬¸, ê¸°ìˆ  ë¬¸ì„œ, ì‹¤í—˜ ë°ì´í„°ì˜ íš¨ìœ¨ì  ê²€ìƒ‰
- **ì¸ì‚¬ì´íŠ¸ ë„ì¶œ**: ì—°êµ¬ ê°„ ì—°ê´€ì„± ë°œê²¬ ë° ìƒˆë¡œìš´ ì—°êµ¬ ë°©í–¥ ì œì‹œ
- **ì§€ì‹ í†µí•©**: ë‹¤ì–‘í•œ ì†ŒìŠ¤ì˜ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì¢…í•©ì  ê´€ì  ì œê³µ

### 2. ì„±ëŠ¥ ìµœì í™” ëª©í‘œ
- **ê²€ìƒ‰ ì •í™•ë„**: 90% ì´ìƒì˜ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ê²€ìƒ‰
- **ì‘ë‹µ í’ˆì§ˆ**: ì—°êµ¬ì ìˆ˜ì¤€ì˜ ì „ë¬¸ì ì´ê³  ì •í™•í•œ ë‹µë³€ ìƒì„±
- **ì²˜ë¦¬ ì†ë„**: ì‹¤ì‹œê°„ ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ ì§€ì›
- **í™•ì¥ì„±**: ëŒ€ê·œëª¨ ì—°êµ¬ ë°ì´í„°ë² ì´ìŠ¤ ì²˜ë¦¬ ëŠ¥ë ¥

---

## ğŸ” í˜„ì¬ í”„ë¡œì íŠ¸ ë¶„ì„

### ê¸°ì¡´ ì‹œìŠ¤í…œ ê°•ì 
1. **êµ¬ì¡° ì¸ì‹ ì²­í‚¹**: PDF, PPTX, XLSX, TXT íŒŒì¼ë³„ ìµœì í™”ëœ ì²­í‚¹ ì „ëµ
2. **Small-to-Large ì•„í‚¤í…ì²˜**: ì •í™•ì„±ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë™ì‹œì— í™•ë³´í•˜ëŠ” ê³„ì¸µì  ê²€ìƒ‰
3. **ë‹¤ì¤‘ ì¿¼ë¦¬ ì „ëµ**: ë™ì˜ì–´ í™•ì¥, ì¿¼ë¦¬ ì¬ì‘ì„±, HyDE ë“± ê³ ê¸‰ ê²€ìƒ‰ ê¸°ë²•
4. **Re-ranking ì‹œìŠ¤í…œ**: ê²€ìƒ‰ ê²°ê³¼ì˜ ê´€ë ¨ì„± ì¬í‰ê°€ ë° ìµœì í™”

### ê°œì„  í•„ìš” ì˜ì—­
1. **ì—°êµ¬ ë„ë©”ì¸ íŠ¹í™”**: í•™ìˆ  ë…¼ë¬¸, ê¸°ìˆ  ë¬¸ì„œì˜ íŠ¹ì„± ë°˜ì˜ ë¶€ì¡±
2. **ë©”íƒ€ë°ì´í„° í™œìš©**: ì €ì, ë°œí–‰ë…„ë„, ì¸ìš©ìˆ˜ ë“± ì—°êµ¬ ë©”íƒ€ë°ì´í„° ë¯¸í™œìš©
3. **ë‹¤êµ­ì–´ ì§€ì›**: êµ­ì œ ì—°êµ¬ ë…¼ë¬¸ ì²˜ë¦¬ ëŠ¥ë ¥ ì œí•œ
4. **ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸**: ìƒˆë¡œìš´ ì—°êµ¬ ì„±ê³¼ ë°˜ì˜ ë©”ì»¤ë‹ˆì¦˜ ë¶€ì¬

---

## ğŸš€ ê³ ì„±ëŠ¥ RAG ì „ëµ

### 1. ì—°êµ¬ ë„ë©”ì¸ íŠ¹í™” ì•„í‚¤í…ì²˜

#### 1.1 Excel ìˆ˜ì‹ ë° ê³„ì‚° ë¡œì§ íŠ¹í™” ì²˜ë¦¬

Excel íŒŒì¼ì€ ë‹¨ìˆœí•œ ë°ì´í„° ì €ì¥ì†Œê°€ ì•„ë‹Œ ë³µì¡í•œ ê³„ì‚° ì—”ì§„ì…ë‹ˆë‹¤. ì—°êµ¬ì—ì„œ ì‚¬ìš©ë˜ëŠ” Excel íŒŒì¼ë“¤ì€ ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ê°€ì§‘ë‹ˆë‹¤:

- **ì‹¤í—˜ ë°ì´í„° ë¶„ì„**: í†µê³„ ê³„ì‚°, íšŒê·€ ë¶„ì„, ìƒê´€ê´€ê³„ ë¶„ì„
- **ìˆ˜í•™ì  ëª¨ë¸ë§**: ë¯¸ë¶„ë°©ì •ì‹, ìµœì í™” ë¬¸ì œ, ì‹œë®¬ë ˆì´ì…˜
- **ì¬ë¬´ ë¶„ì„**: NPV, IRR, í• ì¸ìœ¨ ê³„ì‚°
- **ê³¼í•™ ê³„ì‚°**: ë¬¼ë¦¬ ìƒìˆ˜, í™”í•™ ë°˜ì‘ì‹, ìƒë¬¼í•™ì  ëª¨ë¸

```python
class ExcelFormulaAnalyzer:
    """Excel ìˆ˜ì‹ ë° ê³„ì‚° ë¡œì§ ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.formula_parser = FormulaParser()
        self.calculation_engine = CalculationEngine()
        self.dependency_analyzer = DependencyAnalyzer()
        self.semantic_extractor = SemanticExtractor()
    
    def analyze_excel_workbook(self, file_path: str) -> ExcelWorkbookAnalysis:
        """Excel ì›Œí¬ë¶ ì „ì²´ ë¶„ì„"""
        analysis = ExcelWorkbookAnalysis()
        
        # 1. ì›Œí¬ì‹œíŠ¸ë³„ ë¶„ì„
        for sheet_name in self._get_worksheet_names(file_path):
            sheet_analysis = self._analyze_worksheet(file_path, sheet_name)
            analysis.add_sheet_analysis(sheet_name, sheet_analysis)
        
        # 2. ì›Œí¬ì‹œíŠ¸ ê°„ ì˜ì¡´ì„± ë¶„ì„
        analysis.cross_sheet_dependencies = self._analyze_cross_sheet_dependencies(analysis)
        
        # 3. ê³„ì‚° ì²´ì¸ ë¶„ì„
        analysis.calculation_chains = self._analyze_calculation_chains(analysis)
        
        # 4. ë°ì´í„° íë¦„ ë¶„ì„
        analysis.data_flow = self._analyze_data_flow(analysis)
        
        return analysis
    
    def _analyze_worksheet(self, file_path: str, sheet_name: str) -> WorksheetAnalysis:
        """ê°œë³„ ì›Œí¬ì‹œíŠ¸ ë¶„ì„"""
        worksheet = self._load_worksheet(file_path, sheet_name)
        analysis = WorksheetAnalysis(sheet_name=sheet_name)
        
        # 1. ìˆ˜ì‹ ì¶”ì¶œ ë° ë¶„ë¥˜
        formulas = self._extract_formulas(worksheet)
        analysis.formulas = self._classify_formulas(formulas)
        
        # 2. ë°ì´í„° ì˜ì—­ ì‹ë³„
        analysis.data_regions = self._identify_data_regions(worksheet)
        
        # 3. ê³„ì‚° ë¡œì§ ì¶”ì¶œ
        analysis.calculation_logic = self._extract_calculation_logic(formulas)
        
        # 4. ì˜ì¡´ì„± ê·¸ë˜í”„ ìƒì„±
        analysis.dependency_graph = self._build_dependency_graph(formulas)
        
        # 5. ì˜ë¯¸ë¡ ì  ì²­í‚¹
        analysis.semantic_chunks = self._create_semantic_chunks(worksheet, analysis)
        
        return analysis
    
    def _classify_formulas(self, formulas: List[Formula]) -> Dict[str, List[Formula]]:
        """ìˆ˜ì‹ ë¶„ë¥˜ (ì—°êµ¬ ë„ë©”ì¸ë³„)"""
        classified = {
            "statistical": [],      # í†µê³„ í•¨ìˆ˜ (AVERAGE, STDEV, CORREL)
            "mathematical": [],     # ìˆ˜í•™ í•¨ìˆ˜ (SUM, PRODUCT, POWER)
            "financial": [],        # ì¬ë¬´ í•¨ìˆ˜ (NPV, IRR, PMT)
            "scientific": [],       # ê³¼í•™ í•¨ìˆ˜ (SIN, COS, EXP)
            "logical": [],          # ë…¼ë¦¬ í•¨ìˆ˜ (IF, AND, OR)
            "lookup": [],           # ì¡°íšŒ í•¨ìˆ˜ (VLOOKUP, INDEX, MATCH)
            "text": [],             # í…ìŠ¤íŠ¸ í•¨ìˆ˜ (CONCATENATE, LEFT, RIGHT)
            "date_time": [],        # ë‚ ì§œ/ì‹œê°„ í•¨ìˆ˜ (NOW, DATE, DATEDIF)
            "custom": []            # ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜
        }
        
        for formula in formulas:
            formula_type = self._determine_formula_type(formula)
            classified[formula_type].append(formula)
        
        return classified
    
    def _extract_calculation_logic(self, formulas: List[Formula]) -> CalculationLogic:
        """ê³„ì‚° ë¡œì§ ì¶”ì¶œ ë° êµ¬ì¡°í™”"""
        logic = CalculationLogic()
        
        for formula in formulas:
            # 1. ìˆ˜ì‹ì˜ ëª©ì  ë¶„ì„
            purpose = self._analyze_formula_purpose(formula)
            logic.add_purpose(formula.cell_reference, purpose)
            
            # 2. ê³„ì‚° ë‹¨ê³„ ë¶„í•´
            steps = self._decompose_calculation_steps(formula)
            logic.add_calculation_steps(formula.cell_reference, steps)
            
            # 3. ì…ë ¥/ì¶œë ¥ ê´€ê³„ ì •ì˜
            inputs, outputs = self._extract_input_output_relationship(formula)
            logic.add_io_relationship(formula.cell_reference, inputs, outputs)
            
            # 4. ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì¶”ì¶œ
            business_rules = self._extract_business_rules(formula)
            logic.add_business_rules(formula.cell_reference, business_rules)
        
        return logic
    
    def _create_semantic_chunks(self, worksheet, analysis: WorksheetAnalysis) -> List[ExcelChunk]:
        """Excel íŠ¹í™” ì˜ë¯¸ë¡ ì  ì²­í¬ ìƒì„±"""
        chunks = []
        
        # 1. ìˆ˜ì‹ ê·¸ë£¹ë³„ ì²­í¬ ìƒì„±
        for formula_type, formulas in analysis.formulas.items():
            if formulas:
                chunk = self._create_formula_group_chunk(formula_type, formulas, analysis)
                chunks.append(chunk)
        
        # 2. ë°ì´í„° ì˜ì—­ë³„ ì²­í¬ ìƒì„±
        for region in analysis.data_regions:
            chunk = self._create_data_region_chunk(region, analysis)
            chunks.append(chunk)
        
        # 3. ê³„ì‚° ì²´ì¸ë³„ ì²­í¬ ìƒì„±
        for chain in analysis.calculation_chains:
            chunk = self._create_calculation_chain_chunk(chain, analysis)
            chunks.append(chunk)
        
        # 4. ì˜ì¡´ì„± ê·¸ë£¹ë³„ ì²­í¬ ìƒì„±
        dependency_groups = self._group_by_dependencies(analysis.dependency_graph)
        for group in dependency_groups:
            chunk = self._create_dependency_group_chunk(group, analysis)
            chunks.append(chunk)
        
        return chunks
    
    def _create_formula_group_chunk(self, formula_type: str, formulas: List[Formula], 
                                   analysis: WorksheetAnalysis) -> ExcelChunk:
        """ìˆ˜ì‹ ê·¸ë£¹ë³„ ì²­í¬ ìƒì„±"""
        chunk_content = f"ìˆ˜ì‹ ìœ í˜•: {formula_type}\n\n"
        
        # ìˆ˜ì‹ë³„ ìƒì„¸ ì„¤ëª…
        for formula in formulas:
            chunk_content += f"ì…€ {formula.cell_reference}: {formula.formula_text}\n"
            chunk_content += f"ëª©ì : {formula.purpose}\n"
            chunk_content += f"ì…ë ¥: {', '.join(formula.input_cells)}\n"
            chunk_content += f"ì¶œë ¥: {formula.output_description}\n\n"
        
        # ê³„ì‚° ë¡œì§ ìš”ì•½
        calculation_summary = self._summarize_calculation_logic(formulas, analysis)
        chunk_content += f"ê³„ì‚° ë¡œì§ ìš”ì•½:\n{calculation_summary}\n"
        
        return ExcelChunk(
            content=chunk_content,
            chunk_type=f"formula_group_{formula_type}",
            metadata={
                "formula_type": formula_type,
                "formula_count": len(formulas),
                "complexity_score": self._calculate_complexity_score(formulas),
                "calculation_purpose": self._extract_group_purpose(formulas),
                "data_dependencies": self._extract_group_dependencies(formulas),
                "research_relevance": self._assess_research_relevance(formulas)
            },
            weight=self._calculate_formula_group_weight(formula_type, formulas)
        )
    
    def _create_calculation_chain_chunk(self, chain: CalculationChain, 
                                       analysis: WorksheetAnalysis) -> ExcelChunk:
        """ê³„ì‚° ì²´ì¸ë³„ ì²­í¬ ìƒì„±"""
        chunk_content = f"ê³„ì‚° ì²´ì¸: {chain.name}\n\n"
        
        # ì²´ì¸ ë‹¨ê³„ë³„ ì„¤ëª…
        for i, step in enumerate(chain.steps):
            chunk_content += f"ë‹¨ê³„ {i+1}: {step.description}\n"
            chunk_content += f"ìˆ˜ì‹: {step.formula}\n"
            chunk_content += f"ì…ë ¥ê°’: {step.inputs}\n"
            chunk_content += f"ì¶œë ¥ê°’: {step.outputs}\n\n"
        
        # ì „ì²´ ê³„ì‚° íë¦„ ìš”ì•½
        flow_summary = self._summarize_calculation_flow(chain)
        chunk_content += f"ê³„ì‚° íë¦„:\n{flow_summary}\n"
        
        return ExcelChunk(
            content=chunk_content,
            chunk_type="calculation_chain",
            metadata={
                "chain_name": chain.name,
                "step_count": len(chain.steps),
                "complexity_level": chain.complexity_level,
                "calculation_type": chain.calculation_type,
                "research_domain": chain.research_domain,
                "validation_rules": chain.validation_rules
            },
            weight=2.5  # ê³„ì‚° ì²´ì¸ì€ ë†’ì€ ê°€ì¤‘ì¹˜
        )
```

#### 1.2 Excel ìˆ˜ì‹ ê¸°ë°˜ ì—°êµ¬ ì¸ì‚¬ì´íŠ¸ ìƒì„±

```python
class ExcelInsightGenerator:
    """Excel ìˆ˜ì‹ ê¸°ë°˜ ì—°êµ¬ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
    
    def __init__(self, llm_service, formula_analyzer):
        self.llm_service = llm_service
        self.formula_analyzer = formula_analyzer
        self.pattern_recognizer = FormulaPatternRecognizer()
        self.research_connector = ResearchConnector()
    
    def generate_excel_insights(self, excel_analysis: ExcelWorkbookAnalysis, 
                              research_context: str) -> Dict:
        """Excel ë¶„ì„ ê¸°ë°˜ ì—°êµ¬ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = {
            "calculation_methodology": "",
            "data_analysis_patterns": [],
            "research_methodology_match": [],
            "potential_improvements": [],
            "alternative_calculations": [],
            "validation_suggestions": [],
            "research_applications": []
        }
        
        # 1. ê³„ì‚° ë°©ë²•ë¡  ë¶„ì„
        insights["calculation_methodology"] = self._analyze_calculation_methodology(excel_analysis)
        
        # 2. ë°ì´í„° ë¶„ì„ íŒ¨í„´ ì‹ë³„
        insights["data_analysis_patterns"] = self._identify_analysis_patterns(excel_analysis)
        
        # 3. ì—°êµ¬ ë°©ë²•ë¡  ë§¤ì¹­
        insights["research_methodology_match"] = self._match_research_methodology(
            excel_analysis, research_context
        )
        
        # 4. ê°œì„  ì œì•ˆ
        insights["potential_improvements"] = self._suggest_improvements(excel_analysis)
        
        # 5. ëŒ€ì•ˆ ê³„ì‚° ë°©ë²•
        insights["alternative_calculations"] = self._suggest_alternatives(excel_analysis)
        
        # 6. ê²€ì¦ ì œì•ˆ
        insights["validation_suggestions"] = self._suggest_validations(excel_analysis)
        
        # 7. ì—°êµ¬ ì‘ìš© ë¶„ì•¼
        insights["research_applications"] = self._identify_research_applications(excel_analysis)
        
        return insights
    
    def _analyze_calculation_methodology(self, analysis: ExcelWorkbookAnalysis) -> str:
        """ê³„ì‚° ë°©ë²•ë¡  ë¶„ì„"""
        methodology_analysis = []
        
        # í†µê³„ ë¶„ì„ ë°©ë²•ë¡ 
        if analysis.has_statistical_formulas():
            stats_methods = self._identify_statistical_methods(analysis)
            methodology_analysis.append(f"í†µê³„ ë¶„ì„: {', '.join(stats_methods)}")
        
        # ìˆ˜í•™ì  ëª¨ë¸ë§
        if analysis.has_mathematical_formulas():
            math_methods = self._identify_mathematical_methods(analysis)
            methodology_analysis.append(f"ìˆ˜í•™ì  ëª¨ë¸ë§: {', '.join(math_methods)}")
        
        # ìµœì í™” ê¸°ë²•
        if analysis.has_optimization_formulas():
            opt_methods = self._identify_optimization_methods(analysis)
            methodology_analysis.append(f"ìµœì í™” ê¸°ë²•: {', '.join(opt_methods)}")
        
        return "\n".join(methodology_analysis)
    
    def _identify_analysis_patterns(self, analysis: ExcelWorkbookAnalysis) -> List[str]:
        """ë°ì´í„° ë¶„ì„ íŒ¨í„´ ì‹ë³„"""
        patterns = []
        
        # ìƒê´€ê´€ê³„ ë¶„ì„ íŒ¨í„´
        if self._has_correlation_analysis(analysis):
            patterns.append("ìƒê´€ê´€ê³„ ë¶„ì„: ë³€ìˆ˜ ê°„ ê´€ê³„ ë¶„ì„")
        
        # íšŒê·€ ë¶„ì„ íŒ¨í„´
        if self._has_regression_analysis(analysis):
            patterns.append("íšŒê·€ ë¶„ì„: ì¢…ì†ë³€ìˆ˜ ì˜ˆì¸¡ ëª¨ë¸")
        
        # ì‹œê³„ì—´ ë¶„ì„ íŒ¨í„´
        if self._has_time_series_analysis(analysis):
            patterns.append("ì‹œê³„ì—´ ë¶„ì„: ì‹œê°„ì— ë”°ë¥¸ ë³€í™” íŒ¨í„´")
        
        # ë¶„ì‚° ë¶„ì„ íŒ¨í„´
        if self._has_anova_analysis(analysis):
            patterns.append("ë¶„ì‚° ë¶„ì„: ê·¸ë£¹ ê°„ ì°¨ì´ ê²€ì •")
        
        return patterns
    
    def _suggest_improvements(self, analysis: ExcelWorkbookAnalysis) -> List[str]:
        """Excel ê³„ì‚° ê°œì„  ì œì•ˆ"""
        improvements = []
        
        # ìˆ˜ì‹ ìµœì í™” ì œì•ˆ
        for formula_group in analysis.formula_groups:
            if formula_group.complexity_score > 0.8:
                improvements.append(
                    f"ë³µì¡í•œ ìˆ˜ì‹ ê·¸ë£¹ '{formula_group.name}'ì„ ë‹¨ê³„ë³„ë¡œ ë¶„í•´í•˜ì—¬ ê°€ë…ì„± í–¥ìƒ"
                )
        
        # ë°ì´í„° ê²€ì¦ ì œì•ˆ
        if not analysis.has_data_validation():
            improvements.append("ë°ì´í„° ì…ë ¥ ê²€ì¦ ê·œì¹™ ì¶”ê°€ë¡œ ì˜¤ë¥˜ ë°©ì§€")
        
        # ê³„ì‚° íš¨ìœ¨ì„± ì œì•ˆ
        inefficient_formulas = self._identify_inefficient_formulas(analysis)
        for formula in inefficient_formulas:
            improvements.append(
                f"ì…€ {formula.cell_reference}ì˜ ìˆ˜ì‹ì„ ë” íš¨ìœ¨ì ì¸ ë°©ë²•ìœ¼ë¡œ ê°œì„ "
            )
        
        return improvements
```

#### 1.3 Excel ìˆ˜ì‹ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ

```python
class ExcelFormulaQA:
    """Excel ìˆ˜ì‹ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ"""
    
    def __init__(self, formula_analyzer, llm_service):
        self.formula_analyzer = formula_analyzer
        self.llm_service = llm_service
        self.formula_explainer = FormulaExplainer()
        self.calculation_simulator = CalculationSimulator()
    
    def answer_excel_questions(self, question: str, excel_context: ExcelWorkbookAnalysis) -> Dict:
        """Excel ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
        answer = {
            "response": "",
            "formula_explanations": [],
            "calculation_examples": [],
            "related_formulas": [],
            "validation_checks": [],
            "improvement_suggestions": []
        }
        
        # 1. ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜
        question_type = self._classify_question_type(question)
        
        # 2. ê´€ë ¨ ìˆ˜ì‹ ê²€ìƒ‰
        relevant_formulas = self._find_relevant_formulas(question, excel_context)
        
        # 3. ìˆ˜ì‹ ì„¤ëª… ìƒì„±
        for formula in relevant_formulas:
            explanation = self.formula_explainer.explain_formula(formula, question)
            answer["formula_explanations"].append(explanation)
        
        # 4. ê³„ì‚° ì˜ˆì‹œ ìƒì„±
        if question_type == "calculation":
            examples = self._generate_calculation_examples(question, relevant_formulas)
            answer["calculation_examples"] = examples
        
        # 5. ê´€ë ¨ ìˆ˜ì‹ ì œì•ˆ
        answer["related_formulas"] = self._suggest_related_formulas(question, excel_context)
        
        # 6. ê²€ì¦ ë°©ë²• ì œì•ˆ
        answer["validation_checks"] = self._suggest_validation_checks(question, relevant_formulas)
        
        # 7. ê°œì„  ì œì•ˆ
        answer["improvement_suggestions"] = self._suggest_improvements(question, relevant_formulas)
        
        # 8. ì¢…í•© ë‹µë³€ ìƒì„±
        answer["response"] = self._generate_comprehensive_answer(question, answer)
        
        return answer
    
    def _classify_question_type(self, question: str) -> str:
        """ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜"""
        question_lower = question.lower()
        
        if any(word in question_lower for word in ["ì–´ë–»ê²Œ", "ë°©ë²•", "ê³„ì‚°"]):
            return "calculation"
        elif any(word in question_lower for word in ["ì™œ", "ì´ìœ ", "ëª©ì "]):
            return "explanation"
        elif any(word in question_lower for word in ["ê°œì„ ", "ìµœì í™”", "íš¨ìœ¨"]):
            return "optimization"
        elif any(word in question_lower for word in ["ê²€ì¦", "í™•ì¸", "ì •í™•"]):
            return "validation"
        else:
            return "general"
    
    def _find_relevant_formulas(self, question: str, excel_context: ExcelWorkbookAnalysis) -> List[Formula]:
        """ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ìˆ˜ì‹ ê²€ìƒ‰"""
        relevant_formulas = []
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
        keywords = self._extract_keywords(question)
        for keyword in keywords:
            formulas = excel_context.find_formulas_by_keyword(keyword)
            relevant_formulas.extend(formulas)
        
        # ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„± ê¸°ë°˜ ê²€ìƒ‰
        semantic_formulas = excel_context.find_formulas_by_semantic_similarity(question)
        relevant_formulas.extend(semantic_formulas)
        
        # ì¤‘ë³µ ì œê±° ë° ê´€ë ¨ì„± ìˆœ ì •ë ¬
        unique_formulas = list(set(relevant_formulas))
        unique_formulas.sort(key=lambda f: f.relevance_score, reverse=True)
        
        return unique_formulas[:5]  # ìƒìœ„ 5ê°œ ë°˜í™˜
    
    def _generate_calculation_examples(self, question: str, formulas: List[Formula]) -> List[Dict]:
        """ê³„ì‚° ì˜ˆì‹œ ìƒì„±"""
        examples = []
        
        for formula in formulas:
            example = {
                "formula": formula.formula_text,
                "cell_reference": formula.cell_reference,
                "step_by_step": [],
                "sample_data": {},
                "result": None
            }
            
            # ë‹¨ê³„ë³„ ê³„ì‚° ê³¼ì •
            steps = self.formula_explainer.decompose_calculation_steps(formula)
            example["step_by_step"] = steps
            
            # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
            sample_data = self._generate_sample_data(formula)
            example["sample_data"] = sample_data
            
            # ê²°ê³¼ ê³„ì‚°
            result = self.calculation_simulator.simulate_calculation(formula, sample_data)
            example["result"] = result
            
            examples.append(example)
        
        return examples
```

#### 1.4 Excel-PPT ì—°ê³„ ë¶„ì„ ì‹œìŠ¤í…œ

ì—°êµ¬ì—ì„œ Excelê³¼ PPTëŠ” ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. Excelì—ì„œ ê³„ì‚°ëœ ê²°ê³¼ê°€ PPTë¡œ ì‹œê°í™”ë˜ê³ , PPTì˜ ë‚´ìš©ì´ Excel ë¶„ì„ì˜ ë°©í–¥ì„ ì œì‹œí•©ë‹ˆë‹¤.

```python
class ExcelPPTConnector:
    """Excelê³¼ PPT ì—°ê³„ ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self, excel_analyzer, ppt_analyzer):
        self.excel_analyzer = excel_analyzer
        self.ppt_analyzer = ppt_analyzer
        self.correlation_finder = CorrelationFinder()
        self.visualization_matcher = VisualizationMatcher()
    
    def analyze_excel_ppt_relationship(self, excel_file: str, ppt_file: str) -> Dict:
        """Excelê³¼ PPT ê°„ì˜ ì—°ê´€ì„± ë¶„ì„"""
        analysis = {
            "data_visualization_mapping": [],
            "calculation_presentation_flow": [],
            "cross_references": [],
            "inconsistencies": [],
            "enhancement_suggestions": []
        }
        
        # 1. Excel ë°ì´í„°ì™€ PPT ì°¨íŠ¸ ë§¤í•‘
        excel_data = self.excel_analyzer.extract_data_tables(excel_file)
        ppt_charts = self.ppt_analyzer.extract_charts(ppt_file)
        
        analysis["data_visualization_mapping"] = self._map_data_to_charts(excel_data, ppt_charts)
        
        # 2. ê³„ì‚° ê²°ê³¼ì™€ í”„ë ˆì  í…Œì´ì…˜ íë¦„ ë¶„ì„
        excel_calculations = self.excel_analyzer.extract_calculation_results(excel_file)
        ppt_narrative = self.ppt_analyzer.extract_narrative_flow(ppt_file)
        
        analysis["calculation_presentation_flow"] = self._analyze_calculation_flow(
            excel_calculations, ppt_narrative
        )
        
        # 3. ìƒí˜¸ ì°¸ì¡° ë¶„ì„
        analysis["cross_references"] = self._find_cross_references(excel_file, ppt_file)
        
        # 4. ë¶ˆì¼ì¹˜ ì‚¬í•­ ê²€ì¶œ
        analysis["inconsistencies"] = self._detect_inconsistencies(excel_file, ppt_file)
        
        # 5. ê°œì„  ì œì•ˆ
        analysis["enhancement_suggestions"] = self._suggest_enhancements(analysis)
        
        return analysis
    
    def _map_data_to_charts(self, excel_data: List[DataTable], ppt_charts: List[Chart]) -> List[Dict]:
        """Excel ë°ì´í„°ì™€ PPT ì°¨íŠ¸ ë§¤í•‘"""
        mappings = []
        
        for chart in ppt_charts:
            best_match = None
            best_score = 0
            
            for data_table in excel_data:
                # ë°ì´í„° êµ¬ì¡° ìœ ì‚¬ì„± ë¶„ì„
                structure_similarity = self._calculate_structure_similarity(data_table, chart)
                
                # ìˆ˜ì¹˜ ë²”ìœ„ ìœ ì‚¬ì„± ë¶„ì„
                value_similarity = self._calculate_value_similarity(data_table, chart)
                
                # ë ˆì´ë¸” ìœ ì‚¬ì„± ë¶„ì„
                label_similarity = self._calculate_label_similarity(data_table, chart)
                
                # ì¢…í•© ì ìˆ˜ ê³„ì‚°
                total_score = (structure_similarity * 0.4 + 
                             value_similarity * 0.4 + 
                             label_similarity * 0.2)
                
                if total_score > best_score:
                    best_score = total_score
                    best_match = data_table
            
            if best_match and best_score > 0.6:  # ì„ê³„ê°’ 0.6
                mappings.append({
                    "chart": chart,
                    "data_table": best_match,
                    "confidence": best_score,
                    "mapping_type": self._determine_mapping_type(best_match, chart)
                })
        
        return mappings
    
    def _analyze_calculation_flow(self, calculations: List[Calculation], 
                                 narrative: List[NarrativeElement]) -> List[Dict]:
        """ê³„ì‚° ê²°ê³¼ì™€ í”„ë ˆì  í…Œì´ì…˜ íë¦„ ë¶„ì„"""
        flow_analysis = []
        
        for calc in calculations:
            # ê³„ì‚° ê²°ê³¼ì™€ ê´€ë ¨ëœ ë‚´ëŸ¬í‹°ë¸Œ ìš”ì†Œ ì°¾ê¸°
            related_narrative = self._find_related_narrative(calc, narrative)
            
            if related_narrative:
                flow_analysis.append({
                    "calculation": calc,
                    "narrative_element": related_narrative,
                    "presentation_order": self._determine_presentation_order(calc, related_narrative),
                    "logical_consistency": self._check_logical_consistency(calc, related_narrative),
                    "enhancement_opportunities": self._identify_enhancement_opportunities(calc, related_narrative)
                })
        
        return flow_analysis
```

#### 1.5 Excel ìˆ˜ì‹ ê²€ì¦ ë° í’ˆì§ˆ ê´€ë¦¬

```python
class ExcelFormulaValidator:
    """Excel ìˆ˜ì‹ ê²€ì¦ ë° í’ˆì§ˆ ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.syntax_validator = SyntaxValidator()
        self.logic_validator = LogicValidator()
        self.performance_analyzer = PerformanceAnalyzer()
        self.best_practice_checker = BestPracticeChecker()
    
    def validate_excel_formulas(self, excel_analysis: ExcelWorkbookAnalysis) -> ValidationReport:
        """Excel ìˆ˜ì‹ ì¢…í•© ê²€ì¦"""
        report = ValidationReport()
        
        # 1. êµ¬ë¬¸ ê²€ì¦
        syntax_issues = self.syntax_validator.validate_all_formulas(excel_analysis.formulas)
        report.add_issues("syntax", syntax_issues)
        
        # 2. ë…¼ë¦¬ ê²€ì¦
        logic_issues = self.logic_validator.validate_formula_logic(excel_analysis.formulas)
        report.add_issues("logic", logic_issues)
        
        # 3. ì„±ëŠ¥ ë¶„ì„
        performance_issues = self.performance_analyzer.analyze_performance(excel_analysis.formulas)
        report.add_issues("performance", performance_issues)
        
        # 4. ëª¨ë²” ì‚¬ë¡€ ê²€ì‚¬
        best_practice_issues = self.best_practice_checker.check_best_practices(excel_analysis.formulas)
        report.add_issues("best_practices", best_practice_issues)
        
        # 5. ì¢…í•© ì ìˆ˜ ê³„ì‚°
        report.calculate_overall_score()
        
        return report
    
    def suggest_formula_improvements(self, formula: Formula) -> List[ImprovementSuggestion]:
        """ìˆ˜ì‹ ê°œì„  ì œì•ˆ"""
        suggestions = []
        
        # 1. ì„±ëŠ¥ ìµœì í™” ì œì•ˆ
        if self._is_inefficient_formula(formula):
            suggestions.append(ImprovementSuggestion(
                type="performance",
                description="ìˆ˜ì‹ ì„±ëŠ¥ ìµœì í™”",
                current_formula=formula.formula_text,
                suggested_formula=self._optimize_formula(formula),
                expected_improvement="ê³„ì‚° ì†ë„ 30% í–¥ìƒ"
            ))
        
        # 2. ê°€ë…ì„± ê°œì„  ì œì•ˆ
        if self._is_complex_formula(formula):
            suggestions.append(ImprovementSuggestion(
                type="readability",
                description="ìˆ˜ì‹ ê°€ë…ì„± ê°œì„ ",
                current_formula=formula.formula_text,
                suggested_formula=self._simplify_formula(formula),
                expected_improvement="ì´í•´ë„ í–¥ìƒ ë° ìœ ì§€ë³´ìˆ˜ì„± ê°œì„ "
            ))
        
        # 3. ì˜¤ë¥˜ ë°©ì§€ ì œì•ˆ
        if self._has_error_prone_patterns(formula):
            suggestions.append(ImprovementSuggestion(
                type="error_prevention",
                description="ì˜¤ë¥˜ ë°©ì§€ ê°œì„ ",
                current_formula=formula.formula_text,
                suggested_formula=self._add_error_handling(formula),
                expected_improvement="ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥ì„± ê°ì†Œ"
            ))
        
        return suggestions
```

#### 1.7 PPT ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ë° ì‹œê°ì  ìš”ì†Œ ì¸ì‹

PPT íŒŒì¼ì€ í…ìŠ¤íŠ¸ë¿ë§Œ ì•„ë‹ˆë¼ ê·¸ë˜í”„, ì°¨íŠ¸, í…Œì´ë¸”, ì´ë¯¸ì§€ ë“± ë‹¤ì–‘í•œ ì‹œê°ì  ìš”ì†Œë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë³µí•©ì ì¸ ì½˜í…ì¸ ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë©€í‹°ëª¨ë‹¬ ì ‘ê·¼ë²•ì´ í•„ìš”í•©ë‹ˆë‹¤.

```python
class PPTMultimodalProcessor:
    """PPT ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.ocr_engine = OCREngine()
        self.layout_analyzer = LayoutAnalyzer()
        self.chart_recognizer = ChartRecognizer()
        self.table_extractor = TableExtractor()
        self.multimodal_llm = MultimodalLLM()
        self.image_converter = ImageConverter()
    
    def process_ppt_multimodal(self, ppt_path: str) -> PPTMultimodalAnalysis:
        """PPT íŒŒì¼ì„ ë©€í‹°ëª¨ë‹¬ë¡œ ì²˜ë¦¬"""
        analysis = PPTMultimodalAnalysis()
        
        # 1. ìŠ¬ë¼ì´ë“œë³„ ì´ë¯¸ì§€ ë³€í™˜
        slide_images = self.image_converter.convert_slides_to_images(ppt_path)
        
        # 2. ê° ìŠ¬ë¼ì´ë“œë³„ ë©€í‹°ëª¨ë‹¬ ë¶„ì„
        for slide_num, slide_image in enumerate(slide_images):
            slide_analysis = self._analyze_slide_multimodal(slide_image, slide_num)
            analysis.add_slide_analysis(slide_num, slide_analysis)
        
        # 3. ì „ì²´ í”„ë ˆì  í…Œì´ì…˜ ë§¥ë½ ë¶„ì„
        analysis.presentation_context = self._analyze_presentation_context(analysis)
        
        # 4. ìŠ¬ë¼ì´ë“œ ê°„ ì—°ê´€ì„± ë¶„ì„
        analysis.slide_relationships = self._analyze_slide_relationships(analysis)
        
        return analysis
    
    def _analyze_slide_multimodal(self, slide_image: np.ndarray, slide_num: int) -> SlideMultimodalAnalysis:
        """ê°œë³„ ìŠ¬ë¼ì´ë“œ ë©€í‹°ëª¨ë‹¬ ë¶„ì„"""
        analysis = SlideMultimodalAnalysis(slide_number=slide_num)
        
        # 1. OCRì„ í†µí•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text_elements = self.ocr_engine.extract_text_from_image(slide_image)
        analysis.text_elements = text_elements
        
        # 2. ë ˆì´ì•„ì›ƒ ë¶„ì„
        layout_info = self.layout_analyzer.analyze_layout(slide_image)
        analysis.layout_info = layout_info
        
        # 3. ì°¨íŠ¸/ê·¸ë˜í”„ ì¸ì‹
        charts = self.chart_recognizer.recognize_charts(slide_image)
        analysis.charts = charts
        
        # 4. í…Œì´ë¸” ì¶”ì¶œ
        tables = self.table_extractor.extract_tables(slide_image)
        analysis.tables = tables
        
        # 5. ë©€í‹°ëª¨ë‹¬ LLM ë¶„ì„
        multimodal_analysis = self.multimodal_llm.analyze_slide(
            image=slide_image,
            text_elements=text_elements,
            charts=charts,
            tables=tables,
            layout_info=layout_info
        )
        analysis.multimodal_content = multimodal_analysis
        
        return analysis
    
    def _analyze_presentation_context(self, analysis: PPTMultimodalAnalysis) -> PresentationContext:
        """ì „ì²´ í”„ë ˆì  í…Œì´ì…˜ ë§¥ë½ ë¶„ì„"""
        context = PresentationContext()
        
        # 1. ì „ì²´ í…ìŠ¤íŠ¸ ìš”ì•½
        all_text = []
        for slide_analysis in analysis.slide_analyses.values():
            all_text.extend(slide_analysis.text_elements)
        
        context.overall_summary = self.multimodal_llm.summarize_presentation(all_text)
        
        # 2. ì£¼ìš” ì£¼ì œ ë° í‚¤ì›Œë“œ ì¶”ì¶œ
        context.main_topics = self.multimodal_llm.extract_main_topics(all_text)
        context.keywords = self.multimodal_llm.extract_keywords(all_text)
        
        # 3. í”„ë ˆì  í…Œì´ì…˜ êµ¬ì¡° ë¶„ì„
        context.structure = self._analyze_presentation_structure(analysis)
        
        # 4. ì‹œê°ì  ìš”ì†Œ í†µê³„
        context.visual_statistics = self._calculate_visual_statistics(analysis)
        
        return context
```

#### 1.8 OCR ë° ì‹œê°ì  ìš”ì†Œ ì¸ì‹ ì—”ì§„

```python
class OCREngine:
    """ê³ ê¸‰ OCR ì—”ì§„"""
    
    def __init__(self):
        self.tesseract_ocr = TesseractOCR()
        self.google_vision = GoogleVisionAPI()
        self.paddle_ocr = PaddleOCR()
        self.formula_ocr = FormulaOCR()  # ìˆ˜ì‹ ì „ìš© OCR
    
    def extract_text_from_image(self, image: np.ndarray) -> List[TextElement]:
        """ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        text_elements = []
        
        # 1. ì¼ë°˜ í…ìŠ¤íŠ¸ OCR
        general_text = self.tesseract_ocr.extract_text(image)
        text_elements.extend(self._parse_text_elements(general_text))
        
        # 2. ìˆ˜ì‹ OCR (ìˆ˜í•™ì  í‘œí˜„)
        formulas = self.formula_ocr.extract_formulas(image)
        text_elements.extend(formulas)
        
        # 3. ê³ í’ˆì§ˆ OCR (Google Vision)
        high_quality_text = self.google_vision.extract_text(image)
        text_elements.extend(self._parse_text_elements(high_quality_text))
        
        # 4. ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        text_elements = self._deduplicate_and_clean(text_elements)
        
        return text_elements
    
    def _parse_text_elements(self, raw_text: str) -> List[TextElement]:
        """ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”ëœ ìš”ì†Œë¡œ íŒŒì‹±"""
        elements = []
        
        # í…ìŠ¤íŠ¸ ë¸”ë¡ë³„ë¡œ ë¶„í• 
        blocks = raw_text.split('\n\n')
        
        for block in blocks:
            if block.strip():
                element = TextElement(
                    content=block.strip(),
                    element_type=self._classify_text_type(block),
                    confidence=self._calculate_confidence(block),
                    position=self._extract_position(block)
                )
                elements.append(element)
        
        return elements

class ChartRecognizer:
    """ì°¨íŠ¸/ê·¸ë˜í”„ ì¸ì‹ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.yolo_detector = YOLOChartDetector()
        self.chart_classifier = ChartClassifier()
        self.data_extractor = ChartDataExtractor()
        self.multimodal_llm = MultimodalLLM()
    
    def recognize_charts(self, image: np.ndarray) -> List[ChartElement]:
        """ì´ë¯¸ì§€ì—ì„œ ì°¨íŠ¸/ê·¸ë˜í”„ ì¸ì‹"""
        charts = []
        
        # 1. ì°¨íŠ¸ ì˜ì—­ ê°ì§€
        chart_regions = self.yolo_detector.detect_charts(image)
        
        for region in chart_regions:
            # 2. ì°¨íŠ¸ ìœ í˜• ë¶„ë¥˜
            chart_type = self.chart_classifier.classify_chart_type(region)
            
            # 3. ì°¨íŠ¸ ë°ì´í„° ì¶”ì¶œ
            chart_data = self.data_extractor.extract_data(region, chart_type)
            
            # 4. ë©€í‹°ëª¨ë‹¬ LLMì„ í†µí•œ ì°¨íŠ¸ í•´ì„
            chart_interpretation = self.multimodal_llm.interpret_chart(
                image=region,
                chart_type=chart_type,
                extracted_data=chart_data
            )
            
            chart = ChartElement(
                chart_type=chart_type,
                data=chart_data,
                interpretation=chart_interpretation,
                confidence=region.confidence,
                position=region.position
            )
            charts.append(chart)
        
        return charts

class TableExtractor:
    """í…Œì´ë¸” ì¶”ì¶œ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.table_detector = TableDetector()
        self.cell_extractor = CellExtractor()
        self.table_parser = TableParser()
        self.multimodal_llm = MultimodalLLM()
    
    def extract_tables(self, image: np.ndarray) -> List[TableElement]:
        """ì´ë¯¸ì§€ì—ì„œ í…Œì´ë¸” ì¶”ì¶œ"""
        tables = []
        
        # 1. í…Œì´ë¸” ì˜ì—­ ê°ì§€
        table_regions = self.table_detector.detect_tables(image)
        
        for region in table_regions:
            # 2. ì…€ ê²½ê³„ ê°ì§€
            cell_boundaries = self.cell_extractor.extract_cell_boundaries(region)
            
            # 3. ì…€ ë‚´ìš© ì¶”ì¶œ
            cell_contents = self.cell_extractor.extract_cell_contents(region, cell_boundaries)
            
            # 4. í…Œì´ë¸” êµ¬ì¡° íŒŒì‹±
            table_structure = self.table_parser.parse_table_structure(cell_contents)
            
            # 5. ë©€í‹°ëª¨ë‹¬ LLMì„ í†µí•œ í…Œì´ë¸” í•´ì„
            table_interpretation = self.multimodal_llm.interpret_table(
                image=region,
                structure=table_structure,
                cell_contents=cell_contents
            )
            
            table = TableElement(
                structure=table_structure,
                cell_contents=cell_contents,
                interpretation=table_interpretation,
                confidence=region.confidence,
                position=region.position
            )
            tables.append(table)
        
        return tables
```

#### 1.9 ë©€í‹°ëª¨ë‹¬ LLM í†µí•© ì‹œìŠ¤í…œ

```python
class MultimodalLLM:
    """ë©€í‹°ëª¨ë‹¬ LLM í†µí•© ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.gpt4v = GPT4Vision()
        self.claude3 = Claude3Vision()
        self.gemini_pro = GeminiProVision()
        self.llava = LLaVA()
        self.ensemble_analyzer = EnsembleAnalyzer()
    
    def analyze_slide(self, image: np.ndarray, text_elements: List[TextElement],
                     charts: List[ChartElement], tables: List[TableElement],
                     layout_info: LayoutInfo) -> MultimodalAnalysis:
        """ìŠ¬ë¼ì´ë“œ ë©€í‹°ëª¨ë‹¬ ë¶„ì„"""
        
        # 1. ê° ëª¨ë¸ë³„ ë¶„ì„ ìˆ˜í–‰
        analyses = {}
        
        # GPT-4V ë¶„ì„
        analyses['gpt4v'] = self.gpt4v.analyze_slide(
            image=image,
            text_elements=text_elements,
            charts=charts,
            tables=tables
        )
        
        # Claude 3 Vision ë¶„ì„
        analyses['claude3'] = self.claude3.analyze_slide(
            image=image,
            text_elements=text_elements,
            charts=charts,
            tables=tables
        )
        
        # Gemini Pro Vision ë¶„ì„
        analyses['gemini'] = self.gemini_pro.analyze_slide(
            image=image,
            text_elements=text_elements,
            charts=charts,
            tables=tables
        )
        
        # 2. ì•™ìƒë¸” ë¶„ì„
        ensemble_result = self.ensemble_analyzer.combine_analyses(analyses)
        
        # 3. ìµœì¢… ë¶„ì„ ê²°ê³¼ êµ¬ì„±
        final_analysis = MultimodalAnalysis(
            slide_summary=ensemble_result.summary,
            key_points=ensemble_result.key_points,
            visual_elements=ensemble_result.visual_elements,
            data_insights=ensemble_result.data_insights,
            research_relevance=ensemble_result.research_relevance,
            confidence_scores=ensemble_result.confidence_scores
        )
        
        return final_analysis
    
    def interpret_chart(self, image: np.ndarray, chart_type: str, 
                       extracted_data: Dict) -> ChartInterpretation:
        """ì°¨íŠ¸ í•´ì„"""
        prompt = f"""
        ë‹¤ìŒ {chart_type} ì°¨íŠ¸ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:
        
        1. ì°¨íŠ¸ì˜ ì£¼ìš” íŒ¨í„´ê³¼ íŠ¸ë Œë“œ
        2. ì¤‘ìš”í•œ ë°ì´í„° í¬ì¸íŠ¸
        3. ì—°êµ¬ì  ì˜ë¯¸ì™€ ì¸ì‚¬ì´íŠ¸
        4. ê´€ë ¨ ì—°êµ¬ ë¶„ì•¼ ì œì•ˆ
        
        ì¶”ì¶œëœ ë°ì´í„°: {extracted_data}
        """
        
        interpretation = self.gpt4v.analyze_image_with_prompt(image, prompt)
        
        return ChartInterpretation(
            patterns=interpretation.patterns,
            trends=interpretation.trends,
            key_insights=interpretation.insights,
            research_implications=interpretation.implications,
            related_fields=interpretation.related_fields
        )
    
    def interpret_table(self, image: np.ndarray, structure: TableStructure,
                       cell_contents: Dict) -> TableInterpretation:
        """í…Œì´ë¸” í•´ì„"""
        prompt = f"""
        ë‹¤ìŒ í…Œì´ë¸”ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
        
        1. í…Œì´ë¸”ì˜ êµ¬ì¡°ì™€ ë°ì´í„° ìœ í˜•
        2. ì£¼ìš” í†µê³„ ë° ìˆ˜ì¹˜
        3. ë°ì´í„° ê°„ì˜ ê´€ê³„ì™€ íŒ¨í„´
        4. ì—°êµ¬ì  ì˜ë¯¸ì™€ í™œìš© ë°©ì•ˆ
        
        í…Œì´ë¸” êµ¬ì¡°: {structure}
        ì…€ ë‚´ìš©: {cell_contents}
        """
        
        interpretation = self.gpt4v.analyze_image_with_prompt(image, prompt)
        
        return TableInterpretation(
            data_summary=interpretation.summary,
            statistics=interpretation.statistics,
            relationships=interpretation.relationships,
            research_applications=interpretation.applications
        )
```

#### 1.10 PPT ë©€í‹°ëª¨ë‹¬ ì²­í‚¹ ì „ëµ

```python
class PPTMultimodalChunking:
    """PPT ë©€í‹°ëª¨ë‹¬ ì²­í‚¹ ì‹œìŠ¤í…œ"""
    
    def __init__(self, multimodal_processor: PPTMultimodalProcessor):
        self.processor = multimodal_processor
        self.chunk_optimizer = ChunkOptimizer()
    
    def create_multimodal_chunks(self, ppt_analysis: PPTMultimodalAnalysis) -> List[MultimodalChunk]:
        """ë©€í‹°ëª¨ë‹¬ ì²­í¬ ìƒì„±"""
        chunks = []
        
        # 1. ìŠ¬ë¼ì´ë“œë³„ í†µí•© ì²­í¬
        for slide_num, slide_analysis in ppt_analysis.slide_analyses.items():
            slide_chunk = self._create_slide_integrated_chunk(slide_analysis)
            chunks.append(slide_chunk)
        
        # 2. ì‹œê°ì  ìš”ì†Œë³„ íŠ¹í™” ì²­í¬
        visual_chunks = self._create_visual_element_chunks(ppt_analysis)
        chunks.extend(visual_chunks)
        
        # 3. ì£¼ì œë³„ ê·¸ë£¹ ì²­í¬
        topic_chunks = self._create_topic_group_chunks(ppt_analysis)
        chunks.extend(topic_chunks)
        
        # 4. ë°ì´í„° ì¤‘ì‹¬ ì²­í¬
        data_chunks = self._create_data_centric_chunks(ppt_analysis)
        chunks.extend(data_chunks)
        
        # 5. ì²­í¬ ìµœì í™”
        optimized_chunks = self.chunk_optimizer.optimize_chunks(chunks)
        
        return optimized_chunks
    
    def _create_slide_integrated_chunk(self, slide_analysis: SlideMultimodalAnalysis) -> MultimodalChunk:
        """ìŠ¬ë¼ì´ë“œ í†µí•© ì²­í¬ ìƒì„±"""
        content_parts = []
        
        # í…ìŠ¤íŠ¸ ìš”ì†Œ ì¶”ê°€
        for text_elem in slide_analysis.text_elements:
            content_parts.append(f"[í…ìŠ¤íŠ¸] {text_elem.content}")
        
        # ì°¨íŠ¸ í•´ì„ ì¶”ê°€
        for chart in slide_analysis.charts:
            content_parts.append(f"[ì°¨íŠ¸-{chart.chart_type}] {chart.interpretation.summary}")
            content_parts.append(f"[ì°¨íŠ¸ ë°ì´í„°] {chart.data}")
        
        # í…Œì´ë¸” í•´ì„ ì¶”ê°€
        for table in slide_analysis.tables:
            content_parts.append(f"[í…Œì´ë¸”] {table.interpretation.data_summary}")
            content_parts.append(f"[í…Œì´ë¸” êµ¬ì¡°] {table.structure}")
        
        # ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ê²°ê³¼ ì¶”ê°€
        multimodal_content = slide_analysis.multimodal_content
        content_parts.append(f"[ì¢…í•© ë¶„ì„] {multimodal_content.slide_summary}")
        content_parts.append(f"[í•µì‹¬ í¬ì¸íŠ¸] {', '.join(multimodal_content.key_points)}")
        
        chunk_content = "\n\n".join(content_parts)
        
        return MultimodalChunk(
            content=chunk_content,
            chunk_type="slide_integrated",
            metadata={
                "slide_number": slide_analysis.slide_number,
                "has_charts": len(slide_analysis.charts) > 0,
                "has_tables": len(slide_analysis.tables) > 0,
                "visual_complexity": self._calculate_visual_complexity(slide_analysis),
                "research_relevance": multimodal_content.research_relevance,
                "confidence_score": multimodal_content.confidence_scores.overall
            },
            weight=2.0  # í†µí•© ì²­í¬ëŠ” ë†’ì€ ê°€ì¤‘ì¹˜
        )
    
    def _create_visual_element_chunks(self, ppt_analysis: PPTMultimodalAnalysis) -> List[MultimodalChunk]:
        """ì‹œê°ì  ìš”ì†Œë³„ íŠ¹í™” ì²­í¬ ìƒì„±"""
        chunks = []
        
        # ì°¨íŠ¸ë³„ ì²­í¬
        for slide_num, slide_analysis in ppt_analysis.slide_analyses.items():
            for chart in slide_analysis.charts:
                chart_chunk = MultimodalChunk(
                    content=f"ì°¨íŠ¸ ìœ í˜•: {chart.chart_type}\n"
                           f"ë°ì´í„°: {chart.data}\n"
                           f"íŒ¨í„´: {chart.interpretation.patterns}\n"
                           f"íŠ¸ë Œë“œ: {chart.interpretation.trends}\n"
                           f"ì—°êµ¬ì  ì˜ë¯¸: {chart.interpretation.research_implications}",
                    chunk_type="chart_specific",
                    metadata={
                        "slide_number": slide_num,
                        "chart_type": chart.chart_type,
                        "data_points": len(chart.data.get('values', [])),
                        "research_relevance": chart.interpretation.research_implications
                    },
                    weight=1.8
                )
                chunks.append(chart_chunk)
        
        # í…Œì´ë¸”ë³„ ì²­í¬
        for slide_num, slide_analysis in ppt_analysis.slide_analyses.items():
            for table in slide_analysis.tables:
                table_chunk = MultimodalChunk(
                    content=f"í…Œì´ë¸” êµ¬ì¡°: {table.structure}\n"
                           f"ë°ì´í„° ìš”ì•½: {table.interpretation.data_summary}\n"
                           f"í†µê³„: {table.interpretation.statistics}\n"
                           f"ê´€ê³„: {table.interpretation.relationships}\n"
                           f"ì—°êµ¬ ì‘ìš©: {table.interpretation.research_applications}",
                    chunk_type="table_specific",
                    metadata={
                        "slide_number": slide_num,
                        "table_size": f"{table.structure.rows}x{table.structure.columns}",
                        "data_types": table.structure.data_types,
                        "research_applications": table.interpretation.research_applications
                    },
                    weight=1.7
                )
                chunks.append(table_chunk)
        
        return chunks
```

#### 1.11 í•™ìˆ  ë…¼ë¬¸ íŠ¹í™” ì²˜ë¦¬
```python
class AcademicPaperProcessor:
    """í•™ìˆ  ë…¼ë¬¸ ì „ìš© ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.citation_extractor = CitationExtractor()
        self.metadata_extractor = AcademicMetadataExtractor()
        self.structure_analyzer = PaperStructureAnalyzer()
    
    def process_academic_paper(self, pdf_path: str) -> List[AcademicChunk]:
        """í•™ìˆ  ë…¼ë¬¸ì„ ì—°êµ¬ êµ¬ì¡°ì— ë§ê²Œ ì²˜ë¦¬"""
        # 1. ë…¼ë¬¸ êµ¬ì¡° ë¶„ì„ (Abstract, Introduction, Methods, Results, Discussion)
        structure = self.structure_analyzer.analyze_paper_structure(pdf_path)
        
        # 2. ì¸ìš© ì •ë³´ ì¶”ì¶œ
        citations = self.citation_extractor.extract_citations(pdf_path)
        
        # 3. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì €ì, ë°œí–‰ë…„ë„, ì €ë„, DOI, í‚¤ì›Œë“œ)
        metadata = self.metadata_extractor.extract_metadata(pdf_path)
        
        # 4. ì„¹ì…˜ë³„ íŠ¹í™” ì²­í‚¹
        chunks = self._create_academic_chunks(structure, citations, metadata)
        
        return chunks
    
    def _create_academic_chunks(self, structure, citations, metadata):
        """í•™ìˆ  ë…¼ë¬¸ íŠ¹í™” ì²­í¬ ìƒì„±"""
        chunks = []
        
        # Abstract ì²­í¬ (ê°€ì¤‘ì¹˜ 3.0)
        if structure.abstract:
            chunks.append(AcademicChunk(
                content=structure.abstract,
                chunk_type="abstract",
                weight=3.0,
                metadata={
                    **metadata,
                    "section": "abstract",
                    "has_citations": bool(citations),
                    "research_field": metadata.get("field", "unknown")
                }
            ))
        
        # Methods ì²­í¬ (ê°€ì¤‘ì¹˜ 2.5)
        if structure.methods:
            chunks.append(AcademicChunk(
                content=structure.methods,
                chunk_type="methods",
                weight=2.5,
                metadata={
                    **metadata,
                    "section": "methods",
                    "methodology_type": self._classify_methodology(structure.methods)
                }
            ))
        
        # Results ì²­í¬ (ê°€ì¤‘ì¹˜ 2.0)
        if structure.results:
            chunks.append(AcademicChunk(
                content=structure.results,
                chunk_type="results",
                weight=2.0,
                metadata={
                    **metadata,
                    "section": "results",
                    "has_figures": self._extract_figure_references(structure.results),
                    "has_tables": self._extract_table_references(structure.results)
                }
            ))
        
        return chunks
```

#### 1.2 ì—°êµ¬ ë©”íƒ€ë°ì´í„° ì‹œìŠ¤í…œ
```python
class ResearchMetadataSystem:
    """ì—°êµ¬ ë©”íƒ€ë°ì´í„° í†µí•© ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.metadata_schema = {
            "paper_id": str,
            "title": str,
            "authors": List[str],
            "publication_year": int,
            "journal": str,
            "doi": str,
            "keywords": List[str],
            "research_field": str,
            "methodology": str,
            "citation_count": int,
            "impact_factor": float,
            "funding_source": str,
            "language": str,
            "access_level": str  # open_access, subscription, etc.
        }
    
    def enrich_chunk_metadata(self, chunk: Chunk, paper_metadata: Dict) -> Chunk:
        """ì²­í¬ì— ì—°êµ¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€"""
        enriched_metadata = {
            **chunk.metadata,
            **paper_metadata,
            "research_relevance_score": self._calculate_research_relevance(chunk, paper_metadata),
            "temporal_relevance": self._calculate_temporal_relevance(paper_metadata),
            "authority_score": self._calculate_authority_score(paper_metadata)
        }
        
        chunk.metadata = enriched_metadata
        return chunk
    
    def _calculate_research_relevance(self, chunk: Chunk, metadata: Dict) -> float:
        """ì—°êµ¬ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        score = 1.0
        
        # ìµœì‹ ì„± ê°€ì¤‘ì¹˜ (ìµœê·¼ 5ë…„ ë‚´ ë°œí–‰)
        current_year = datetime.now().year
        if metadata.get("publication_year"):
            years_ago = current_year - metadata["publication_year"]
            if years_ago <= 5:
                score += 0.5
            elif years_ago <= 10:
                score += 0.3
        
        # ì €ë„ ì˜í–¥ë ¥ ê°€ì¤‘ì¹˜
        if metadata.get("impact_factor"):
            if metadata["impact_factor"] > 5.0:
                score += 0.4
            elif metadata["impact_factor"] > 2.0:
                score += 0.2
        
        # ì¸ìš©ìˆ˜ ê°€ì¤‘ì¹˜
        if metadata.get("citation_count"):
            if metadata["citation_count"] > 100:
                score += 0.3
            elif metadata["citation_count"] > 50:
                score += 0.2
        
        return min(score, 3.0)  # ìµœëŒ€ 3.0
```

### 2. ê³ ê¸‰ ê²€ìƒ‰ ë° ì¬ìˆœìœ„ ë§¤ê¹€ ì‹œìŠ¤í…œ

#### 2.1 ì—°êµ¬ íŠ¹í™” ê²€ìƒ‰ ì „ëµ
```python
class ResearchAwareSearch:
    """ì—°êµ¬ ë„ë©”ì¸ íŠ¹í™” ê²€ìƒ‰ ì‹œìŠ¤í…œ"""
    
    def __init__(self, vectorstore, reranker):
        self.vectorstore = vectorstore
        self.reranker = reranker
        self.research_query_expander = ResearchQueryExpander()
        self.temporal_filter = TemporalFilter()
        self.authority_ranker = AuthorityRanker()
    
    def search_research_documents(self, query: str, filters: Dict = None) -> List[Document]:
        """ì—°êµ¬ ë¬¸ì„œ íŠ¹í™” ê²€ìƒ‰"""
        # 1. ì—°êµ¬ ë„ë©”ì¸ ì¿¼ë¦¬ í™•ì¥
        expanded_queries = self.research_query_expander.expand_query(query)
        
        # 2. ë‹¤ì¤‘ ê²€ìƒ‰ ì „ëµ ì ìš©
        all_results = []
        for expanded_query in expanded_queries:
            # ë²¡í„° ê²€ìƒ‰
            vector_results = self.vectorstore.similarity_search_with_score(
                expanded_query, k=50
            )
            
            # í‚¤ì›Œë“œ ê²€ìƒ‰ (ì €ìëª…, ì €ë„ëª…, í‚¤ì›Œë“œ)
            keyword_results = self._keyword_search(expanded_query)
            
            # í•˜ì´ë¸Œë¦¬ë“œ ê²°í•©
            hybrid_results = self._combine_search_results(vector_results, keyword_results)
            all_results.extend(hybrid_results)
        
        # 3. ì¤‘ë³µ ì œê±° ë° í•„í„°ë§
        unique_results = self._deduplicate_results(all_results)
        filtered_results = self._apply_filters(unique_results, filters)
        
        # 4. ì—°êµ¬ ê´€ë ¨ì„± ì¬ìˆœìœ„ ë§¤ê¹€
        reranked_results = self._research_aware_rerank(query, filtered_results)
        
        return reranked_results[:20]  # ìƒìœ„ 20ê°œ ë°˜í™˜
    
    def _research_aware_rerank(self, query: str, documents: List[Document]) -> List[Document]:
        """ì—°êµ¬ íŠ¹í™” ì¬ìˆœìœ„ ë§¤ê¹€"""
        scored_docs = []
        
        for doc in documents:
            score = 0.0
            
            # 1. ê¸°ë³¸ ê´€ë ¨ì„± ì ìˆ˜
            relevance_score = self.reranker.score(query, doc.page_content)
            score += relevance_score * 0.4
            
            # 2. ì—°êµ¬ ë©”íƒ€ë°ì´í„° ì ìˆ˜
            metadata_score = self._calculate_metadata_score(doc.metadata)
            score += metadata_score * 0.3
            
            # 3. ì„¹ì…˜ë³„ ê°€ì¤‘ì¹˜
            section_weight = self._get_section_weight(doc.metadata.get("section", "unknown"))
            score += section_weight * 0.2
            
            # 4. ìµœì‹ ì„± ì ìˆ˜
            recency_score = self._calculate_recency_score(doc.metadata)
            score += recency_score * 0.1
            
            scored_docs.append((doc, score))
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        return [doc for doc, score in scored_docs]
```

#### 2.2 ë‹¤êµ­ì–´ ì—°êµ¬ ì§€ì›
```python
class MultilingualResearchSupport:
    """ë‹¤êµ­ì–´ ì—°êµ¬ ë¬¸ì„œ ì§€ì› ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.language_detector = LanguageDetector()
        self.translator = ResearchTranslator()
        self.multilingual_embedder = MultilingualEmbedder()
    
    def process_multilingual_query(self, query: str) -> List[str]:
        """ë‹¤êµ­ì–´ ì¿¼ë¦¬ ì²˜ë¦¬"""
        # 1. ì–¸ì–´ ê°ì§€
        detected_language = self.language_detector.detect(query)
        
        # 2. ì£¼ìš” ì–¸ì–´ë¡œ ë²ˆì—­ (ì˜ì–´, í•œêµ­ì–´, ì¤‘êµ­ì–´, ì¼ë³¸ì–´)
        target_languages = ["en", "ko", "zh", "ja"]
        translated_queries = []
        
        for target_lang in target_languages:
            if target_lang != detected_language:
                translated = self.translator.translate(query, detected_language, target_lang)
                translated_queries.append(translated)
        
        # 3. ì›ë³¸ê³¼ ë²ˆì—­ë³¸ ëª¨ë‘ í¬í•¨
        all_queries = [query] + translated_queries
        
        return all_queries
    
    def create_multilingual_embeddings(self, text: str) -> np.ndarray:
        """ë‹¤êµ­ì–´ ì„ë² ë”© ìƒì„±"""
        return self.multilingual_embedder.embed(text)
```

### 3. ì—°êµ¬ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹œìŠ¤í…œ

#### 3.1 ì—°êµ¬ ê°„ ì—°ê´€ì„± ë¶„ì„
```python
class ResearchInsightGenerator:
    """ì—°êµ¬ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹œìŠ¤í…œ"""
    
    def __init__(self, llm_service, knowledge_graph):
        self.llm_service = llm_service
        self.knowledge_graph = knowledge_graph
        self.pattern_analyzer = ResearchPatternAnalyzer()
        self.trend_analyzer = ResearchTrendAnalyzer()
    
    def generate_research_insights(self, query: str, retrieved_docs: List[Document]) -> Dict:
        """ì—°êµ¬ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = {
            "summary": "",
            "key_findings": [],
            "research_gaps": [],
            "future_directions": [],
            "related_works": [],
            "methodology_trends": [],
            "citation_network": {}
        }
        
        # 1. í•µì‹¬ ë°œê²¬ì‚¬í•­ ì¶”ì¶œ
        insights["key_findings"] = self._extract_key_findings(retrieved_docs)
        
        # 2. ì—°êµ¬ ê°„ ì—°ê´€ì„± ë¶„ì„
        insights["related_works"] = self._analyze_related_works(retrieved_docs)
        
        # 3. ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„
        insights["methodology_trends"] = self._analyze_methodology_trends(retrieved_docs)
        
        # 4. ì—°êµ¬ ê³µë°± ì‹ë³„
        insights["research_gaps"] = self._identify_research_gaps(query, retrieved_docs)
        
        # 5. í–¥í›„ ì—°êµ¬ ë°©í–¥ ì œì‹œ
        insights["future_directions"] = self._suggest_future_directions(insights)
        
        # 6. ì¢…í•© ìš”ì•½ ìƒì„±
        insights["summary"] = self._generate_comprehensive_summary(insights)
        
        return insights
    
    def _extract_key_findings(self, docs: List[Document]) -> List[str]:
        """í•µì‹¬ ë°œê²¬ì‚¬í•­ ì¶”ì¶œ"""
        findings = []
        
        for doc in docs:
            if doc.metadata.get("section") == "results":
                # ê²°ê³¼ ì„¹ì…˜ì—ì„œ í•µì‹¬ ë°œê²¬ì‚¬í•­ ì¶”ì¶œ
                finding = self.llm_service.extract_findings(doc.page_content)
                if finding:
                    findings.append(finding)
        
        return findings
    
    def _analyze_related_works(self, docs: List[Document]) -> List[Dict]:
        """ê´€ë ¨ ì—°êµ¬ ë¶„ì„"""
        related_works = []
        
        for doc in docs:
            if doc.metadata.get("section") == "introduction":
                # ì„œë¡ ì—ì„œ ê´€ë ¨ ì—°êµ¬ ì¶”ì¶œ
                related = self.llm_service.extract_related_works(doc.page_content)
                related_works.extend(related)
        
        return related_works
```

#### 3.2 ì—°êµ¬ íŒ¨í„´ ë° íŠ¸ë Œë“œ ë¶„ì„
```python
class ResearchPatternAnalyzer:
    """ì—°êµ¬ íŒ¨í„´ ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.methodology_classifier = MethodologyClassifier()
        self.trend_detector = TrendDetector()
        self.citation_analyzer = CitationAnalyzer()
    
    def analyze_research_patterns(self, documents: List[Document]) -> Dict:
        """ì—°êµ¬ íŒ¨í„´ ë¶„ì„"""
        patterns = {
            "methodology_distribution": {},
            "temporal_trends": {},
            "author_collaboration": {},
            "journal_distribution": {},
            "keyword_evolution": {}
        }
        
        # 1. ë°©ë²•ë¡  ë¶„í¬ ë¶„ì„
        patterns["methodology_distribution"] = self._analyze_methodology_distribution(documents)
        
        # 2. ì‹œê°„ì  íŠ¸ë Œë“œ ë¶„ì„
        patterns["temporal_trends"] = self._analyze_temporal_trends(documents)
        
        # 3. ì €ì í˜‘ì—… íŒ¨í„´ ë¶„ì„
        patterns["author_collaboration"] = self._analyze_author_collaboration(documents)
        
        # 4. ì €ë„ ë¶„í¬ ë¶„ì„
        patterns["journal_distribution"] = self._analyze_journal_distribution(documents)
        
        # 5. í‚¤ì›Œë“œ ì§„í™” ë¶„ì„
        patterns["keyword_evolution"] = self._analyze_keyword_evolution(documents)
        
        return patterns
```

### 4. ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ë° ë™ê¸°í™” ì‹œìŠ¤í…œ

#### 4.1 ì—°êµ¬ ë°ì´í„°ë² ì´ìŠ¤ ë™ê¸°í™”
```python
class ResearchDatabaseSync:
    """ì—°êµ¬ ë°ì´í„°ë² ì´ìŠ¤ ì‹¤ì‹œê°„ ë™ê¸°í™”"""
    
    def __init__(self, vectorstore, external_apis):
        self.vectorstore = vectorstore
        self.external_apis = external_apis  # arXiv, PubMed, Google Scholar ë“±
        self.update_scheduler = UpdateScheduler()
        self.change_detector = ChangeDetector()
    
    def schedule_periodic_updates(self):
        """ì£¼ê¸°ì  ì—…ë°ì´íŠ¸ ìŠ¤ì¼€ì¤„ë§"""
        # ë§¤ì¼ ìƒˆë²½ 2ì‹œì— ìµœì‹  ë…¼ë¬¸ ì—…ë°ì´íŠ¸
        self.update_scheduler.schedule_daily_update(
            time="02:00",
            task=self._update_latest_papers
        )
        
        # ì£¼ê°„ ì¸ìš©ìˆ˜ ì—…ë°ì´íŠ¸
        self.update_scheduler.schedule_weekly_update(
            day="sunday",
            time="03:00",
            task=self._update_citation_counts
        )
    
    def _update_latest_papers(self):
        """ìµœì‹  ë…¼ë¬¸ ì—…ë°ì´íŠ¸"""
        for api in self.external_apis:
            try:
                # ìµœì‹  ë…¼ë¬¸ ê²€ìƒ‰ (ì§€ë‚œ 24ì‹œê°„)
                new_papers = api.get_recent_papers(hours=24)
                
                for paper in new_papers:
                    # ì¤‘ë³µ í™•ì¸
                    if not self._is_duplicate(paper):
                        # ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„°ìŠ¤í† ì–´ ì¶”ê°€
                        processed_docs = self._process_new_paper(paper)
                        self.vectorstore.add_documents(processed_docs)
                        
            except Exception as e:
                print(f"API {api.name} ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _update_citation_counts(self):
        """ì¸ìš©ìˆ˜ ì—…ë°ì´íŠ¸"""
        # ê¸°ì¡´ ë…¼ë¬¸ë“¤ì˜ ì¸ìš©ìˆ˜ ì—…ë°ì´íŠ¸
        existing_papers = self.vectorstore.get_all_documents()
        
        for doc in existing_papers:
            if doc.metadata.get("doi"):
                try:
                    # ì™¸ë¶€ APIì—ì„œ ìµœì‹  ì¸ìš©ìˆ˜ ì¡°íšŒ
                    citation_count = self.external_apis.get_citation_count(doc.metadata["doi"])
                    
                    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                    doc.metadata["citation_count"] = citation_count
                    self.vectorstore.update_document_metadata(doc.id, doc.metadata)
                    
                except Exception as e:
                    print(f"ì¸ìš©ìˆ˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ ({doc.metadata.get('doi')}): {e}")
```

### 5. ì—°êµ¬ì ë§ì¶¤í˜• ì¸í„°í˜ì´ìŠ¤

#### 5.1 ì—°êµ¬ í”„ë¡œí•„ ê¸°ë°˜ ê°œì¸í™”
```python
class ResearcherProfileSystem:
    """ì—°êµ¬ì í”„ë¡œí•„ ê¸°ë°˜ ê°œì¸í™” ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.profile_manager = ProfileManager()
        self.preference_learner = PreferenceLearner()
        self.recommendation_engine = RecommendationEngine()
    
    def create_researcher_profile(self, researcher_id: str, initial_preferences: Dict) -> ResearcherProfile:
        """ì—°êµ¬ì í”„ë¡œí•„ ìƒì„±"""
        profile = ResearcherProfile(
            researcher_id=researcher_id,
            research_fields=initial_preferences.get("fields", []),
            preferred_languages=initial_preferences.get("languages", ["ko", "en"]),
            experience_level=initial_preferences.get("level", "intermediate"),
            preferred_methodologies=initial_preferences.get("methodologies", []),
            citation_preferences=initial_preferences.get("citation_prefs", "recent"),
            update_frequency=initial_preferences.get("update_freq", "daily")
        )
        
        return profile
    
    def personalize_search_results(self, query: str, profile: ResearcherProfile, 
                                 raw_results: List[Document]) -> List[Document]:
        """ì—°êµ¬ì í”„ë¡œí•„ ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼ ê°œì¸í™”"""
        personalized_results = []
        
        for doc in raw_results:
            # í”„ë¡œí•„ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
            personalization_score = self._calculate_personalization_score(doc, profile)
            
            # ì›ë³¸ ì ìˆ˜ì™€ ê°œì¸í™” ì ìˆ˜ ê²°í•©
            final_score = doc.metadata.get("relevance_score", 0.5) * 0.7 + personalization_score * 0.3
            
            doc.metadata["personalized_score"] = final_score
            personalized_results.append(doc)
        
        # ê°œì¸í™” ì ìˆ˜ìˆœ ì •ë ¬
        personalized_results.sort(key=lambda x: x.metadata["personalized_score"], reverse=True)
        
        return personalized_results
    
    def _calculate_personalization_score(self, doc: Document, profile: ResearcherProfile) -> float:
        """ê°œì¸í™” ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # ì—°êµ¬ ë¶„ì•¼ ì¼ì¹˜ë„
        doc_field = doc.metadata.get("research_field", "")
        if doc_field in profile.research_fields:
            score += 0.4
        
        # ì„ í˜¸ ì–¸ì–´ ì¼ì¹˜ë„
        doc_language = doc.metadata.get("language", "en")
        if doc_language in profile.preferred_languages:
            score += 0.2
        
        # ê²½í—˜ ìˆ˜ì¤€ ì í•©ì„±
        complexity_score = self._assess_document_complexity(doc)
        if profile.experience_level == "beginner" and complexity_score < 0.3:
            score += 0.2
        elif profile.experience_level == "expert" and complexity_score > 0.7:
            score += 0.2
        
        # ë°©ë²•ë¡  ì„ í˜¸ë„
        doc_methodology = doc.metadata.get("methodology", "")
        if doc_methodology in profile.preferred_methodologies:
            score += 0.2
        
        return min(score, 1.0)
```

---

## ğŸ“Š ì„±ëŠ¥ ì§€í‘œ ë° í‰ê°€ ì²´ê³„

### 1. ê²€ìƒ‰ ì„±ëŠ¥ ì§€í‘œ
```python
class ResearchRAGMetrics:
    """ì—°êµ¬ RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ ì§€í‘œ"""
    
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.evaluator = ResearchEvaluator()
    
    def evaluate_search_performance(self, query: str, retrieved_docs: List[Document], 
                                  ground_truth: List[str]) -> Dict:
        """ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€"""
        metrics = {
            "precision_at_k": {},
            "recall_at_k": {},
            "ndcg_at_k": {},
            "research_relevance": 0.0,
            "temporal_relevance": 0.0,
            "authority_score": 0.0
        }
        
        # K=5, 10, 20ì—ì„œ ì •ë°€ë„/ì¬í˜„ìœ¨ ê³„ì‚°
        for k in [5, 10, 20]:
            metrics[f"precision_at_{k}"] = self._calculate_precision_at_k(
                retrieved_docs[:k], ground_truth
            )
            metrics[f"recall_at_{k}"] = self._calculate_recall_at_k(
                retrieved_docs[:k], ground_truth
            )
            metrics[f"ndcg_at_{k}"] = self._calculate_ndcg_at_k(
                retrieved_docs[:k], ground_truth
            )
        
        # ì—°êµ¬ íŠ¹í™” ì§€í‘œ
        metrics["research_relevance"] = self._evaluate_research_relevance(retrieved_docs)
        metrics["temporal_relevance"] = self._evaluate_temporal_relevance(retrieved_docs)
        metrics["authority_score"] = self._evaluate_authority_score(retrieved_docs)
        
        return metrics
    
    def evaluate_response_quality(self, query: str, response: str, 
                                retrieved_docs: List[Document]) -> Dict:
        """ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
        quality_metrics = {
            "factual_accuracy": 0.0,
            "completeness": 0.0,
            "research_depth": 0.0,
            "citation_quality": 0.0,
            "clarity": 0.0
        }
        
        # ì‚¬ì‹¤ì  ì •í™•ì„± í‰ê°€
        quality_metrics["factual_accuracy"] = self._evaluate_factual_accuracy(
            response, retrieved_docs
        )
        
        # ì™„ì„±ë„ í‰ê°€
        quality_metrics["completeness"] = self._evaluate_completeness(
            query, response
        )
        
        # ì—°êµ¬ ê¹Šì´ í‰ê°€
        quality_metrics["research_depth"] = self._evaluate_research_depth(
            response, retrieved_docs
        )
        
        # ì¸ìš© í’ˆì§ˆ í‰ê°€
        quality_metrics["citation_quality"] = self._evaluate_citation_quality(
            response, retrieved_docs
        )
        
        # ëª…í™•ì„± í‰ê°€
        quality_metrics["clarity"] = self._evaluate_clarity(response)
        
        return quality_metrics
```

### 2. ì‚¬ìš©ì ë§Œì¡±ë„ í‰ê°€
```python
class UserSatisfactionEvaluator:
    """ì‚¬ìš©ì ë§Œì¡±ë„ í‰ê°€ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.feedback_collector = FeedbackCollector()
        self.satisfaction_analyzer = SatisfactionAnalyzer()
    
    def collect_user_feedback(self, session_id: str, query: str, response: str, 
                            sources: List[Dict]) -> UserFeedback:
        """ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘"""
        feedback = UserFeedback(
            session_id=session_id,
            query=query,
            response=response,
            sources=sources,
            timestamp=datetime.now(),
            ratings={},
            comments=""
        )
        
        return feedback
    
    def analyze_satisfaction_trends(self, feedback_data: List[UserFeedback]) -> Dict:
        """ë§Œì¡±ë„ íŠ¸ë Œë“œ ë¶„ì„"""
        trends = {
            "overall_satisfaction": 0.0,
            "search_accuracy_trend": [],
            "response_quality_trend": [],
            "source_relevance_trend": [],
            "common_complaints": [],
            "improvement_suggestions": []
        }
        
        # ì „ì²´ ë§Œì¡±ë„ ê³„ì‚°
        trends["overall_satisfaction"] = self._calculate_overall_satisfaction(feedback_data)
        
        # ì‹œê°„ë³„ íŠ¸ë Œë“œ ë¶„ì„
        trends["search_accuracy_trend"] = self._analyze_search_accuracy_trend(feedback_data)
        trends["response_quality_trend"] = self._analyze_response_quality_trend(feedback_data)
        trends["source_relevance_trend"] = self._analyze_source_relevance_trend(feedback_data)
        
        # ê³µí†µ ë¶ˆë§Œì‚¬í•­ ë° ê°œì„  ì œì•ˆ
        trends["common_complaints"] = self._identify_common_complaints(feedback_data)
        trends["improvement_suggestions"] = self._extract_improvement_suggestions(feedback_data)
        
        return trends
```

---

## ğŸ› ï¸ êµ¬í˜„ ë¡œë“œë§µ

### Phase 1: ê¸°ë°˜ ì‹œìŠ¤í…œ êµ¬ì¶• (8ì£¼)
- [ ] Excel ìˆ˜ì‹ ë¶„ì„ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] PPT ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ ê°œë°œ
- [ ] OCR ë° ì‹œê°ì  ìš”ì†Œ ì¸ì‹ ì—”ì§„ êµ¬ì¶•
- [ ] ë©€í‹°ëª¨ë‹¬ LLM í†µí•© ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] Excel-PPT ì—°ê³„ ë¶„ì„ ì‹œìŠ¤í…œ ê°œë°œ
- [ ] í•™ìˆ  ë…¼ë¬¸ íŠ¹í™” ì²˜ë¦¬ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ì—°êµ¬ ë©”íƒ€ë°ì´í„° ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] ë‹¤êµ­ì–´ ì§€ì› ì‹œìŠ¤í…œ ê°œë°œ
- [ ] ê¸°ë³¸ ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘ ì‹œìŠ¤í…œ êµ¬ì¶•

### Phase 2: ê³ ê¸‰ ê²€ìƒ‰ ë° ë¶„ì„ ì‹œìŠ¤í…œ (10ì£¼)
- [ ] Excel ìˆ˜ì‹ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] PPT ë©€í‹°ëª¨ë‹¬ ì²­í‚¹ ì „ëµ êµ¬í˜„
- [ ] Excel ìˆ˜ì‹ ê²€ì¦ ë° í’ˆì§ˆ ê´€ë¦¬ ì‹œìŠ¤í…œ ê°œë°œ
- [ ] ì—°êµ¬ íŠ¹í™” ê²€ìƒ‰ ì „ëµ êµ¬í˜„
- [ ] ì—°êµ¬ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹œìŠ¤í…œ ê°œë°œ
- [ ] ì—°êµ¬ íŒ¨í„´ ë¶„ì„ ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ì°¨íŠ¸/í…Œì´ë¸” ìë™ í•´ì„ ì‹œìŠ¤í…œ ê°œë°œ

### Phase 3: ê°œì¸í™” ë° ìµœì í™” (8ì£¼)
- [ ] ì—°êµ¬ì í”„ë¡œí•„ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] Excel ê³„ì‚° ë¡œì§ ê°œì¸í™” ì‹œìŠ¤í…œ ê°œë°œ
- [ ] PPT ì‹œê°ì  ìš”ì†Œ ê°œì¸í™” ì‹œìŠ¤í…œ ê°œë°œ
- [ ] ê°œì¸í™” ê²€ìƒ‰ ì‹œìŠ¤í…œ ê°œë°œ
- [ ] ì„±ëŠ¥ ìµœì í™” ë° íŠœë‹
- [ ] ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ ê°œì„ 
- [ ] ë©€í‹°ëª¨ë‹¬ ì‘ë‹µ ìƒì„± ì‹œìŠ¤í…œ êµ¬í˜„

### Phase 4: í‰ê°€ ë° ê°œì„  (6ì£¼)
- [ ] Excel ìˆ˜ì‹ ì •í™•ë„ í‰ê°€ ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] PPT ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì •í™•ë„ í‰ê°€ ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] ì¢…í•© ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] ì‚¬ìš©ì ë§Œì¡±ë„ í‰ê°€ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] í”¼ë“œë°± ê¸°ë°˜ ì‹œìŠ¤í…œ ê°œì„ 
- [ ] ìµœì¢… ì„±ëŠ¥ ìµœì í™”

---

## ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ

### ê²€ìƒ‰ ì„±ëŠ¥
- **ì •í™•ë„**: 85% â†’ 95% (10%p í–¥ìƒ)
- **ì¬í˜„ìœ¨**: 70% â†’ 90% (20%p í–¥ìƒ)
- **ì—°êµ¬ ê´€ë ¨ì„±**: 60% â†’ 90% (30%p í–¥ìƒ)
- **Excel ìˆ˜ì‹ ì´í•´ë„**: 40% â†’ 85% (45%p í–¥ìƒ)
- **Excel-PPT ì—°ê´€ì„± ë¶„ì„**: 30% â†’ 80% (50%p í–¥ìƒ)
- **PPT ì‹œê°ì  ìš”ì†Œ ì¸ì‹**: 25% â†’ 85% (60%p í–¥ìƒ)
- **OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ì •í™•ë„**: 70% â†’ 95% (25%p í–¥ìƒ)
- **ì°¨íŠ¸/í…Œì´ë¸” ìë™ í•´ì„**: 20% â†’ 80% (60%p í–¥ìƒ)

### ì‘ë‹µ í’ˆì§ˆ
- **ì‚¬ì‹¤ì  ì •í™•ì„±**: 80% â†’ 95% (15%p í–¥ìƒ)
- **ì—°êµ¬ ê¹Šì´**: 65% â†’ 85% (20%p í–¥ìƒ)
- **ì¸ìš© í’ˆì§ˆ**: 70% â†’ 90% (20%p í–¥ìƒ)
- **ìˆ˜ì‹ ì„¤ëª… ì •í™•ë„**: 50% â†’ 90% (40%p í–¥ìƒ)
- **ê³„ì‚° ë¡œì§ ì´í•´ë„**: 45% â†’ 85% (40%p í–¥ìƒ)
- **ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì •í™•ë„**: 30% â†’ 85% (55%p í–¥ìƒ)
- **ì‹œê°ì  ìš”ì†Œ í•´ì„ í’ˆì§ˆ**: 25% â†’ 80% (55%p í–¥ìƒ)

### ì‚¬ìš©ì ê²½í—˜
- **ì‘ë‹µ ì‹œê°„**: 3ì´ˆ â†’ 1.5ì´ˆ (50% ë‹¨ì¶•)
- **ì‚¬ìš©ì ë§Œì¡±ë„**: 75% â†’ 90% (15%p í–¥ìƒ)
- **ì—°êµ¬ íš¨ìœ¨ì„±**: 60% â†’ 85% (25%p í–¥ìƒ)
- **Excel ì‘ì—… ì§€ì›ë„**: 30% â†’ 80% (50%p í–¥ìƒ)
- **PPT ì—°ê³„ ë¶„ì„ ì •í™•ë„**: 25% â†’ 75% (50%p í–¥ìƒ)
- **ë©€í‹°ëª¨ë‹¬ ì‘ë‹µ í’ˆì§ˆ**: 20% â†’ 80% (60%p í–¥ìƒ)
- **ì‹œê°ì  ì½˜í…ì¸  ì´í•´ë„**: 15% â†’ 75% (60%p í–¥ìƒ)

---

## ğŸ”§ ê¸°ìˆ  ìŠ¤íƒ ë° ì˜ì¡´ì„±

### í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
```python
# ë¬¸ì„œ ì²˜ë¦¬
pdfplumber>=0.9.0
python-pptx>=0.6.21
openpyxl>=3.1.0
PyMuPDF>=1.23.0
xlwings>=0.30.0  # Excel ìë™í™”
xlsxwriter>=3.1.0  # Excel íŒŒì¼ ìƒì„±
pandas>=2.0.0  # ë°ì´í„° ë¶„ì„
numpy>=1.24.0  # ìˆ˜ì¹˜ ê³„ì‚°

# Excel ìˆ˜ì‹ ë¶„ì„
formula-parser>=1.0.0  # ìˆ˜ì‹ íŒŒì‹±
excel-formula-parser>=0.1.0  # Excel ìˆ˜ì‹ ë¶„ì„
sympy>=1.12.0  # ìˆ˜í•™ì  ìˆ˜ì‹ ì²˜ë¦¬

# OCR ë° ì´ë¯¸ì§€ ì²˜ë¦¬
tesseract>=0.1.3  # OCR ì—”ì§„
paddleocr>=2.7.0  # PaddleOCR
opencv-python>=4.8.0  # ì»´í“¨í„° ë¹„ì „
pillow>=10.0.0  # ì´ë¯¸ì§€ ì²˜ë¦¬
easyocr>=1.7.0  # EasyOCR
pytesseract>=0.3.10  # Tesseract Python ë˜í¼

# ë©€í‹°ëª¨ë‹¬ LLM
openai>=1.0.0  # GPT-4V
anthropic>=0.3.0  # Claude 3 Vision
google-generativeai>=0.3.0  # Gemini Pro Vision
transformers>=4.30.0  # Hugging Face ëª¨ë¸ë“¤
torch>=2.0.0  # PyTorch
torchvision>=0.15.0  # ì»´í“¨í„° ë¹„ì „ ëª¨ë¸

# ì°¨íŠ¸/í…Œì´ë¸” ì¸ì‹
ultralytics>=8.0.0  # YOLO ëª¨ë¸
detectron2>=0.6.0  # ê°ì²´ ê°ì§€
layoutparser>=0.3.4  # ë ˆì´ì•„ì›ƒ ë¶„ì„
table-transformer>=0.1.0  # í…Œì´ë¸” ê°ì§€

# ìì—°ì–´ ì²˜ë¦¬
sentence-transformers>=2.2.0
spacy>=3.6.0
langchain>=0.1.0

# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
chromadb>=0.4.0
faiss-cpu>=1.7.4

# ì—°êµ¬ ë°ì´í„° API
requests>=2.31.0
arxiv>=2.0.0
scholarly>=1.7.0

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
prometheus-client>=0.17.0
grafana-api>=1.0.3

# Excel íŠ¹í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
xlrd>=2.0.1  # Excel íŒŒì¼ ì½ê¸°
xlwt>=1.3.0  # Excel íŒŒì¼ ì“°ê¸°
pyexcel>=0.7.0  # Excel íŒŒì¼ ì²˜ë¦¬
formulaic>=0.6.0  # ìˆ˜ì‹ ì²˜ë¦¬

# PPT íŠ¹í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
python-pptx>=0.6.21  # PPT íŒŒì¼ ì²˜ë¦¬
aspose-slides>=23.0.0  # ê³ ê¸‰ PPT ì²˜ë¦¬
pptx2pdf>=0.1.0  # PPT to PDF ë³€í™˜
```

### ì™¸ë¶€ ì„œë¹„ìŠ¤ ì—°ë™
- **arXiv API**: ìµœì‹  ë…¼ë¬¸ ìë™ ìˆ˜ì§‘
- **PubMed API**: ì˜í•™/ìƒëª…ê³¼í•™ ë…¼ë¬¸ ìˆ˜ì§‘
- **Google Scholar API**: ì¸ìš©ìˆ˜ ë° ì˜í–¥ë ¥ ì§€í‘œ
- **CrossRef API**: DOI ê¸°ë°˜ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
- **ORCID API**: ì €ì ì •ë³´ ë° ì—°êµ¬ í”„ë¡œí•„

---

## ğŸ“š ì°¸ê³  ë¬¸í—Œ ë° ì—°êµ¬

### ìµœì‹  ì—°êµ¬ ë™í–¥
1. **SceneRAG**: ì¥ë©´ ìˆ˜ì¤€ ê²€ìƒ‰ ì¦ê°• ìƒì„± (2024)
2. **Small-to-Large RAG**: ê³„ì¸µì  ê²€ìƒ‰ ì•„í‚¤í…ì²˜
3. **HyDE**: ê°€ìƒ ë¬¸ì„œ ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰
4. **Multi-Query RAG**: ë‹¤ì¤‘ ì¿¼ë¦¬ ì „ëµ

### ìƒìš© ì„œë¹„ìŠ¤ ë²¤ì¹˜ë§ˆí‚¹
1. **Perplexity AI**: ì—°êµ¬ ì¤‘ì‹¬ ê²€ìƒ‰ ì—”ì§„
2. **Elicit**: AI ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸
3. **Semantic Scholar**: í•™ìˆ  ê²€ìƒ‰ í”Œë«í¼
4. **Research Rabbit**: ì—°êµ¬ ë…¼ë¬¸ ì¶”ì²œ ì‹œìŠ¤í…œ

### ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸
1. **LangChain**: RAG í”„ë ˆì„ì›Œí¬
2. **LlamaIndex**: ë°ì´í„° ì—°ê²° í”„ë ˆì„ì›Œí¬
3. **Haystack**: ê²€ìƒ‰ ë° NLP íŒŒì´í”„ë¼ì¸
4. **Weaviate**: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤

---

## ğŸ“Š Excel íŠ¹í™” ê¸°ëŠ¥ ìƒì„¸

### 1. ìˆ˜ì‹ ë¶„ë¥˜ ë° ë¶„ì„

#### 1.1 ì—°êµ¬ ë„ë©”ì¸ë³„ ìˆ˜ì‹ ë¶„ë¥˜
```python
RESEARCH_FORMULA_CATEGORIES = {
    "statistical_analysis": {
        "functions": ["AVERAGE", "STDEV", "CORREL", "COVAR", "PEARSON", "RSQ"],
        "description": "í†µê³„ ë¶„ì„ ë° ìƒê´€ê´€ê³„ ë¶„ì„",
        "research_applications": ["ì‹¤í—˜ ë°ì´í„° ë¶„ì„", "ë³€ìˆ˜ ê°„ ê´€ê³„ ë¶„ì„", "ìœ ì˜ì„± ê²€ì •"]
    },
    "mathematical_modeling": {
        "functions": ["SUM", "PRODUCT", "POWER", "SQRT", "LOG", "EXP"],
        "description": "ìˆ˜í•™ì  ëª¨ë¸ë§ ë° ê³„ì‚°",
        "research_applications": ["ìˆ˜í•™ì  ëª¨ë¸ êµ¬ì¶•", "ì‹œë®¬ë ˆì´ì…˜", "ìµœì í™” ë¬¸ì œ"]
    },
    "financial_analysis": {
        "functions": ["NPV", "IRR", "PMT", "PV", "FV", "RATE"],
        "description": "ì¬ë¬´ ë¶„ì„ ë° íˆ¬ì í‰ê°€",
        "research_applications": ["ê²½ì œì„± ë¶„ì„", "íˆ¬ì ìˆ˜ìµë¥  ê³„ì‚°", "í• ì¸ìœ¨ ì ìš©"]
    },
    "scientific_calculation": {
        "functions": ["SIN", "COS", "TAN", "ATAN", "RADIANS", "DEGREES"],
        "description": "ê³¼í•™ ê³„ì‚° ë° ë¬¼ë¦¬ ìƒìˆ˜",
        "research_applications": ["ë¬¼ë¦¬ ì‹¤í—˜ ë¶„ì„", "í™”í•™ ë°˜ì‘ ê³„ì‚°", "ê³µí•™ ì„¤ê³„"]
    },
    "data_processing": {
        "functions": ["VLOOKUP", "HLOOKUP", "INDEX", "MATCH", "IF", "COUNTIF"],
        "description": "ë°ì´í„° ì²˜ë¦¬ ë° ì¡°íšŒ",
        "research_applications": ["ë°ì´í„° ì •ì œ", "í‘œì¤€í™”", "ë¶„ë¥˜ ë° ê·¸ë£¹í™”"]
    }
}
```

#### 1.2 ìˆ˜ì‹ ë³µì¡ë„ ë¶„ì„
```python
class FormulaComplexityAnalyzer:
    """ìˆ˜ì‹ ë³µì¡ë„ ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def calculate_complexity_score(self, formula: Formula) -> float:
        """ìˆ˜ì‹ ë³µì¡ë„ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)"""
        score = 0.0
        
        # 1. í•¨ìˆ˜ ê°œìˆ˜ ê°€ì¤‘ì¹˜ (30%)
        function_count = len(formula.functions)
        score += min(function_count / 10.0, 1.0) * 0.3
        
        # 2. ì¤‘ì²© ê¹Šì´ ê°€ì¤‘ì¹˜ (25%)
        nesting_depth = self._calculate_nesting_depth(formula)
        score += min(nesting_depth / 5.0, 1.0) * 0.25
        
        # 3. ì°¸ì¡° ë²”ìœ„ ê°€ì¤‘ì¹˜ (20%)
        reference_range = self._calculate_reference_range(formula)
        score += min(reference_range / 100.0, 1.0) * 0.2
        
        # 4. ì¡°ê±´ë¬¸ ë³µì¡ë„ ê°€ì¤‘ì¹˜ (15%)
        conditional_complexity = self._calculate_conditional_complexity(formula)
        score += min(conditional_complexity / 3.0, 1.0) * 0.15
        
        # 5. ë°°ì—´ ìˆ˜ì‹ ì—¬ë¶€ ê°€ì¤‘ì¹˜ (10%)
        if formula.is_array_formula:
            score += 0.1
        
        return min(score, 1.0)
    
    def _calculate_nesting_depth(self, formula: Formula) -> int:
        """ìˆ˜ì‹ ì¤‘ì²© ê¹Šì´ ê³„ì‚°"""
        max_depth = 0
        current_depth = 0
        
        for char in formula.formula_text:
            if char == '(':
                current_depth += 1
                max_depth = max(max_depth, current_depth)
            elif char == ')':
                current_depth -= 1
        
        return max_depth
```

### 2. Excel-PPT ì—°ê³„ ë¶„ì„ ìƒì„¸

#### 2.1 ë°ì´í„° ì‹œê°í™” ë§¤í•‘ ì•Œê³ ë¦¬ì¦˜
```python
class DataVisualizationMapper:
    """ë°ì´í„°ì™€ ì‹œê°í™” ë§¤í•‘ ì‹œìŠ¤í…œ"""
    
    def map_excel_to_ppt(self, excel_data: DataTable, ppt_chart: Chart) -> MappingResult:
        """Excel ë°ì´í„°ì™€ PPT ì°¨íŠ¸ ë§¤í•‘"""
        mapping_score = 0.0
        mapping_details = {}
        
        # 1. ë°ì´í„° êµ¬ì¡° ë§¤ì¹­ (40%)
        structure_score = self._match_data_structure(excel_data, ppt_chart)
        mapping_score += structure_score * 0.4
        mapping_details["structure_match"] = structure_score
        
        # 2. ìˆ˜ì¹˜ ë²”ìœ„ ë§¤ì¹­ (30%)
        value_range_score = self._match_value_ranges(excel_data, ppt_chart)
        mapping_score += value_range_score * 0.3
        mapping_details["value_range_match"] = value_range_score
        
        # 3. ë ˆì´ë¸” ìœ ì‚¬ì„± (20%)
        label_similarity_score = self._calculate_label_similarity(excel_data, ppt_chart)
        mapping_score += label_similarity_score * 0.2
        mapping_details["label_similarity"] = label_similarity_score
        
        # 4. ì°¨íŠ¸ ìœ í˜• ì í•©ì„± (10%)
        chart_type_score = self._assess_chart_type_fitness(excel_data, ppt_chart)
        mapping_score += chart_type_score * 0.1
        mapping_details["chart_type_fitness"] = chart_type_score
        
        return MappingResult(
            confidence=mapping_score,
            details=mapping_details,
            is_valid=mapping_score > 0.6
        )
    
    def _match_data_structure(self, data: DataTable, chart: Chart) -> float:
        """ë°ì´í„° êµ¬ì¡° ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        # í–‰/ì—´ ìˆ˜ ë¹„êµ
        data_rows, data_cols = data.get_dimensions()
        chart_series_count = chart.get_series_count()
        
        # êµ¬ì¡°ì  ìœ ì‚¬ì„± ê³„ì‚°
        row_similarity = 1.0 - abs(data_rows - chart_series_count) / max(data_rows, chart_series_count)
        col_similarity = 1.0 - abs(data_cols - 2) / max(data_cols, 2)  # ì¼ë°˜ì ìœ¼ë¡œ X, Y ì¶•
        
        return (row_similarity + col_similarity) / 2.0
```

### 3. Excel ìˆ˜ì‹ ê¸°ë°˜ ì—°êµ¬ ì§€ì› ê¸°ëŠ¥

#### 3.1 ìë™ ìˆ˜ì‹ ìƒì„± ì œì•ˆ
```python
class FormulaSuggestionEngine:
    """ìˆ˜ì‹ ìƒì„± ì œì•ˆ ì—”ì§„"""
    
    def suggest_formulas_for_research_task(self, task_description: str, 
                                         data_context: DataContext) -> List[FormulaSuggestion]:
        """ì—°êµ¬ ì‘ì—…ì— ëŒ€í•œ ìˆ˜ì‹ ì œì•ˆ"""
        suggestions = []
        
        # 1. ì‘ì—… ìœ í˜• ë¶„ë¥˜
        task_type = self._classify_research_task(task_description)
        
        # 2. ë°ì´í„° íŠ¹ì„± ë¶„ì„
        data_characteristics = self._analyze_data_characteristics(data_context)
        
        # 3. ì í•©í•œ ìˆ˜ì‹ íŒ¨í„´ ì œì•ˆ
        formula_patterns = self._get_formula_patterns_for_task(task_type, data_characteristics)
        
        # 4. êµ¬ì²´ì ì¸ ìˆ˜ì‹ ìƒì„±
        for pattern in formula_patterns:
            suggestion = FormulaSuggestion(
                formula_template=pattern.template,
                description=pattern.description,
                use_case=pattern.use_case,
                example_data=pattern.example_data,
                expected_result=pattern.expected_result,
                confidence=pattern.confidence
            )
            suggestions.append(suggestion)
        
        return suggestions
    
    def _classify_research_task(self, task_description: str) -> str:
        """ì—°êµ¬ ì‘ì—… ìœ í˜• ë¶„ë¥˜"""
        task_lower = task_description.lower()
        
        if any(word in task_lower for word in ["í‰ê· ", "mean", "average"]):
            return "descriptive_statistics"
        elif any(word in task_lower for word in ["ìƒê´€", "correlation", "ê´€ê³„"]):
            return "correlation_analysis"
        elif any(word in task_lower for word in ["íšŒê·€", "regression", "ì˜ˆì¸¡"]):
            return "regression_analysis"
        elif any(word in task_lower for word in ["ìµœì í™”", "optimization", "ìµœì "]):
            return "optimization"
        elif any(word in task_lower for word in ["ì‹œë®¬ë ˆì´ì…˜", "simulation", "ëª¨ë¸ë§"]):
            return "simulation"
        else:
            return "general_analysis"
```

#### 3.2 ìˆ˜ì‹ ì˜¤ë¥˜ ì§„ë‹¨ ë° ìˆ˜ì • ì œì•ˆ
```python
class FormulaErrorDiagnostic:
    """ìˆ˜ì‹ ì˜¤ë¥˜ ì§„ë‹¨ ì‹œìŠ¤í…œ"""
    
    def diagnose_formula_errors(self, formula: Formula, 
                               data_context: DataContext) -> List[ErrorDiagnostic]:
        """ìˆ˜ì‹ ì˜¤ë¥˜ ì§„ë‹¨ ë° ìˆ˜ì • ì œì•ˆ"""
        diagnostics = []
        
        # 1. êµ¬ë¬¸ ì˜¤ë¥˜ ê²€ì‚¬
        syntax_errors = self._check_syntax_errors(formula)
        diagnostics.extend(syntax_errors)
        
        # 2. ë…¼ë¦¬ ì˜¤ë¥˜ ê²€ì‚¬
        logic_errors = self._check_logic_errors(formula, data_context)
        diagnostics.extend(logic_errors)
        
        # 3. ì„±ëŠ¥ ë¬¸ì œ ê²€ì‚¬
        performance_issues = self._check_performance_issues(formula)
        diagnostics.extend(performance_issues)
        
        # 4. ë°ì´í„° íƒ€ì… ì˜¤ë¥˜ ê²€ì‚¬
        data_type_errors = self._check_data_type_errors(formula, data_context)
        diagnostics.extend(data_type_errors)
        
        return diagnostics
    
    def _check_logic_errors(self, formula: Formula, data_context: DataContext) -> List[ErrorDiagnostic]:
        """ë…¼ë¦¬ ì˜¤ë¥˜ ê²€ì‚¬"""
        errors = []
        
        # ìˆœí™˜ ì°¸ì¡° ê²€ì‚¬
        if self._has_circular_reference(formula):
            errors.append(ErrorDiagnostic(
                error_type="circular_reference",
                severity="high",
                description="ìˆœí™˜ ì°¸ì¡°ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤",
                suggestion="ì°¸ì¡° ì²´ì¸ì„ í™•ì¸í•˜ê³  ìˆ˜ì •í•˜ì„¸ìš”",
                affected_cells=self._get_circular_reference_cells(formula)
            ))
        
        # ì˜ëª»ëœ ë²”ìœ„ ì°¸ì¡° ê²€ì‚¬
        invalid_ranges = self._find_invalid_range_references(formula, data_context)
        for invalid_range in invalid_ranges:
            errors.append(ErrorDiagnostic(
                error_type="invalid_range",
                severity="medium",
                description=f"ì˜ëª»ëœ ë²”ìœ„ ì°¸ì¡°: {invalid_range}",
                suggestion="ì˜¬ë°”ë¥¸ ë²”ìœ„ë¡œ ìˆ˜ì •í•˜ì„¸ìš”",
                affected_cells=[invalid_range]
            ))
        
        return errors
```

---

## ğŸ¯ Excel íŠ¹í™” ì‚¬ìš© ì‚¬ë¡€

### 1. ì‹¤í—˜ ë°ì´í„° ë¶„ì„ ì§€ì›
- **ìƒí™©**: ì—°êµ¬ìê°€ ì‹¤í—˜ ë°ì´í„°ë¥¼ Excelë¡œ ë¶„ì„í•˜ê³  PPTë¡œ ë°œí‘œ
- **ì§€ì› ê¸°ëŠ¥**:
  - ì‹¤í—˜ ë°ì´í„°ì— ì í•©í•œ í†µê³„ í•¨ìˆ˜ ìë™ ì œì•ˆ
  - ë°ì´í„° ì‹œê°í™”ë¥¼ ìœ„í•œ ì°¨íŠ¸ íƒ€ì… ì¶”ì²œ
  - Excel-PPT ê°„ ë°ì´í„° ì¼ê´€ì„± ê²€ì¦
  - í†µê³„ì  ìœ ì˜ì„± ê²€ì • ìˆ˜ì‹ ìƒì„±

### 2. ìˆ˜í•™ì  ëª¨ë¸ë§ ì§€ì›
- **ìƒí™©**: ë³µì¡í•œ ìˆ˜í•™ì  ëª¨ë¸ì„ Excelë¡œ êµ¬í˜„
- **ì§€ì› ê¸°ëŠ¥**:
  - ëª¨ë¸ ë°©ì •ì‹ì„ Excel ìˆ˜ì‹ìœ¼ë¡œ ë³€í™˜
  - ìˆ˜ì‹ ë³µì¡ë„ ë¶„ì„ ë° ìµœì í™” ì œì•ˆ
  - ê³„ì‚° ì²´ì¸ ì‹œê°í™” ë° ì˜ì¡´ì„± ë¶„ì„
  - ìˆ˜ì¹˜ í•´ë²• ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ ì§€ì›

### 3. ì¬ë¬´ ë¶„ì„ ì§€ì›
- **ìƒí™©**: ì—°êµ¬ í”„ë¡œì íŠ¸ì˜ ê²½ì œì„± ë¶„ì„
- **ì§€ì› ê¸°ëŠ¥**:
  - NPV, IRR ë“± ì¬ë¬´ í•¨ìˆ˜ ìë™ ì ìš©
  - ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° í…Œì´ë¸” ìƒì„±
  - ë¯¼ê°ë„ ë¶„ì„ ìˆ˜ì‹ ì œì•ˆ
  - íˆ¬ì ìˆ˜ìµë¥  ê³„ì‚° ìë™í™”

### 4. ê³¼í•™ ê³„ì‚° ì§€ì›
- **ìƒí™©**: ë¬¼ë¦¬, í™”í•™ ì‹¤í—˜ ë°ì´í„° ì²˜ë¦¬
- **ì§€ì› ê¸°ëŠ¥**:
  - ê³¼í•™ ìƒìˆ˜ ë° ë‹¨ìœ„ ë³€í™˜ ì§€ì›
  - ì‹¤í—˜ ì˜¤ì°¨ ë¶„ì„ ìˆ˜ì‹ ìƒì„±
  - í‘œì¤€ ê³¡ì„  í”¼íŒ… í•¨ìˆ˜ ì œì•ˆ
  - í†µê³„ì  ê²€ì • ìˆ˜ì‹ ìë™ ìƒì„±

---

## ğŸ“š Excel íŠ¹í™” ì°¸ê³  ìë£Œ

### ìˆ˜ì‹ ë¶„ì„ ê´€ë ¨ ì—°êµ¬
1. **Excel Formula Analysis**: ìˆ˜ì‹ êµ¬ì¡° ë¶„ì„ ë° ìµœì í™” ì—°êµ¬
2. **Spreadsheet Verification**: ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ê²€ì¦ ë° ì˜¤ë¥˜ ë°©ì§€ ê¸°ë²•
3. **Data Visualization Mapping**: ë°ì´í„°ì™€ ì‹œê°í™” ê°„ ë§¤í•‘ ì•Œê³ ë¦¬ì¦˜

### ìƒìš© ë„êµ¬ ë²¤ì¹˜ë§ˆí‚¹
1. **Excel Formula Auditing**: Microsoft Excelì˜ ìˆ˜ì‹ ê°ì‚¬ ê¸°ëŠ¥
2. **Spreadsheet Compare**: ìˆ˜ì‹ ë¹„êµ ë° ì°¨ì´ì  ë¶„ì„ ë„êµ¬
3. **FormulaDesk**: Excel ìˆ˜ì‹ ë¶„ì„ ë° ìµœì í™” ë„êµ¬

### ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸
1. **Formula Parser**: Excel ìˆ˜ì‹ íŒŒì‹± ë¼ì´ë¸ŒëŸ¬ë¦¬
2. **SpreadsheetML**: ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ë§ˆí¬ì—… ì–¸ì–´
3. **Excel Formula Engine**: Excel ìˆ˜ì‹ ê³„ì‚° ì—”ì§„

---

## ğŸ“Š PPT ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ìƒì„¸

### 1. ìµœì‹  ì—°êµ¬ ë™í–¥ ë° ìƒìš© ì„œë¹„ìŠ¤

#### 1.1 ë©€í‹°ëª¨ë‹¬ LLM ì—°êµ¬ í˜„í™©
- **GPT-4V (Vision)**: OpenAIì˜ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ë¡œ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ë™ì‹œì— ì²˜ë¦¬
- **Claude 3 Vision**: Anthropicì˜ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ë¡œ ë¬¸ì„œ ë¶„ì„ì— íŠ¹í™”
- **Gemini Pro Vision**: Googleì˜ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ë¡œ ì°¨íŠ¸/í…Œì´ë¸” ì¸ì‹ì— ê°•ì 
- **LLaVA**: ì˜¤í”ˆì†ŒìŠ¤ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ë¡œ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥

#### 1.2 ë¬¸ì„œ ë ˆì´ì•„ì›ƒ ë¶„ì„ ì—°êµ¬
- **LayoutLM**: Microsoftì˜ ë¬¸ì„œ ë ˆì´ì•„ì›ƒ ì´í•´ ëª¨ë¸
- **Table Transformer**: í…Œì´ë¸” êµ¬ì¡° ì¸ì‹ ì „ìš© ëª¨ë¸
- **PubLayNet**: í•™ìˆ  ë…¼ë¬¸ ë ˆì´ì•„ì›ƒ ë¶„ì„ ë°ì´í„°ì…‹
- **DocBank**: ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ì„ ìœ„í•œ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹

#### 1.3 ìƒìš© ì„œë¹„ìŠ¤ ì‚¬ë¡€
- **Adobe Acrobat**: PDFì˜ í‘œ, í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ ë ˆì´ì•„ì›ƒì„ PowerPointë¡œ ë³€í™˜
- **UPDF**: OCR ê¸°ëŠ¥ì„ í†µí•œ ìŠ¤ìº”ëœ PDFì˜ í…ìŠ¤íŠ¸ ì¸ì‹
- **GOT-OCR2.0**: ê·¸ë˜í”½ê³¼ í…Œì´ë¸”ì„ í¬í•¨í•œ ë‹¤ì–‘í•œ OCR ì‘ì—… ì§€ì›

### 2. PPT ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

#### 2.1 ìŠ¬ë¼ì´ë“œ ì´ë¯¸ì§€ ë³€í™˜
```python
class SlideImageConverter:
    """ìŠ¬ë¼ì´ë“œ ì´ë¯¸ì§€ ë³€í™˜ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.ppt_converter = PPTConverter()
        self.pdf_converter = PDFConverter()
        self.image_processor = ImageProcessor()
    
    def convert_slides_to_images(self, ppt_path: str) -> List[np.ndarray]:
        """PPT ìŠ¬ë¼ì´ë“œë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        images = []
        
        # 1. PPTë¥¼ PDFë¡œ ë³€í™˜
        pdf_path = self.ppt_converter.convert_to_pdf(ppt_path)
        
        # 2. PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
        pdf_images = self.pdf_converter.convert_to_images(pdf_path)
        
        # 3. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        for img in pdf_images:
            processed_img = self.image_processor.preprocess(img)
            images.append(processed_img)
        
        return images
    
    def convert_slides_to_high_res_images(self, ppt_path: str, dpi: int = 300) -> List[np.ndarray]:
        """ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (OCR ì •í™•ë„ í–¥ìƒ)"""
        images = []
        
        # ê³ í•´ìƒë„ ë³€í™˜
        high_res_images = self.ppt_converter.convert_to_images(ppt_path, dpi=dpi)
        
        for img in high_res_images:
            # ì´ë¯¸ì§€ í’ˆì§ˆ í–¥ìƒ
            enhanced_img = self.image_processor.enhance_for_ocr(img)
            images.append(enhanced_img)
        
        return images
```

### 3. PPT íŠ¹í™” ì‚¬ìš© ì‚¬ë¡€

#### 3.1 í•™ìˆ  ë°œí‘œ ë¶„ì„ ì§€ì›
- **ìƒí™©**: ì—°êµ¬ìê°€ í•™ìˆ  ë°œí‘œ PPTë¥¼ ì—…ë¡œë“œí•˜ì—¬ ë‚´ìš© ë¶„ì„ ìš”ì²­
- **ì§€ì› ê¸°ëŠ¥**:
  - ìŠ¬ë¼ì´ë“œë³„ í•µì‹¬ ë‚´ìš© ìë™ ì¶”ì¶œ
  - ì°¨íŠ¸/ê·¸ë˜í”„ ë°ì´í„° ìë™ í•´ì„
  - ë°œí‘œ êµ¬ì¡° ë° ë…¼ë¦¬ì  íë¦„ ë¶„ì„
  - ê´€ë ¨ ì—°êµ¬ ë¶„ì•¼ ìë™ ì œì•ˆ

#### 3.2 ì—°êµ¬ ê²°ê³¼ ì‹œê°í™” ì§€ì›
- **ìƒí™©**: ì‹¤í—˜ ê²°ê³¼ë¥¼ PPTë¡œ ì •ë¦¬í•œ í›„ ì¶”ê°€ ë¶„ì„ ìš”ì²­
- **ì§€ì› ê¸°ëŠ¥**:
  - ì‹¤í—˜ ë°ì´í„°ì™€ PPT ì°¨íŠ¸ ê°„ ì¼ê´€ì„± ê²€ì¦
  - í†µê³„ì  ìœ ì˜ì„± ìë™ ê³„ì‚° ë° ì œì•ˆ
  - ì‹œê°í™” ê°œì„  ë°©ì•ˆ ì œì•ˆ
  - ì¶”ê°€ ë¶„ì„ ë°©í–¥ ì œì‹œ

#### 3.3 ì—°êµ¬ ë…¼ë¬¸ ì‘ì„± ì§€ì›
- **ìƒí™©**: ì—°êµ¬ ë…¼ë¬¸ì˜ ê·¸ë¦¼ê³¼ í‘œë¥¼ PPTë¡œ ì •ë¦¬í•œ í›„ ë…¼ë¬¸ ì‘ì„± ì§€ì›
- **ì§€ì› ê¸°ëŠ¥**:
  - ê·¸ë¦¼ ì„¤ëª… ìë™ ìƒì„±
  - í‘œ ë°ì´í„° ìš”ì•½ ë° í•´ì„
  - ë…¼ë¬¸ êµ¬ì¡° ìµœì í™” ì œì•ˆ
  - ê´€ë ¨ ë¬¸í—Œ ìë™ ê²€ìƒ‰

---

## ğŸ“š PPT ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ì°¸ê³  ìë£Œ

### ìµœì‹  ì—°êµ¬ ë…¼ë¬¸
1. **"Multimodal Document Analysis with Vision-Language Models"** (2024)
2. **"ChartQA: A Dataset for Question Answering about Charts"** (2022)
3. **"TableNet: Deep Learning model for end-to-end Table detection and Tabular data extraction"** (2019)
4. **"LayoutLM: Pre-training of Text and Layout for Document Image Understanding"** (2020)

### ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸
1. **LayoutLM**: Microsoftì˜ ë¬¸ì„œ ë ˆì´ì•„ì›ƒ ì´í•´ ëª¨ë¸
2. **Table Transformer**: Facebookì˜ í…Œì´ë¸” ê°ì§€ ëª¨ë¸
3. **PaddleOCR**: Baiduì˜ OCR íˆ´í‚·
4. **EasyOCR**: ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ OCR ë¼ì´ë¸ŒëŸ¬ë¦¬

### ìƒìš© API ì„œë¹„ìŠ¤
1. **Google Vision API**: ê³ í’ˆì§ˆ OCR ë° ì´ë¯¸ì§€ ë¶„ì„
2. **Azure Computer Vision**: Microsoftì˜ ì»´í“¨í„° ë¹„ì „ ì„œë¹„ìŠ¤
3. **AWS Textract**: Amazonì˜ ë¬¸ì„œ ë¶„ì„ ì„œë¹„ìŠ¤
4. **Adobe PDF Services API**: PDF ì²˜ë¦¬ ë° ë³€í™˜ ì„œë¹„ìŠ¤

---

**ë¬¸ì„œ ë²„ì „**: 2.0  
**ìµœì¢… ìˆ˜ì •ì¼**: 2024-12-19  
**ì‘ì„±ì**: RAG Development Team  
**ê²€í† ì**: Research Strategy Team
